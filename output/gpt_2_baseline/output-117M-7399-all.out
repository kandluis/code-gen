Loading checkpoint models/117M/model.ckpt
Loading dataset...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.62it/s]
dataset has 61451752 tokens
Training...
[1 | 8.32] loss=2.05 avg=2.05
[2 | 13.20] loss=2.20 avg=2.12
[3 | 18.27] loss=2.10 avg=2.11
[4 | 23.30] loss=1.69 avg=2.01
[5 | 28.64] loss=1.79 avg=1.96
[6 | 33.63] loss=1.71 avg=1.92
[7 | 38.59] loss=2.01 avg=1.93
[8 | 43.56] loss=0.97 avg=1.81
[9 | 48.56] loss=1.22 avg=1.74
[10 | 53.56] loss=1.77 avg=1.74
[11 | 58.74] loss=2.06 avg=1.77
[12 | 63.90] loss=1.33 avg=1.73
[13 | 68.97] loss=2.25 avg=1.78
[14 | 74.09] loss=1.66 avg=1.77
[15 | 79.24] loss=1.76 avg=1.77
[16 | 84.29] loss=1.81 avg=1.77
[17 | 89.36] loss=1.51 avg=1.75
[18 | 94.66] loss=1.63 avg=1.75
[19 | 99.91] loss=1.61 avg=1.74
[20 | 105.17] loss=1.61 avg=1.73
[21 | 110.34] loss=1.69 avg=1.73
[22 | 115.57] loss=1.32 avg=1.71
[23 | 120.79] loss=1.75 avg=1.71
[24 | 126.00] loss=1.47 avg=1.70
[25 | 131.27] loss=1.32 avg=1.68
[26 | 136.50] loss=1.67 avg=1.68
[27 | 141.85] loss=1.51 avg=1.67
[28 | 147.18] loss=1.83 avg=1.68
[29 | 152.42] loss=1.62 avg=1.68
[30 | 157.71] loss=1.31 avg=1.66
[31 | 162.97] loss=1.65 avg=1.66
[32 | 168.22] loss=1.35 avg=1.65
[33 | 173.50] loss=1.43 avg=1.64
[34 | 178.87] loss=1.14 avg=1.63
[35 | 184.25] loss=1.72 avg=1.63
[36 | 189.75] loss=1.39 avg=1.62
[37 | 195.11] loss=1.98 avg=1.63
[38 | 200.48] loss=1.41 avg=1.63
[39 | 205.88] loss=2.81 avg=1.66
[40 | 211.28] loss=1.69 avg=1.66
[41 | 216.57] loss=1.63 avg=1.66
[42 | 221.80] loss=1.51 avg=1.66
[43 | 227.04] loss=1.73 avg=1.66
[44 | 232.31] loss=1.53 avg=1.66
[45 | 237.55] loss=0.93 avg=1.64
[46 | 242.81] loss=1.36 avg=1.63
[47 | 248.08] loss=1.70 avg=1.63
[48 | 253.39] loss=1.80 avg=1.64
[49 | 258.66] loss=1.27 avg=1.63
[50 | 263.95] loss=1.54 avg=1.62
[51 | 269.21] loss=1.28 avg=1.62
[52 | 274.60] loss=1.76 avg=1.62
[53 | 279.79] loss=1.54 avg=1.62
[54 | 284.94] loss=1.22 avg=1.61
[55 | 290.15] loss=1.84 avg=1.61
[56 | 295.47] loss=1.54 avg=1.61
[57 | 300.75] loss=1.23 avg=1.60
[58 | 306.18] loss=1.34 avg=1.60
[59 | 311.34] loss=2.02 avg=1.61
[60 | 316.59] loss=1.73 avg=1.61
[61 | 321.86] loss=1.56 avg=1.61
[62 | 327.07] loss=1.17 avg=1.60
[63 | 332.24] loss=1.44 avg=1.59
[64 | 337.44] loss=1.90 avg=1.60
[65 | 342.51] loss=1.63 avg=1.60
[66 | 347.68] loss=1.79 avg=1.61
[67 | 352.85] loss=1.60 avg=1.61
[68 | 358.05] loss=1.35 avg=1.60
[69 | 363.22] loss=1.64 avg=1.60
[70 | 368.35] loss=1.36 avg=1.60
[71 | 373.46] loss=0.92 avg=1.58
[72 | 378.56] loss=0.89 avg=1.57
[73 | 383.64] loss=1.64 avg=1.57
[74 | 388.76] loss=1.60 avg=1.57
[75 | 393.90] loss=1.30 avg=1.57
[76 | 399.02] loss=1.45 avg=1.56
[77 | 404.09] loss=1.39 avg=1.56
[78 | 409.22] loss=1.68 avg=1.56
[79 | 414.43] loss=1.49 avg=1.56
[80 | 419.62] loss=1.21 avg=1.56
[81 | 424.87] loss=1.72 avg=1.56
[82 | 430.00] loss=1.01 avg=1.55
[83 | 435.19] loss=1.50 avg=1.55
[84 | 440.45] loss=1.38 avg=1.54
[85 | 445.53] loss=1.32 avg=1.54
[86 | 450.78] loss=1.18 avg=1.53
[87 | 455.94] loss=1.15 avg=1.53
[88 | 461.23] loss=1.83 avg=1.53
[89 | 466.40] loss=0.99 avg=1.52
[90 | 471.58] loss=1.72 avg=1.53
[91 | 476.76] loss=1.33 avg=1.52
[92 | 482.01] loss=1.48 avg=1.52
[93 | 487.30] loss=1.59 avg=1.52
[94 | 492.47] loss=1.36 avg=1.52
[95 | 497.73] loss=1.41 avg=1.52
[96 | 502.98] loss=1.62 avg=1.52
[97 | 508.09] loss=1.35 avg=1.52
[98 | 513.19] loss=2.36 avg=1.53
[99 | 518.28] loss=1.42 avg=1.53
Generating samples...
======== SAMPLE 1 ========
                                                                                                                                                                                                                                                                                                                            )     _ _                                                                                                                                                                                                              )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                

[100 | 605.27] loss=1.42 avg=1.53
[101 | 610.41] loss=1.24 avg=1.52
[102 | 615.63] loss=1.17 avg=1.52
[103 | 621.01] loss=1.05 avg=1.51
[104 | 626.17] loss=1.21 avg=1.51
[105 | 631.41] loss=1.46 avg=1.51
[106 | 636.62] loss=1.24 avg=1.50
[107 | 641.80] loss=2.31 avg=1.51
[108 | 646.84] loss=1.40 avg=1.51
[109 | 651.91] loss=1.47 avg=1.51
[110 | 657.15] loss=1.31 avg=1.51
[111 | 662.43] loss=1.46 avg=1.51
[112 | 667.64] loss=1.06 avg=1.50
[113 | 672.79] loss=1.56 avg=1.50
[114 | 677.96] loss=1.28 avg=1.50
[115 | 683.23] loss=1.19 avg=1.49
[116 | 688.53] loss=1.59 avg=1.50
[117 | 693.79] loss=1.90 avg=1.50
[118 | 699.05] loss=1.13 avg=1.50
[119 | 704.29] loss=1.24 avg=1.49
[120 | 709.50] loss=1.37 avg=1.49
[121 | 714.76] loss=1.71 avg=1.49
[122 | 719.90] loss=1.36 avg=1.49
[123 | 725.04] loss=1.18 avg=1.49
[124 | 730.26] loss=1.09 avg=1.48
[125 | 735.43] loss=1.18 avg=1.48
[126 | 740.57] loss=1.43 avg=1.48
[127 | 745.70] loss=1.65 avg=1.48
[128 | 751.00] loss=1.46 avg=1.48
[129 | 756.27] loss=1.11 avg=1.47
[130 | 761.64] loss=1.21 avg=1.47
[131 | 766.88] loss=1.16 avg=1.47
[132 | 772.04] loss=1.67 avg=1.47
[133 | 777.31] loss=1.19 avg=1.47
[134 | 782.60] loss=1.32 avg=1.46
[135 | 787.99] loss=1.19 avg=1.46
[136 | 793.36] loss=1.30 avg=1.46
[137 | 798.53] loss=1.60 avg=1.46
[138 | 803.82] loss=1.65 avg=1.46
[139 | 809.01] loss=1.15 avg=1.46
[140 | 814.21] loss=0.97 avg=1.45
[141 | 819.41] loss=1.06 avg=1.45
[142 | 824.60] loss=1.14 avg=1.44
[143 | 829.87] loss=1.24 avg=1.44
[144 | 835.22] loss=1.35 avg=1.44
[145 | 840.50] loss=1.56 avg=1.44
[146 | 845.77] loss=1.15 avg=1.44
[147 | 850.85] loss=1.22 avg=1.43
[148 | 856.06] loss=1.11 avg=1.43
[149 | 861.08] loss=1.23 avg=1.43
[150 | 866.17] loss=1.35 avg=1.43
[151 | 871.25] loss=1.36 avg=1.43
[152 | 876.47] loss=1.14 avg=1.42
[153 | 881.65] loss=1.10 avg=1.42
[154 | 886.74] loss=1.47 avg=1.42
[155 | 891.88] loss=1.26 avg=1.42
[156 | 896.97] loss=1.50 avg=1.42
[157 | 902.06] loss=1.29 avg=1.42
[158 | 907.14] loss=1.82 avg=1.42
[159 | 912.25] loss=1.28 avg=1.42
[160 | 917.47] loss=1.35 avg=1.42
[161 | 922.78] loss=1.25 avg=1.42
[162 | 928.02] loss=2.18 avg=1.43
[163 | 933.39] loss=1.45 avg=1.43
[164 | 938.66] loss=0.84 avg=1.42
[165 | 943.90] loss=1.19 avg=1.42
[166 | 949.16] loss=1.70 avg=1.42
[167 | 954.43] loss=1.06 avg=1.41
[168 | 959.67] loss=1.25 avg=1.41
[169 | 964.72] loss=1.61 avg=1.42
[170 | 969.84] loss=1.28 avg=1.41
[171 | 974.93] loss=1.57 avg=1.42
[172 | 980.05] loss=1.14 avg=1.41
[173 | 985.18] loss=1.05 avg=1.41
[174 | 990.38] loss=1.22 avg=1.41
[175 | 995.76] loss=1.42 avg=1.41
[176 | 1001.06] loss=1.65 avg=1.41
[177 | 1006.29] loss=1.04 avg=1.40
[178 | 1011.70] loss=1.54 avg=1.41
[179 | 1016.91] loss=0.97 avg=1.40
[180 | 1022.13] loss=1.84 avg=1.41
[181 | 1027.36] loss=2.28 avg=1.42
[182 | 1033.06] loss=1.09 avg=1.41
[183 | 1038.26] loss=1.01 avg=1.41
[184 | 1043.43] loss=1.47 avg=1.41
[185 | 1048.68] loss=1.57 avg=1.41
[186 | 1053.98] loss=1.24 avg=1.41
[187 | 1059.30] loss=0.96 avg=1.40
[188 | 1064.53] loss=1.05 avg=1.40
[189 | 1069.56] loss=1.03 avg=1.39
[190 | 1074.61] loss=1.49 avg=1.40
[191 | 1079.74] loss=1.40 avg=1.40
[192 | 1084.91] loss=1.24 avg=1.39
[193 | 1089.92] loss=0.98 avg=1.39
[194 | 1095.05] loss=1.96 avg=1.40
[195 | 1100.19] loss=1.55 avg=1.40
[196 | 1105.34] loss=1.57 avg=1.40
[197 | 1110.44] loss=1.23 avg=1.40
[198 | 1115.52] loss=1.23 avg=1.40
[199 | 1120.69] loss=1.01 avg=1.39
Generating samples...
======== SAMPLE 1 ========
                                  __returns
                  self.update_list()
                       self.pop()
         ''' self.update_list()
self.__returns.pop(values)
def add_list(self, values):
    """ Add an item to an unordered list, and return its first value.
   Arguments:
    {0: True} """
                self.update_list()
                 self.pop()
        ''' self.update_list_as_list(values)

    def update_list(self, values, ):
    """ Increment an item by value
   Arguments:
    {0: False} """
          "Increment"
                Values:
                   value.remove_list()

    """
        
        values = [None, None, None, ...]
    return _update_list(values) ** [None] - [None]

    def update_list(self, values):
    """ Increment an item by value
  Arguments:
    {0: False} """
          "Increment"
                values = [None, None, None, ...]
    print "Increment" - "a list"
    args.pop(values)
    def update_list_as_list(self, values, ):
    """ Increment an item by value
  Arguments:
    {0: False} """
         "Increment"
            values = [[None, None, None, None, None], _update_list](values)

    """
         "Increment"
             values = [None, None, None, None, ...] - [None]

    def pop(self, values):
    """ Pop an item from the list and return its first value after the self.update_list process.
  Examples:
               self.pop().append(values)
                 self.pop().pop()
                self.pop().pop()
                self.pop().pop().pop()

    """
            items = [None, None, None, None, None]

    return _pop/append(values)

    def update_list_as_list(self, values):
    """ Increment an item by value  Arguments:
    {0: True} """
         "Increment"
             values = [None, None, None, None, None], _update_list()

    print "Increment" - "a list"
    return _update_list(values) ** [None] - [None]

    def update_list_as_list(self, values, ):
    """ Increment an item by value
  Arguments:
    {0: False} """
        "Increment"
            values = []
    print "Decrement" - "a list"
    return _update_list(values) ** [None] - [None]

   

[200 | 1204.54] loss=1.28 avg=1.39
[201 | 1209.62] loss=1.22 avg=1.39
[202 | 1214.84] loss=1.23 avg=1.39
[203 | 1220.01] loss=1.06 avg=1.38
[204 | 1225.08] loss=1.22 avg=1.38
[205 | 1230.17] loss=1.70 avg=1.38
[206 | 1235.23] loss=1.38 avg=1.38
[207 | 1240.47] loss=0.88 avg=1.38
[208 | 1245.55] loss=1.27 avg=1.38
[209 | 1250.76] loss=1.53 avg=1.38
[210 | 1255.76] loss=1.28 avg=1.38
[211 | 1260.78] loss=1.04 avg=1.37
[212 | 1265.89] loss=1.23 avg=1.37
[213 | 1271.07] loss=1.92 avg=1.38
[214 | 1276.34] loss=1.13 avg=1.38
[215 | 1281.57] loss=0.89 avg=1.37
[216 | 1286.66] loss=2.04 avg=1.38
[217 | 1291.80] loss=1.11 avg=1.37
[218 | 1296.75] loss=1.31 avg=1.37
[219 | 1301.85] loss=1.37 avg=1.37
[220 | 1307.07] loss=1.69 avg=1.38
[221 | 1312.19] loss=1.36 avg=1.38
[222 | 1317.35] loss=0.77 avg=1.37
[223 | 1322.47] loss=0.85 avg=1.36
[224 | 1327.68] loss=1.75 avg=1.37
[225 | 1332.81] loss=0.57 avg=1.36
[226 | 1338.72] loss=1.78 avg=1.36
[227 | 1344.44] loss=1.17 avg=1.36
[228 | 1349.71] loss=1.17 avg=1.36
[229 | 1355.00] loss=1.12 avg=1.36
[230 | 1360.45] loss=0.93 avg=1.35
[231 | 1365.58] loss=1.26 avg=1.35
[232 | 1370.76] loss=1.91 avg=1.36
[233 | 1375.94] loss=1.12 avg=1.36
[234 | 1381.13] loss=1.25 avg=1.35
[235 | 1386.45] loss=1.16 avg=1.35
[236 | 1391.72] loss=0.89 avg=1.35
[237 | 1397.06] loss=1.23 avg=1.35
[238 | 1402.31] loss=1.92 avg=1.35
[239 | 1407.58] loss=1.52 avg=1.35
[240 | 1412.81] loss=1.53 avg=1.36
[241 | 1418.11] loss=1.16 avg=1.35
[242 | 1423.37] loss=1.27 avg=1.35
[243 | 1428.59] loss=1.49 avg=1.35
[244 | 1433.74] loss=1.11 avg=1.35
[245 | 1438.93] loss=1.74 avg=1.36
[246 | 1444.11] loss=0.79 avg=1.35
[247 | 1449.19] loss=1.12 avg=1.35
[248 | 1454.40] loss=1.47 avg=1.35
[249 | 1459.65] loss=1.53 avg=1.35
[250 | 1464.80] loss=1.31 avg=1.35
[251 | 1470.09] loss=1.58 avg=1.35
[252 | 1475.42] loss=1.58 avg=1.35
[253 | 1480.57] loss=2.25 avg=1.36
[254 | 1485.30] loss=1.60 avg=1.37
[255 | 1490.43] loss=1.41 avg=1.37
[256 | 1495.70] loss=1.75 avg=1.37
[257 | 1500.88] loss=1.24 avg=1.37
[258 | 1506.06] loss=1.63 avg=1.37
[259 | 1511.27] loss=1.64 avg=1.38
[260 | 1516.49] loss=1.35 avg=1.38
[261 | 1521.81] loss=1.49 avg=1.38
[262 | 1527.12] loss=1.30 avg=1.38
[263 | 1532.41] loss=1.32 avg=1.38
[264 | 1537.76] loss=1.03 avg=1.37
[265 | 1542.94] loss=1.07 avg=1.37
[266 | 1548.19] loss=1.64 avg=1.37
[267 | 1553.32] loss=0.85 avg=1.37
[268 | 1558.63] loss=1.51 avg=1.37
[269 | 1563.99] loss=1.12 avg=1.36
[270 | 1569.22] loss=0.47 avg=1.36
[271 | 1574.40] loss=0.89 avg=1.35
[272 | 1579.50] loss=1.55 avg=1.35
[273 | 1584.64] loss=1.07 avg=1.35
[274 | 1589.90] loss=1.14 avg=1.35
[275 | 1595.12] loss=1.77 avg=1.35
[276 | 1600.43] loss=1.29 avg=1.35
[277 | 1605.70] loss=1.62 avg=1.35
[278 | 1610.96] loss=0.99 avg=1.35
[279 | 1616.19] loss=1.81 avg=1.35
[280 | 1621.37] loss=1.24 avg=1.35
[281 | 1626.59] loss=1.62 avg=1.36
[282 | 1631.78] loss=1.82 avg=1.36
[283 | 1637.18] loss=1.50 avg=1.36
[284 | 1642.57] loss=0.66 avg=1.36
[285 | 1647.81] loss=0.98 avg=1.35
[286 | 1653.08] loss=1.21 avg=1.35
[287 | 1658.25] loss=0.98 avg=1.35
[288 | 1663.67] loss=1.43 avg=1.35
[289 | 1668.90] loss=1.81 avg=1.35
[290 | 1674.23] loss=0.67 avg=1.34
[291 | 1679.49] loss=1.22 avg=1.34
[292 | 1684.77] loss=1.34 avg=1.34
[293 | 1690.08] loss=1.57 avg=1.35
[294 | 1695.33] loss=1.35 avg=1.35
[295 | 1700.61] loss=1.28 avg=1.35
[296 | 1705.80] loss=1.21 avg=1.34
[297 | 1711.09] loss=1.17 avg=1.34
[298 | 1716.38] loss=1.33 avg=1.34
[299 | 1721.69] loss=1.01 avg=1.34
Generating samples...
======== SAMPLE 1 ========
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                )                                               
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 

[300 | 1808.95] loss=1.03 avg=1.34
[301 | 1814.03] loss=1.11 avg=1.33
[302 | 1819.17] loss=1.54 avg=1.33
[303 | 1824.27] loss=1.40 avg=1.34
[304 | 1829.38] loss=1.79 avg=1.34
[305 | 1834.50] loss=0.91 avg=1.34
[306 | 1839.81] loss=1.59 avg=1.34
[307 | 1845.06] loss=1.13 avg=1.34
[308 | 1850.26] loss=1.28 avg=1.34
[309 | 1855.51] loss=1.40 avg=1.34
[310 | 1860.74] loss=1.51 avg=1.34
[311 | 1865.89] loss=1.71 avg=1.34
[312 | 1871.02] loss=1.17 avg=1.34
[313 | 1876.22] loss=1.21 avg=1.34
[314 | 1881.39] loss=0.91 avg=1.33
[315 | 1886.57] loss=1.49 avg=1.34
[316 | 1891.74] loss=1.11 avg=1.33
[317 | 1896.94] loss=1.44 avg=1.33
[318 | 1902.12] loss=1.28 avg=1.33
[319 | 1907.41] loss=1.10 avg=1.33
[320 | 1912.66] loss=1.46 avg=1.33
[321 | 1917.82] loss=1.20 avg=1.33
[322 | 1923.06] loss=0.97 avg=1.33
[323 | 1928.27] loss=0.90 avg=1.32
[324 | 1933.43] loss=1.25 avg=1.32
[325 | 1938.58] loss=1.72 avg=1.33
[326 | 1943.84] loss=1.23 avg=1.33
[327 | 1949.01] loss=1.92 avg=1.33
[328 | 1954.10] loss=1.11 avg=1.33
[329 | 1959.19] loss=1.05 avg=1.33
[330 | 1964.27] loss=1.56 avg=1.33
[331 | 1969.49] loss=1.11 avg=1.33
[332 | 1974.57] loss=1.38 avg=1.33
[333 | 1979.71] loss=1.33 avg=1.33
[334 | 1985.02] loss=0.81 avg=1.32
[335 | 1990.22] loss=1.27 avg=1.32
[336 | 1995.31] loss=1.65 avg=1.33
[337 | 2000.66] loss=1.29 avg=1.32
[338 | 2006.02] loss=1.14 avg=1.32
[339 | 2011.38] loss=1.23 avg=1.32
[340 | 2016.79] loss=1.91 avg=1.33
[341 | 2022.10] loss=1.62 avg=1.33
[342 | 2027.45] loss=1.56 avg=1.33
[343 | 2032.77] loss=1.67 avg=1.34
[344 | 2038.04] loss=1.08 avg=1.33
[345 | 2043.28] loss=1.22 avg=1.33
[346 | 2048.38] loss=1.65 avg=1.34
[347 | 2053.56] loss=1.84 avg=1.34
[348 | 2058.74] loss=1.76 avg=1.35
[349 | 2063.89] loss=1.04 avg=1.34
[350 | 2068.95] loss=1.00 avg=1.34
[351 | 2074.02] loss=1.80 avg=1.34
[352 | 2079.16] loss=1.67 avg=1.35
[353 | 2084.17] loss=1.93 avg=1.35
[354 | 2089.25] loss=0.88 avg=1.35
[355 | 2094.44] loss=1.02 avg=1.34
[356 | 2099.70] loss=1.40 avg=1.35
[357 | 2104.95] loss=1.15 avg=1.34
[358 | 2110.15] loss=1.65 avg=1.35
[359 | 2115.39] loss=1.58 avg=1.35
[360 | 2120.59] loss=1.16 avg=1.35
[361 | 2125.89] loss=0.93 avg=1.34
[362 | 2131.09] loss=0.96 avg=1.34
[363 | 2136.30] loss=1.40 avg=1.34
[364 | 2141.60] loss=0.82 avg=1.33
[365 | 2146.80] loss=1.00 avg=1.33
[366 | 2152.04] loss=1.17 avg=1.33
[367 | 2157.45] loss=1.73 avg=1.33
[368 | 2162.80] loss=1.28 avg=1.33
[369 | 2168.01] loss=1.02 avg=1.33
[370 | 2173.25] loss=1.38 avg=1.33
[371 | 2178.63] loss=1.05 avg=1.33
[372 | 2183.99] loss=1.09 avg=1.32
[373 | 2189.18] loss=0.77 avg=1.32
[374 | 2194.35] loss=2.26 avg=1.33
[375 | 2199.50] loss=0.94 avg=1.32
[376 | 2204.66] loss=1.00 avg=1.32
[377 | 2209.84] loss=1.56 avg=1.32
[378 | 2214.88] loss=1.20 avg=1.32
[379 | 2220.12] loss=1.18 avg=1.32
[380 | 2225.34] loss=1.27 avg=1.32
[381 | 2230.52] loss=0.94 avg=1.32
[382 | 2235.73] loss=1.90 avg=1.32
[383 | 2240.97] loss=1.21 avg=1.32
[384 | 2246.09] loss=0.98 avg=1.32
[385 | 2251.26] loss=0.95 avg=1.31
[386 | 2256.36] loss=1.95 avg=1.32
[387 | 2261.46] loss=1.49 avg=1.32
[388 | 2266.70] loss=1.08 avg=1.32
[389 | 2271.91] loss=1.80 avg=1.32
[390 | 2277.24] loss=2.23 avg=1.33
[391 | 2282.48] loss=0.69 avg=1.33
[392 | 2287.74] loss=1.63 avg=1.33
[393 | 2292.88] loss=1.03 avg=1.33
[394 | 2298.06] loss=0.79 avg=1.32
[395 | 2303.26] loss=1.61 avg=1.32
[396 | 2308.51] loss=1.61 avg=1.33
[397 | 2313.65] loss=1.31 avg=1.33
[398 | 2318.87] loss=1.18 avg=1.33
[399 | 2324.06] loss=1.89 avg=1.33
Generating samples...
======== SAMPLE 1 ========
 {',
                                '''
         return  }

      return {}, index : {}, index_num : 0 }
     .run('run a.cs', function(self, index) {
        self._index = self._index
        self._index_num = self._index
        self._index = self._index_num
        self._index_num = self._index_num

    __salt__ = self.store, 'sessions'  
     return del self [], index : {}, index_num : 0 }
     self.update(self._index_num, index).run()

    self.update(self._index_num, index_num).run()
     print 'Running a.cs' [dyn.debug(self, 'running'.join_lines())]  [u'def', u'targets_list', u'(', u')', u':', u'int', u'=', u'int32', u'.', u'extend_extension_types', u'.', u'extend_types', u'.', u'extend':', u'(', u')', u'defaults', u'=', u'self', u'.', u'extend', u':', u'[', u'self', u']', u'.', u'update', u'.', u'(', u')', u'setup', u'=', u'self', u'.', u'update', u'.', u'(', u')', u'defaults', u'(', u'', u')', u'=', u'self', u'.', u'setup', u'.', u'trace', u'.', u'[', u'self', u']', u'.', u'update', u'.', u'set', u'.', u'run', u'(', u'self', u',', u'index', u')', u'defaults', u'(', u')', u'defaults', u'=', u'self', u'.', u'sets', u'.', u'update', u'.', u'(', u')', u'defaults', u'(', u'setup', u'.', u'trace', u')'] Run a python script with: # -------------------------    """
        [touches_list] [self, self list_list_list]
        Run a python script with: # ------------------------     """
        self: """Run a python python script with:
        list_list_list Python list.
         """
        set_path(self.executor_path) [u'Run', u'to', u'run', u'a', u'the', u'script', u'with', u'to', u'list_list_list', u', u'sparsed_list', u')<|endoftext|>This is the first in a series of articles on the issue of 'non-cognitive processing', in which I will highlight just how complex cognitive and learning processes of the brain function. The issues of non-cognitive processing and its implications are well-researched in the article.


The neurobiology of non-cognitive processing and its benefits are now more complex and a topic of future research.


The effects of non-cognitive processes on learning and memory span into the complex of the brain. The complex includes complex neural networks, complex sub-cortical structures, complex and complex structures of the brain, the brain's processing and the process of acquiring, processing, and using them.


If for some reason you cannot get enough of the basic details of the brain, we think we must look at the different 'dysplacement' stages, the different processes, the different processing periods, the different function stages. This is one of the most complex questions in neurobiology and its importance today, in the fields of basic information processing, and of both learning and memory.<|endoftext|>The top 10 biggest new apps of 2014
The top 10 big new apps of 2014 is finally over!

[400 | 2410.69] loss=1.47 avg=1.33
[401 | 2415.89] loss=1.24 avg=1.33
[402 | 2421.06] loss=0.96 avg=1.33
[403 | 2426.21] loss=1.07 avg=1.33
[404 | 2431.29] loss=1.25 avg=1.33
[405 | 2436.61] loss=1.04 avg=1.32
[406 | 2441.86] loss=1.34 avg=1.32
[407 | 2447.06] loss=1.03 avg=1.32
[408 | 2452.26] loss=0.80 avg=1.31
[409 | 2457.55] loss=1.17 avg=1.31
[410 | 2462.76] loss=1.13 avg=1.31
[411 | 2467.94] loss=0.92 avg=1.31
[412 | 2473.23] loss=1.56 avg=1.31
[413 | 2478.37] loss=1.61 avg=1.31
[414 | 2483.47] loss=1.04 avg=1.31
[415 | 2488.67] loss=1.19 avg=1.31
[416 | 2493.74] loss=0.68 avg=1.30
[417 | 2498.82] loss=1.07 avg=1.30
[418 | 2503.90] loss=1.02 avg=1.30
[419 | 2509.06] loss=0.97 avg=1.29
[420 | 2514.20] loss=1.02 avg=1.29
[421 | 2519.30] loss=1.04 avg=1.29
[422 | 2524.54] loss=1.67 avg=1.29
[423 | 2529.72] loss=0.83 avg=1.29
[424 | 2534.88] loss=1.82 avg=1.29
[425 | 2540.03] loss=0.91 avg=1.29
[426 | 2545.25] loss=1.05 avg=1.29
[427 | 2550.35] loss=1.28 avg=1.29
[428 | 2555.48] loss=1.00 avg=1.28
[429 | 2560.68] loss=0.91 avg=1.28
[430 | 2565.85] loss=1.21 avg=1.28
[431 | 2570.97] loss=0.83 avg=1.27
[432 | 2576.10] loss=1.93 avg=1.28
[433 | 2581.17] loss=1.19 avg=1.28
[434 | 2586.22] loss=1.20 avg=1.28
[435 | 2591.31] loss=1.32 avg=1.28
[436 | 2596.43] loss=1.02 avg=1.28
[437 | 2601.63] loss=1.23 avg=1.28
[438 | 2606.68] loss=1.11 avg=1.28
[439 | 2612.02] loss=1.31 avg=1.28
[440 | 2617.08] loss=1.34 avg=1.28
[441 | 2622.34] loss=1.62 avg=1.28
[442 | 2627.54] loss=1.20 avg=1.28
[443 | 2632.73] loss=1.24 avg=1.28
[444 | 2637.92] loss=1.11 avg=1.28
[445 | 2643.21] loss=1.12 avg=1.28
[446 | 2648.40] loss=2.22 avg=1.28
[447 | 2653.54] loss=0.93 avg=1.28
[448 | 2658.74] loss=1.33 avg=1.28
[449 | 2664.00] loss=0.90 avg=1.28
[450 | 2669.20] loss=1.30 avg=1.28
[451 | 2674.31] loss=1.37 avg=1.28
[452 | 2679.48] loss=1.21 avg=1.28
[453 | 2684.73] loss=1.25 avg=1.28
[454 | 2689.82] loss=2.26 avg=1.29
[455 | 2695.03] loss=1.40 avg=1.29
[456 | 2700.19] loss=1.15 avg=1.29
[457 | 2705.48] loss=0.89 avg=1.28
[458 | 2710.60] loss=1.14 avg=1.28
[459 | 2715.80] loss=1.23 avg=1.28
[460 | 2721.11] loss=1.05 avg=1.28
[461 | 2726.54] loss=1.64 avg=1.28
[462 | 2731.80] loss=0.71 avg=1.28
[463 | 2737.09] loss=0.99 avg=1.27
[464 | 2742.38] loss=0.85 avg=1.27
[465 | 2747.57] loss=1.45 avg=1.27
[466 | 2752.89] loss=1.62 avg=1.28
[467 | 2758.09] loss=1.13 avg=1.27
[468 | 2763.36] loss=1.59 avg=1.28
[469 | 2768.83] loss=0.93 avg=1.27
[470 | 2774.08] loss=0.87 avg=1.27
[471 | 2779.46] loss=1.04 avg=1.27
[472 | 2784.96] loss=1.25 avg=1.27
[473 | 2790.25] loss=2.18 avg=1.28
[474 | 2795.42] loss=1.06 avg=1.27
[475 | 2800.55] loss=2.12 avg=1.28
[476 | 2805.84] loss=1.09 avg=1.28
[477 | 2811.09] loss=1.43 avg=1.28
[478 | 2816.23] loss=1.27 avg=1.28
[479 | 2821.38] loss=1.51 avg=1.28
[480 | 2826.63] loss=1.26 avg=1.28
[481 | 2831.74] loss=0.86 avg=1.28
[482 | 2836.91] loss=0.93 avg=1.28
[483 | 2842.07] loss=1.04 avg=1.27
[484 | 2847.11] loss=1.48 avg=1.28
[485 | 2852.18] loss=1.13 avg=1.27
[486 | 2857.46] loss=1.15 avg=1.27
[487 | 2862.74] loss=1.53 avg=1.28
[488 | 2867.86] loss=1.34 avg=1.28
[489 | 2873.05] loss=1.22 avg=1.28
[490 | 2878.16] loss=0.91 avg=1.27
[491 | 2883.29] loss=1.75 avg=1.28
[492 | 2888.56] loss=1.07 avg=1.27
[493 | 2893.81] loss=1.31 avg=1.28
[494 | 2899.02] loss=1.35 avg=1.28
[495 | 2904.22] loss=1.09 avg=1.27
[496 | 2909.41] loss=1.49 avg=1.28
[497 | 2914.56] loss=1.14 avg=1.28
[498 | 2919.92] loss=1.41 avg=1.28
[499 | 2925.12] loss=2.31 avg=1.29
Generating samples...
======== SAMPLE 1 ========
                                                                        )       }    return   train logger.logging (
                                                             )
                       train logger.logger (
                                                    ) train logger.logger_list.join (
                               )
               train logger.logger_list.join(
                 )

    return train logger.logging(
                       )
    def log_list ( self ,       ):
             self .logging = None
             self .logger = None

    for line in self .log_list:
            log .log_list(
                                                                                                  )
             assert len(self._logging) == len(self.log_list)

    assert_numeric( self._logging) != 0
            self._logging.error_message += ' Logging exception found %s.' % self._logging
    assert_numeric( self._logging.message) != 0
           assert_numeric( self._logging.message) != 0

    assert_numeric( self._logging.info) != 0
        assert_numeric( self._logging.status) != 0
        assert_numeric( self._logging.status) != 0
           assert_numeric( self._logging.error) != 0
 for line in self .log_list:
                 assert_numeric(line[0]):
              assert_numeric(line[1]):
                 assert_numeric(line[2]):
                  assert_numeric(line[3]):
                    assert_numeric(line[4]):
                assert_numeric(line[5]):
                assert_numeric(line[5]):
               assert_numeric(line[6]):
                    assert_numeric(line[7]):
              assert

[500 | 3012.51] loss=1.30 avg=1.29
[501 | 3017.78] loss=1.44 avg=1.29
[502 | 3023.11] loss=1.37 avg=1.29
[503 | 3028.38] loss=1.15 avg=1.29
[504 | 3033.61] loss=2.01 avg=1.30
[505 | 3038.79] loss=1.30 avg=1.30
[506 | 3043.93] loss=0.99 avg=1.29
[507 | 3049.13] loss=1.56 avg=1.29
[508 | 3054.32] loss=0.88 avg=1.29
[509 | 3059.55] loss=1.57 avg=1.29
[510 | 3064.61] loss=1.20 avg=1.29
[511 | 3069.90] loss=0.88 avg=1.29
[512 | 3075.09] loss=1.14 avg=1.29
[513 | 3080.25] loss=0.97 avg=1.28
[514 | 3085.55] loss=0.88 avg=1.28
[515 | 3090.90] loss=1.10 avg=1.28
[516 | 3096.14] loss=1.30 avg=1.28
[517 | 3101.48] loss=0.79 avg=1.27
[518 | 3106.65] loss=0.97 avg=1.27
[519 | 3111.84] loss=1.13 avg=1.27
[520 | 3117.01] loss=1.04 avg=1.27
[521 | 3122.15] loss=1.00 avg=1.26
[522 | 3127.32] loss=0.92 avg=1.26
[523 | 3132.52] loss=0.69 avg=1.25
[524 | 3137.74] loss=0.93 avg=1.25
[525 | 3142.94] loss=0.81 avg=1.25
[526 | 3148.15] loss=1.40 avg=1.25
[527 | 3153.16] loss=1.23 avg=1.25
[528 | 3158.46] loss=1.15 avg=1.25
[529 | 3163.65] loss=1.34 avg=1.25
[530 | 3168.91] loss=1.15 avg=1.25
[531 | 3174.26] loss=0.69 avg=1.24
[532 | 3179.75] loss=1.26 avg=1.24
[533 | 3185.25] loss=1.33 avg=1.24
[534 | 3190.76] loss=1.23 avg=1.24
[535 | 3196.09] loss=1.03 avg=1.24
[536 | 3201.40] loss=0.97 avg=1.24
[537 | 3206.72] loss=1.01 avg=1.24
[538 | 3212.09] loss=0.72 avg=1.23
[539 | 3217.35] loss=0.85 avg=1.23
[540 | 3222.67] loss=0.79 avg=1.22
[541 | 3228.10] loss=1.12 avg=1.22
[542 | 3233.56] loss=1.05 avg=1.22
[543 | 3238.90] loss=1.55 avg=1.22
[544 | 3244.22] loss=0.64 avg=1.22
[545 | 3249.48] loss=1.03 avg=1.21
[546 | 3254.82] loss=1.21 avg=1.21
[547 | 3260.17] loss=0.77 avg=1.21
[548 | 3265.49] loss=1.04 avg=1.21
[549 | 3270.85] loss=1.14 avg=1.21
[550 | 3276.27] loss=1.06 avg=1.21
[551 | 3281.62] loss=0.95 avg=1.20
[552 | 3286.98] loss=0.85 avg=1.20
[553 | 3292.30] loss=1.52 avg=1.20
[554 | 3297.59] loss=1.04 avg=1.20
[555 | 3302.96] loss=0.68 avg=1.20
[556 | 3308.30] loss=1.00 avg=1.19
[557 | 3313.58] loss=1.21 avg=1.19
[558 | 3318.91] loss=1.28 avg=1.20
[559 | 3324.14] loss=1.36 avg=1.20
[560 | 3329.52] loss=1.07 avg=1.20
[561 | 3334.85] loss=1.52 avg=1.20
[562 | 3340.26] loss=1.17 avg=1.20
[563 | 3345.72] loss=0.82 avg=1.20
[564 | 3351.04] loss=1.68 avg=1.20
[565 | 3356.54] loss=1.94 avg=1.21
[566 | 3361.81] loss=0.65 avg=1.20
[567 | 3367.11] loss=1.51 avg=1.21
[568 | 3372.44] loss=1.22 avg=1.21
[569 | 3377.72] loss=1.19 avg=1.20
[570 | 3383.00] loss=1.94 avg=1.21
[571 | 3388.28] loss=0.96 avg=1.21
[572 | 3393.62] loss=1.00 avg=1.21
[573 | 3398.92] loss=1.04 avg=1.21
[574 | 3404.19] loss=0.71 avg=1.20
[575 | 3409.48] loss=0.92 avg=1.20
[576 | 3414.79] loss=1.22 avg=1.20
[577 | 3420.08] loss=1.49 avg=1.20
[578 | 3425.52] loss=1.13 avg=1.20
[579 | 3430.83] loss=1.01 avg=1.20
[580 | 3436.17] loss=1.10 avg=1.20
[581 | 3441.50] loss=0.83 avg=1.19
[582 | 3446.91] loss=0.87 avg=1.19
[583 | 3452.30] loss=0.86 avg=1.19
[584 | 3457.61] loss=1.29 avg=1.19
[585 | 3463.00] loss=0.68 avg=1.18
[586 | 3468.31] loss=0.94 avg=1.18
[587 | 3473.63] loss=1.11 avg=1.18
[588 | 3478.95] loss=1.30 avg=1.18
[589 | 3484.14] loss=1.05 avg=1.18
[590 | 3489.29] loss=0.93 avg=1.18
[591 | 3494.43] loss=1.25 avg=1.18
[592 | 3499.63] loss=0.98 avg=1.18
[593 | 3504.96] loss=1.13 avg=1.18
[594 | 3510.31] loss=0.85 avg=1.17
[595 | 3515.62] loss=1.80 avg=1.18
[596 | 3520.87] loss=0.66 avg=1.17
[597 | 3526.19] loss=0.83 avg=1.17
[598 | 3531.52] loss=1.17 avg=1.17
[599 | 3536.71] loss=1.32 avg=1.17
Generating samples...
======== SAMPLE 1 ========
almuar:
        "The first thing that we need to do is to make a separate configuration for the server's API call.
         "This will give those servers a chance to respond to requests like they do for any other service.
         The default for all server side interfaces is to give those services a
         blank API call, regardless of how this is configured.
         """
        request = self.client.get(client_id, server_id)
         if request else None:
            return self.client.get(server_id, client_id)
           else:
             request.start(request)
           self.server = server_id
           request["api-name"] = (requests)
           if self.client.configures(request["client.api.call"]) and self.client.api.call():
             # make this possible instead of having a whole array of requests, and the list does the rest
            response_orderwise
            # and get them into the array
           update_status = self.client.get(server_id, "update_status")
            self.api(update_status)

           if request does not exist:
              return -1
             return (request, update_status) / request
         return (update_status + "updated" + 1):
              ret = server.status.get(update_status)
           if ret > 0 and update_data.status is not None:
             ret = client_get('/update_data',
             request['update_data'].update(update_data).split(update_data) [u'def', u'set_connection', u'(', u'update_id', u'', u',', u'update_ids', u'=', u'None', u')', u':', u'update_data', u'=', u'update_data', u'.', u'set_url', u'(', u'update_id', u',', u'update_ids', u')', u'except', u'as', u'as.str', u':', u'update_data', u'=', u'{', u':', u'update_id', u',}', u'return', u'update_id'] To create a separate configuration for the server's API call.       Default for all server side interfaces. """
        request = self.client.get(client_id, server_id)
         if request else None:
              # This needs to be a single 'update_data' parameter because it can't get a
              update_data instance
             self.client.configure()
              if request else ''.lower():
              update_type = self.client.sync(update_data)
             updates_type = update_type.get('update-type' + update_id)
              if update_data in list(update_name):
                update_data = update_type.get('update-type' + update_id)
      

[600 | 3623.73] loss=1.14 avg=1.17
[601 | 3628.99] loss=1.26 avg=1.17
[602 | 3634.41] loss=0.98 avg=1.17
[603 | 3639.66] loss=1.01 avg=1.17
[604 | 3645.13] loss=1.54 avg=1.17
[605 | 3650.41] loss=1.21 avg=1.17
[606 | 3655.71] loss=0.92 avg=1.17
[607 | 3660.89] loss=1.14 avg=1.17
[608 | 3666.07] loss=0.81 avg=1.17
[609 | 3671.21] loss=1.74 avg=1.17
[610 | 3676.42] loss=2.05 avg=1.18
[611 | 3681.60] loss=1.49 avg=1.18
[612 | 3686.92] loss=1.09 avg=1.18
[613 | 3692.13] loss=1.25 avg=1.18
[614 | 3697.51] loss=0.84 avg=1.18
[615 | 3702.68] loss=1.10 avg=1.18
[616 | 3708.05] loss=1.21 avg=1.18
[617 | 3713.38] loss=1.64 avg=1.18
[618 | 3718.59] loss=1.34 avg=1.19
[619 | 3723.79] loss=0.70 avg=1.18
[620 | 3728.92] loss=1.69 avg=1.19
[621 | 3734.19] loss=0.81 avg=1.18
[622 | 3739.53] loss=1.15 avg=1.18
[623 | 3744.74] loss=1.34 avg=1.18
[624 | 3749.89] loss=1.59 avg=1.19
[625 | 3755.08] loss=0.86 avg=1.18
[626 | 3760.23] loss=1.14 avg=1.18
[627 | 3765.38] loss=0.82 avg=1.18
[628 | 3770.58] loss=1.64 avg=1.19
[629 | 3775.82] loss=1.26 avg=1.19
[630 | 3781.02] loss=1.73 avg=1.19
[631 | 3786.15] loss=1.08 avg=1.19
[632 | 3791.41] loss=1.07 avg=1.19
[633 | 3796.84] loss=1.31 avg=1.19
[634 | 3802.00] loss=1.16 avg=1.19
[635 | 3807.17] loss=0.89 avg=1.19
[636 | 3812.36] loss=1.03 avg=1.19
[637 | 3817.46] loss=1.00 avg=1.18
[638 | 3822.66] loss=1.07 avg=1.18
[639 | 3827.85] loss=1.09 avg=1.18
[640 | 3833.08] loss=1.33 avg=1.18
[641 | 3838.30] loss=0.99 avg=1.18
[642 | 3843.50] loss=1.41 avg=1.18
[643 | 3848.69] loss=0.89 avg=1.18
[644 | 3853.91] loss=1.48 avg=1.18
[645 | 3859.18] loss=1.05 avg=1.18
[646 | 3864.40] loss=1.48 avg=1.18
[647 | 3869.65] loss=1.98 avg=1.19
[648 | 3874.84] loss=0.95 avg=1.19
[649 | 3879.99] loss=0.78 avg=1.19
[650 | 3885.29] loss=1.20 avg=1.19
[651 | 3890.59] loss=1.64 avg=1.19
[652 | 3895.86] loss=0.87 avg=1.19
[653 | 3901.16] loss=1.05 avg=1.19
[654 | 3906.39] loss=0.77 avg=1.18
[655 | 3911.61] loss=1.49 avg=1.19
[656 | 3916.96] loss=1.12 avg=1.18
[657 | 3922.28] loss=0.91 avg=1.18
[658 | 3927.68] loss=1.08 avg=1.18
[659 | 3933.01] loss=0.92 avg=1.18
[660 | 3938.35] loss=0.97 avg=1.18
[661 | 3943.74] loss=1.11 avg=1.18
[662 | 3949.13] loss=0.88 avg=1.17
[663 | 3954.52] loss=1.14 avg=1.17
[664 | 3960.04] loss=1.28 avg=1.17
[665 | 3965.50] loss=0.96 avg=1.17
[666 | 3970.93] loss=1.16 avg=1.17
[667 | 3976.43] loss=1.04 avg=1.17
[668 | 3981.90] loss=1.91 avg=1.18
[669 | 3987.37] loss=1.59 avg=1.18
[670 | 3992.98] loss=0.78 avg=1.18
[671 | 3998.43] loss=1.15 avg=1.18
[672 | 4003.73] loss=0.94 avg=1.17
[673 | 4008.99] loss=1.96 avg=1.18
[674 | 4014.24] loss=0.68 avg=1.18
[675 | 4019.53] loss=1.33 avg=1.18
[676 | 4024.77] loss=1.11 avg=1.18
[677 | 4029.96] loss=1.18 avg=1.18
[678 | 4035.14] loss=1.40 avg=1.18
[679 | 4040.57] loss=1.01 avg=1.18
[680 | 4045.82] loss=1.52 avg=1.18
[681 | 4051.19] loss=1.27 avg=1.18
[682 | 4056.48] loss=1.08 avg=1.18
[683 | 4061.68] loss=1.16 avg=1.18
[684 | 4067.08] loss=1.36 avg=1.18
[685 | 4072.62] loss=1.11 avg=1.18
[686 | 4078.00] loss=1.04 avg=1.18
[687 | 4083.17] loss=1.00 avg=1.18
[688 | 4088.65] loss=1.15 avg=1.18
[689 | 4094.10] loss=1.50 avg=1.18
[690 | 4099.74] loss=1.32 avg=1.18
[691 | 4105.73] loss=1.45 avg=1.19
[692 | 4111.17] loss=1.21 avg=1.19
[693 | 4116.54] loss=1.71 avg=1.19
[694 | 4122.12] loss=0.75 avg=1.19
[695 | 4127.52] loss=1.40 avg=1.19
[696 | 4132.84] loss=1.32 avg=1.19
[697 | 4138.27] loss=0.92 avg=1.19
[698 | 4144.28] loss=1.34 avg=1.19
[699 | 4149.97] loss=0.77 avg=1.19
Generating samples...
======== SAMPLE 1 ========
                                                         (
                                                 )
                                        )
                                                                     )
                                                                                     ) }

        [ { "name": "M. H. Oates" }, { "name": "M. L" } ]   ..
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          

[700 | 4237.24] loss=0.79 avg=1.18
[701 | 4242.43] loss=0.57 avg=1.18
[702 | 4247.58] loss=0.93 avg=1.17
[703 | 4252.78] loss=1.22 avg=1.17
[704 | 4258.00] loss=0.97 avg=1.17
[705 | 4263.12] loss=1.21 avg=1.17
[706 | 4268.27] loss=0.88 avg=1.17
[707 | 4273.41] loss=1.03 avg=1.17
[708 | 4278.71] loss=1.01 avg=1.17
[709 | 4284.12] loss=1.41 avg=1.17
[710 | 4289.30] loss=1.39 avg=1.17
[711 | 4294.56] loss=1.32 avg=1.17
[712 | 4299.75] loss=1.16 avg=1.17
[713 | 4304.93] loss=0.90 avg=1.17
[714 | 4310.09] loss=2.02 avg=1.18
[715 | 4315.21] loss=1.17 avg=1.18
[716 | 4320.29] loss=0.93 avg=1.18
[717 | 4325.42] loss=0.65 avg=1.17
[718 | 4330.52] loss=1.68 avg=1.18
[719 | 4335.70] loss=1.48 avg=1.18
[720 | 4340.80] loss=1.16 avg=1.18
[721 | 4345.98] loss=0.91 avg=1.18
[722 | 4351.14] loss=1.33 avg=1.18
[723 | 4356.30] loss=1.26 avg=1.18
[724 | 4361.53] loss=1.11 avg=1.18
[725 | 4366.71] loss=0.91 avg=1.17
[726 | 4371.90] loss=1.04 avg=1.17
[727 | 4376.99] loss=0.93 avg=1.17
[728 | 4382.15] loss=1.36 avg=1.17
[729 | 4387.41] loss=1.35 avg=1.17
[730 | 4392.61] loss=1.14 avg=1.17
[731 | 4397.86] loss=0.98 avg=1.17
[732 | 4403.03] loss=1.48 avg=1.17
[733 | 4408.17] loss=1.07 avg=1.17
[734 | 4413.27] loss=1.35 avg=1.18
[735 | 4418.43] loss=1.14 avg=1.18
[736 | 4423.78] loss=1.00 avg=1.17
[737 | 4428.96] loss=1.07 avg=1.17
[738 | 4434.20] loss=1.17 avg=1.17
[739 | 4439.34] loss=1.23 avg=1.17
[740 | 4444.58] loss=1.17 avg=1.17
[741 | 4449.75] loss=1.08 avg=1.17
[742 | 4454.93] loss=1.41 avg=1.17
[743 | 4460.18] loss=0.77 avg=1.17
[744 | 4465.36] loss=1.11 avg=1.17
[745 | 4470.64] loss=1.64 avg=1.17
[746 | 4475.90] loss=0.96 avg=1.17
[747 | 4481.03] loss=1.28 avg=1.17
[748 | 4486.21] loss=1.17 avg=1.17
[749 | 4491.32] loss=1.04 avg=1.17
[750 | 4496.51] loss=1.00 avg=1.17
[751 | 4501.62] loss=0.96 avg=1.17
[752 | 4506.80] loss=1.09 avg=1.17
[753 | 4511.91] loss=1.43 avg=1.17
[754 | 4517.00] loss=0.97 avg=1.17
[755 | 4522.12] loss=1.23 avg=1.17
[756 | 4527.24] loss=1.50 avg=1.17
[757 | 4532.31] loss=1.36 avg=1.17
[758 | 4537.45] loss=0.85 avg=1.17
[759 | 4542.60] loss=1.22 avg=1.17
[760 | 4547.76] loss=0.97 avg=1.17
[761 | 4553.01] loss=1.21 avg=1.17
[762 | 4558.20] loss=0.55 avg=1.16
[763 | 4563.34] loss=1.53 avg=1.17
[764 | 4568.58] loss=1.27 avg=1.17
[765 | 4573.70] loss=1.48 avg=1.17
[766 | 4578.83] loss=1.42 avg=1.17
[767 | 4583.95] loss=0.80 avg=1.17
[768 | 4589.05] loss=1.40 avg=1.17
[769 | 4594.18] loss=1.05 avg=1.17
[770 | 4599.25] loss=0.59 avg=1.17
[771 | 4604.39] loss=1.10 avg=1.16
[772 | 4609.41] loss=1.19 avg=1.16
[773 | 4614.53] loss=0.98 avg=1.16
[774 | 4619.61] loss=1.16 avg=1.16
[775 | 4624.74] loss=1.14 avg=1.16
[776 | 4629.86] loss=0.72 avg=1.16
[777 | 4635.06] loss=1.45 avg=1.16
[778 | 4640.23] loss=1.13 avg=1.16
[779 | 4645.47] loss=1.06 avg=1.16
[780 | 4650.64] loss=1.83 avg=1.17
[781 | 4655.83] loss=0.80 avg=1.16
[782 | 4661.04] loss=0.55 avg=1.16
[783 | 4666.19] loss=1.64 avg=1.16
[784 | 4671.38] loss=1.05 avg=1.16
[785 | 4676.52] loss=0.73 avg=1.16
[786 | 4681.62] loss=0.72 avg=1.15
[787 | 4686.77] loss=1.09 avg=1.15
[788 | 4691.97] loss=1.04 avg=1.15
[789 | 4697.15] loss=1.34 avg=1.15
[790 | 4702.24] loss=0.76 avg=1.15
[791 | 4707.46] loss=1.40 avg=1.15
[792 | 4712.63] loss=1.27 avg=1.15
[793 | 4717.80] loss=1.01 avg=1.15
[794 | 4722.97] loss=0.84 avg=1.15
[795 | 4728.19] loss=1.11 avg=1.15
[796 | 4733.46] loss=1.70 avg=1.15
[797 | 4738.62] loss=0.66 avg=1.15
[798 | 4743.58] loss=1.28 avg=1.15
[799 | 4748.63] loss=1.01 avg=1.15
Generating samples...
======== SAMPLE 1 ========
 u'(', u"'{0}'").trim()', u')= u"'{1}'", u'}', u'if', u'if', u'a', u'is', u'not', u'a', u'in', u'', u'.', u'join', u'(', u"'{1}'").join()', u')', u'else', u':', u'if', u'if', u'a', u'not', u'a', u'in', u'.', u'join', u'(', u"'{2}'".format(u'1', u'(', u',', u"'{0}'".format(u'n', u'(', u'a', u'(', u',', u"'{0}'".format(u'c', u'(', u'a', u')', u')', u')', u')', u')', u')', u'else', u':', u'if', u'a', u'not', u'a', u'in', u'.', u'join', u'(', u"'{1}'".format(u'n', u'(', u'a', u')', u')', u')', u'else', u':', u'if', u'a', u'try', u':', u'abort', u'if', u'a', u'not', u'not', u'and', u'not', u'b', u'.', u'notif', u'(', u'a', u.', u'is', u'True', u')', u':', u'a', u'site', u'.', u'add_query_list_from_a_collection', u'(', u'a', u'where', u'a', u'is', u'True', u')', u'b', u'.', u'join', u'(', u',', u'a', u'(', u',', u"'{0}'").join(), u'(', u',', u"'{1}'".format(u'k', u'(', u')', u')', u')', u',', u')', u'else', u':', u'abort', u'if', u'a', u'not', u'any', u'index', u'in', u'a', u'.', u'join', u'(', u'a', u')', u':', u'a', u'[', u'1', u']', u'join', u'(', u',', u'a', u'[', u'1', u']', u')', u'else', u':', u'return', u'[', u'1', u']', u'continue', u'site', u'.', u'add_query_list_from_a_collection', u'(', u'a', u'where', u'a', u'is', u'True', u')', u'b', u'.', u'join', u'(', u',', u'a', u'(', u')', u')', u',', u')', u'] Indexing Example [u'Indexing', u'site', u'.', u'a_collection', u'; u'make_query_list_from_a_collection', u'data_list_of_a_collection', u')', u':', u'make_query_list_from_a_collection', u'(', u'make_query_list_in_a_collection', u')', u'[', u',', u'1', u':', u'index', u']', u'make_query_list_from_a_collection', u'token_list_from_collection_from', u'(', u'make_query_list_in_a_collection', u')', u'[', u',', u'1', u']', u')', u'[', u',', u'1', u']', u'for', u'rend', u'in', u'servers', u':', u'make', u'data_list_from_collection_from_a_collection', u'(', u'make_query_list_in_a_collection', u'(', u'index', u')', u')', u'[', u',', u'1', u"]', u'site', u'.', u'a_collection', u'(', u'make

[800 | 4836.70] loss=0.89 avg=1.14
[801 | 4841.91] loss=1.20 avg=1.15
[802 | 4847.18] loss=1.70 avg=1.15
[803 | 4852.46] loss=1.53 avg=1.15
[804 | 4857.94] loss=1.03 avg=1.15
[805 | 4863.28] loss=1.17 avg=1.15
[806 | 4868.57] loss=0.56 avg=1.15
[807 | 4873.87] loss=1.37 avg=1.15
[808 | 4879.13] loss=0.71 avg=1.15
[809 | 4884.44] loss=1.67 avg=1.15
[810 | 4889.58] loss=0.79 avg=1.15
[811 | 4894.77] loss=0.85 avg=1.14
[812 | 4900.31] loss=0.95 avg=1.14
[813 | 4905.51] loss=1.92 avg=1.15
[814 | 4910.83] loss=1.05 avg=1.15
[815 | 4916.07] loss=1.32 avg=1.15
[816 | 4921.28] loss=1.28 avg=1.15
[817 | 4926.35] loss=1.28 avg=1.15
[818 | 4931.55] loss=0.55 avg=1.15
[819 | 4936.84] loss=1.37 avg=1.15
[820 | 4942.03] loss=0.95 avg=1.15
[821 | 4947.25] loss=1.21 avg=1.15
[822 | 4952.59] loss=1.11 avg=1.15
[823 | 4957.84] loss=1.19 avg=1.15
[824 | 4963.24] loss=1.72 avg=1.15
[825 | 4968.56] loss=1.16 avg=1.15
[826 | 4973.68] loss=1.23 avg=1.15
[827 | 4978.99] loss=1.02 avg=1.15
[828 | 4984.18] loss=0.93 avg=1.15
[829 | 4989.43] loss=0.90 avg=1.15
[830 | 4994.73] loss=0.85 avg=1.15
[831 | 4999.99] loss=0.87 avg=1.14
[832 | 5005.12] loss=0.58 avg=1.14
[833 | 5010.28] loss=0.98 avg=1.14
[834 | 5015.54] loss=1.13 avg=1.14
[835 | 5020.88] loss=1.11 avg=1.14
[836 | 5026.19] loss=0.97 avg=1.13
[837 | 5031.59] loss=1.35 avg=1.14
[838 | 5037.05] loss=1.01 avg=1.13
[839 | 5042.43] loss=1.06 avg=1.13
[840 | 5047.75] loss=0.85 avg=1.13
[841 | 5053.03] loss=0.91 avg=1.13
[842 | 5058.46] loss=0.95 avg=1.13
[843 | 5063.74] loss=1.24 avg=1.13
[844 | 5069.04] loss=1.36 avg=1.13
[845 | 5074.38] loss=1.43 avg=1.13
[846 | 5079.72] loss=0.96 avg=1.13
[847 | 5084.99] loss=1.39 avg=1.13
[848 | 5090.29] loss=1.14 avg=1.13
[849 | 5095.51] loss=1.32 avg=1.14
[850 | 5100.90] loss=1.03 avg=1.14
[851 | 5106.16] loss=0.89 avg=1.13
[852 | 5111.41] loss=1.50 avg=1.14
[853 | 5116.69] loss=0.91 avg=1.13
[854 | 5121.89] loss=0.87 avg=1.13
[855 | 5127.11] loss=0.75 avg=1.13
[856 | 5132.24] loss=1.38 avg=1.13
[857 | 5137.86] loss=1.64 avg=1.14
[858 | 5144.87] loss=0.86 avg=1.13
[859 | 5150.56] loss=1.24 avg=1.13
[860 | 5156.22] loss=0.99 avg=1.13
[861 | 5161.72] loss=1.18 avg=1.13
[862 | 5167.31] loss=1.11 avg=1.13
[863 | 5172.95] loss=1.31 avg=1.13
[864 | 5178.93] loss=1.70 avg=1.14
[865 | 5184.19] loss=0.68 avg=1.14
[866 | 5189.63] loss=1.24 avg=1.14
[867 | 5194.67] loss=0.96 avg=1.13
[868 | 5199.59] loss=1.24 avg=1.14
[869 | 5204.66] loss=1.04 avg=1.13
[870 | 5209.65] loss=1.53 avg=1.14
[871 | 5214.59] loss=1.42 avg=1.14
[872 | 5219.56] loss=1.35 avg=1.14
[873 | 5224.51] loss=1.35 avg=1.15
[874 | 5229.59] loss=0.97 avg=1.14
[875 | 5234.57] loss=1.06 avg=1.14
[876 | 5239.57] loss=0.92 avg=1.14
[877 | 5244.52] loss=1.30 avg=1.14
[878 | 5249.62] loss=0.91 avg=1.14
[879 | 5254.70] loss=1.87 avg=1.15
[880 | 5259.85] loss=1.52 avg=1.15
[881 | 5264.94] loss=0.75 avg=1.15
[882 | 5270.10] loss=1.14 avg=1.15
[883 | 5275.11] loss=1.54 avg=1.15
[884 | 5280.22] loss=1.05 avg=1.15
[885 | 5285.25] loss=1.03 avg=1.15
[886 | 5290.39] loss=1.22 avg=1.15
[887 | 5295.49] loss=0.81 avg=1.15
[888 | 5300.60] loss=1.11 avg=1.15
[889 | 5305.63] loss=1.27 avg=1.15
[890 | 5310.74] loss=1.55 avg=1.15
[891 | 5315.83] loss=1.17 avg=1.15
[892 | 5320.95] loss=0.92 avg=1.15
[893 | 5326.02] loss=1.47 avg=1.15
[894 | 5331.23] loss=0.97 avg=1.15
[895 | 5336.38] loss=0.73 avg=1.15
[896 | 5341.59] loss=1.18 avg=1.15
[897 | 5346.83] loss=1.23 avg=1.15
[898 | 5352.01] loss=0.82 avg=1.14
[899 | 5357.28] loss=1.10 avg=1.14
Generating samples...
======== SAMPLE 1 ========
     """
                          set_state = state.set_state()
                                if setstate:
                                                                         set_state
                                      )

                  if set_state:
                                                 return _from_list((list))

             if set_state else:
                        set_state = list.all(dict)

                                 if set_state:
                                             _from_list((list)
                                 set_state)

                          if set_state:
                                                               set_state

                   if set_state:
                                       )

                          return
       if __opts__.equals(list, __opts__.size()):
                        set_state = list[0]

                      if set_state:
                                    set_state.set_state()

                                return
      return set_state train/compute.py apache2/utils/get_tls_by_dict.py apache2/utils/get_tls_by_dict.py my/models.json python def get_tls_by_dict(dict):
        """
        Get list of key-value pairs.
        _list() : dict
                                                                 
                                                   

[900 | 5443.98] loss=0.90 avg=1.14
[901 | 5449.12] loss=1.27 avg=1.14
[902 | 5454.25] loss=1.21 avg=1.14
[903 | 5459.43] loss=1.57 avg=1.15
[904 | 5464.62] loss=1.09 avg=1.15
[905 | 5469.75] loss=1.36 avg=1.15
[906 | 5474.95] loss=1.43 avg=1.15
[907 | 5480.08] loss=1.19 avg=1.15
[908 | 5485.36] loss=0.99 avg=1.15
[909 | 5490.54] loss=1.42 avg=1.15
[910 | 5495.71] loss=1.35 avg=1.15
[911 | 5500.99] loss=1.55 avg=1.16
[912 | 5506.12] loss=1.23 avg=1.16
[913 | 5511.28] loss=0.79 avg=1.16
[914 | 5516.56] loss=1.00 avg=1.15
[915 | 5521.79] loss=0.87 avg=1.15
[916 | 5526.96] loss=1.47 avg=1.15
[917 | 5532.06] loss=1.26 avg=1.16
[918 | 5537.17] loss=0.83 avg=1.15
[919 | 5542.38] loss=1.13 avg=1.15
[920 | 5547.58] loss=1.12 avg=1.15
[921 | 5552.70] loss=1.16 avg=1.15
[922 | 5557.91] loss=1.11 avg=1.15
[923 | 5563.17] loss=1.12 avg=1.15
[924 | 5568.34] loss=2.03 avg=1.16
[925 | 5573.58] loss=0.93 avg=1.16
[926 | 5578.78] loss=0.66 avg=1.15
[927 | 5584.06] loss=0.82 avg=1.15
[928 | 5589.28] loss=1.02 avg=1.15
[929 | 5594.46] loss=1.08 avg=1.15
[930 | 5599.82] loss=1.17 avg=1.15
[931 | 5604.98] loss=1.47 avg=1.15
[932 | 5610.22] loss=1.22 avg=1.15
[933 | 5615.61] loss=0.71 avg=1.15
[934 | 5620.79] loss=1.28 avg=1.15
[935 | 5626.06] loss=1.09 avg=1.15
[936 | 5631.26] loss=0.99 avg=1.15
[937 | 5636.51] loss=1.75 avg=1.15
[938 | 5641.84] loss=1.02 avg=1.15
[939 | 5646.92] loss=1.36 avg=1.15
[940 | 5652.11] loss=0.86 avg=1.15
[941 | 5657.22] loss=1.32 avg=1.15
[942 | 5662.41] loss=0.79 avg=1.15
[943 | 5667.76] loss=0.93 avg=1.15
[944 | 5673.07] loss=1.28 avg=1.15
[945 | 5678.22] loss=1.04 avg=1.15
[946 | 5683.29] loss=1.22 avg=1.15
[947 | 5688.63] loss=0.72 avg=1.14
[948 | 5693.76] loss=1.41 avg=1.15
[949 | 5698.93] loss=1.07 avg=1.14
[950 | 5704.12] loss=0.74 avg=1.14
[951 | 5709.31] loss=0.90 avg=1.14
[952 | 5714.49] loss=1.14 avg=1.14
[953 | 5719.62] loss=1.12 avg=1.14
[954 | 5724.88] loss=1.15 avg=1.14
[955 | 5730.06] loss=1.22 avg=1.14
[956 | 5735.26] loss=1.06 avg=1.14
[957 | 5740.36] loss=1.00 avg=1.14
[958 | 5745.55] loss=0.80 avg=1.13
[959 | 5750.66] loss=1.30 avg=1.14
[960 | 5755.89] loss=1.01 avg=1.13
[961 | 5761.03] loss=1.36 avg=1.14
[962 | 5766.27] loss=1.27 avg=1.14
[963 | 5771.55] loss=1.19 avg=1.14
[964 | 5776.72] loss=1.35 avg=1.14
[965 | 5781.88] loss=1.63 avg=1.15
[966 | 5787.21] loss=0.99 avg=1.14
[967 | 5792.47] loss=0.92 avg=1.14
[968 | 5797.68] loss=1.15 avg=1.14
[969 | 5802.81] loss=0.81 avg=1.14
[970 | 5807.97] loss=0.95 avg=1.14
[971 | 5813.21] loss=1.21 avg=1.14
[972 | 5818.44] loss=0.80 avg=1.13
[973 | 5823.72] loss=1.05 avg=1.13
[974 | 5828.93] loss=1.07 avg=1.13
[975 | 5834.15] loss=1.10 avg=1.13
[976 | 5839.32] loss=0.83 avg=1.13
[977 | 5844.54] loss=0.78 avg=1.13
[978 | 5849.72] loss=0.93 avg=1.12
[979 | 5854.92] loss=0.74 avg=1.12
[980 | 5860.19] loss=0.82 avg=1.12
[981 | 5865.41] loss=0.83 avg=1.11
[982 | 5870.65] loss=1.64 avg=1.12
[983 | 5875.73] loss=0.87 avg=1.12
[984 | 5880.96] loss=1.03 avg=1.12
[985 | 5886.16] loss=1.21 avg=1.12
[986 | 5891.21] loss=0.85 avg=1.11
[987 | 5896.46] loss=1.17 avg=1.11
[988 | 5901.64] loss=0.94 avg=1.11
[989 | 5906.83] loss=0.32 avg=1.10
[990 | 5912.10] loss=0.78 avg=1.10
[991 | 5917.25] loss=1.21 avg=1.10
[992 | 5922.53] loss=0.77 avg=1.10
[993 | 5927.82] loss=1.08 avg=1.10
[994 | 5933.04] loss=1.22 avg=1.10
[995 | 5938.24] loss=0.66 avg=1.10
[996 | 5943.35] loss=0.73 avg=1.09
[997 | 5948.56] loss=0.95 avg=1.09
[998 | 5953.76] loss=1.16 avg=1.09
[999 | 5958.95] loss=1.62 avg=1.10
Saving checkpoint/run1/model-1000
Generating samples...
======== SAMPLE 1 ========
__id_for_self"
        if isinstance (self, dict):
            """
         if self.find_sub(self.id, id) != None:
            self.update_sub(self.id, self.id)

     return _update_from_state(self.id, self.id) [u'def', u'update_from_state_str', u'(', u'self', u',', u'self', u',', u'region', u'=', u'None', u',', u'source', u'=', u'source', u')', u'(', u'# TODO: Add salt/salt-dev/salt-master/utils.py', u')'] Update from state of the current entity.

    if self.id == self.id:
        raise SaltBnB._send_message(
                {'*'{', u'{', u"'_id'", u',', u'id', u`', u'}', u'}', u'}', u'else', u':', u'raise SaltBnB._send_message(', u"'_id'", u'{'", u"'_id'", u'+', u'`', u'}']', u'}", u'else', u':', u'raise SaltBnB.', u'send_message', u'(', u"'_id'", u'{', u"'_id'", u'+', u'source', u'}", u')']

    if self.id in get_id:
        raise SaltBnB._send_message(
             {', u"'*'{', u"'_id'", u',', u"id", u'=', u'get_id', u'', u'(', u')', u'/', u'"''", u'}', u"'*'{', u"'_id'", u"'+', u"'_id'", u'"'", u']', u'}', u')', u'return', u'_update_from_state(', u',', u'self', u',', u'id',', u'[', u"'_id'", u']', u',', u'id',', u'[', u"'_id'", u']', u']', u')', u'[', u"'id'", u']', u'.', u'get', u'(', u'(', u',', u'id',', u')', u')', u'self', u'=', u'get_state_str', u'[', u"'id'", u']', u'self', u'try', u'=', u'None', u'region', u'=', u'try', u',', u'region', u'[', u"'_id'", u']', u'.', u"'_id', u'', u'=', u'self', u')', u'else', u':', u'try', u'=', u'None', u'self', u'[', u"'*'", u']', u'in', u'get_id', u',', u'[', u"'_id'", u']', u'.', u'get', u'(', u'(', u',', u'id',', u')', u'self', u'[', u"'id'", u']', u',', u'id',', u'[', u"'_id'", u']', u']', u'source', u'=', u'source', u')', u'return', u'_update_from_state_str', u'(', u'self', u',', u'region', u')', u'versionpy', u'.', u'salt', u'.', u'dict', u'self', u'[', u"'id'", u']', u'.', u'dicode', u'.', u'get', u'(', u'(', u')', u')', u')', u'if', u'self', u'.', u'tis_not_valid', u'(',

[1000 | 6047.65] loss=1.05 avg=1.10
[1001 | 6052.85] loss=1.63 avg=1.10
[1002 | 6058.07] loss=0.89 avg=1.10
[1003 | 6063.26] loss=0.59 avg=1.09
[1004 | 6068.52] loss=1.04 avg=1.09
[1005 | 6073.77] loss=0.58 avg=1.09
[1006 | 6079.05] loss=1.24 avg=1.09
[1007 | 6084.31] loss=1.12 avg=1.09
[1008 | 6089.52] loss=1.32 avg=1.09
[1009 | 6094.65] loss=1.63 avg=1.10
[1010 | 6099.82] loss=1.25 avg=1.10
[1011 | 6104.99] loss=1.51 avg=1.10
[1012 | 6110.09] loss=0.59 avg=1.10
[1013 | 6115.34] loss=0.98 avg=1.10
[1014 | 6120.52] loss=0.91 avg=1.10
[1015 | 6125.75] loss=1.06 avg=1.10
[1016 | 6131.00] loss=1.13 avg=1.10
[1017 | 6136.13] loss=0.86 avg=1.09
[1018 | 6141.47] loss=1.33 avg=1.10
[1019 | 6146.66] loss=1.41 avg=1.10
[1020 | 6151.92] loss=1.01 avg=1.10
[1021 | 6157.18] loss=1.01 avg=1.10
[1022 | 6162.48] loss=0.99 avg=1.10
[1023 | 6167.70] loss=1.15 avg=1.10
[1024 | 6172.95] loss=0.84 avg=1.09
[1025 | 6178.11] loss=0.91 avg=1.09
[1026 | 6183.41] loss=1.11 avg=1.09
[1027 | 6188.62] loss=0.75 avg=1.09
[1028 | 6193.86] loss=1.18 avg=1.09
[1029 | 6199.16] loss=1.11 avg=1.09
[1030 | 6204.43] loss=0.83 avg=1.09
[1031 | 6209.78] loss=1.01 avg=1.09
[1032 | 6214.99] loss=0.95 avg=1.09
[1033 | 6220.28] loss=0.92 avg=1.08
[1034 | 6225.46] loss=1.16 avg=1.08
[1035 | 6230.69] loss=1.10 avg=1.08
[1036 | 6236.03] loss=1.00 avg=1.08
[1037 | 6241.35] loss=1.02 avg=1.08
[1038 | 6246.70] loss=1.62 avg=1.09
[1039 | 6252.00] loss=0.99 avg=1.09
[1040 | 6257.31] loss=1.15 avg=1.09
[1041 | 6262.47] loss=2.05 avg=1.10
[1042 | 6267.72] loss=0.94 avg=1.10
[1043 | 6272.99] loss=0.80 avg=1.09
[1044 | 6278.17] loss=0.85 avg=1.09
[1045 | 6283.43] loss=1.15 avg=1.09
[1046 | 6288.71] loss=1.06 avg=1.09
[1047 | 6293.92] loss=1.33 avg=1.09
[1048 | 6299.24] loss=0.93 avg=1.09
[1049 | 6304.46] loss=1.42 avg=1.10
[1050 | 6309.73] loss=0.78 avg=1.09
[1051 | 6314.94] loss=1.09 avg=1.09
[1052 | 6320.23] loss=1.36 avg=1.09
[1053 | 6325.46] loss=1.20 avg=1.10
[1054 | 6330.86] loss=1.60 avg=1.10
[1055 | 6336.23] loss=1.11 avg=1.10
[1056 | 6341.48] loss=1.20 avg=1.10
[1057 | 6346.78] loss=1.15 avg=1.10
[1058 | 6351.98] loss=1.45 avg=1.11
[1059 | 6357.29] loss=0.77 avg=1.10
[1060 | 6362.48] loss=1.17 avg=1.10
[1061 | 6367.65] loss=1.31 avg=1.11
[1062 | 6372.92] loss=0.84 avg=1.10
[1063 | 6378.54] loss=0.98 avg=1.10
[1064 | 6383.85] loss=1.77 avg=1.11
[1065 | 6389.00] loss=1.37 avg=1.11
[1066 | 6394.10] loss=0.58 avg=1.11
[1067 | 6399.27] loss=1.34 avg=1.11
[1068 | 6404.46] loss=0.89 avg=1.11
[1069 | 6409.64] loss=1.14 avg=1.11
[1070 | 6414.80] loss=0.63 avg=1.10
[1071 | 6419.97] loss=1.73 avg=1.11
[1072 | 6425.22] loss=1.40 avg=1.11
[1073 | 6430.46] loss=0.97 avg=1.11
[1074 | 6435.65] loss=1.38 avg=1.11
[1075 | 6440.83] loss=0.70 avg=1.11
[1076 | 6446.09] loss=0.66 avg=1.10
[1077 | 6451.32] loss=1.15 avg=1.10
[1078 | 6456.56] loss=0.99 avg=1.10
[1079 | 6461.76] loss=1.10 avg=1.10
[1080 | 6467.01] loss=1.00 avg=1.10
[1081 | 6472.19] loss=0.72 avg=1.10
[1082 | 6477.40] loss=0.78 avg=1.09
[1083 | 6482.55] loss=1.54 avg=1.10
[1084 | 6487.88] loss=1.31 avg=1.10
[1085 | 6493.05] loss=0.82 avg=1.10
[1086 | 6498.48] loss=0.56 avg=1.09
[1087 | 6503.73] loss=0.82 avg=1.09
[1088 | 6509.07] loss=0.83 avg=1.09
[1089 | 6514.42] loss=1.26 avg=1.09
[1090 | 6519.69] loss=0.88 avg=1.09
[1091 | 6524.87] loss=0.98 avg=1.09
[1092 | 6530.28] loss=1.60 avg=1.09
[1093 | 6535.55] loss=1.40 avg=1.09
[1094 | 6541.16] loss=0.78 avg=1.09
[1095 | 6546.63] loss=1.38 avg=1.09
[1096 | 6551.97] loss=1.21 avg=1.10
[1097 | 6557.38] loss=1.37 avg=1.10
[1098 | 6563.08] loss=0.98 avg=1.10
[1099 | 6568.36] loss=0.81 avg=1.09
Generating samples...
======== SAMPLE 1 ========
is'', u'(', u'"pam'", u')', u'.', u'get_output(), u'(', u"'data'", u',', u'DataEncoder', u'.', u'Encoder', u'.', u'SetPAM', u'(', u'pAM', u'(', u"'pam'", u'[', u"'cwd'", u']', u')', u')', u')', u'.', u'start_file(), u'.', u'get_current_output(), u'(', u'[', u"'file'", u']', u',', u"'data'", u',', u'Data', u'.', u'SetCachedSuffix(', u'cached_suffix', u'(', u')', u',', u'0', u')', u',', u'', u'# Check if a valid, cached SSP was provided to us', u'# with that SSP', u'start_file(), u'.', u'check_sx_with_cached_ssp_sps_(', u'True', u')', u'# Get the source's current cached SSP so we can use it', u'# as the destination, but don't forget to check for the cached SSP', u'# or at least, not if we have a cached SSP in there.', u'assert_fail_with_cached_ssp_sps_file(cached_ssp, False', u')', u'return', u',', u'start_file', u'(', u'None', u')', u'}', u'# TODO: Check if any cache in our destination had been built for a valid SSP', u'# so we can validate the SSP and the cached SSP', u'if', u'!None', u'start_file', u'(', u'start_file', u')', u'if', u'start_file', u'[', u']', u'.', u'get_backend_metadata', u'(', u"'metadata'", u',', u"'metadata'", u',', u"'backend'", u',', u'/', u'.', u'format_file()', u'[', u"'backend'", u']', u',', u"'metadata'", u',', u'/', u''.', u'format_file()', u')', u'# If a cached SSP was provided, check when we're done with the SSP', u'# with the SSP', u'# with the cached SSP', u'# or we're done with the SSP', u'# but get back the SSP with the new caching', u'# after the SSP', u'#', u'# if we had a cached SSP', u'# then continue', u'def', u'get_backend_metadata', u'(', u"'metadata'", u',', u'', u'"'backend'", u',', u'/', u"'.', u'format_file()', u'[', u"'backend'", u']', u',', u'/', u'.', u'format_file()', u'[', u"'backend'", u']', u',', u'/', u'.', u'format_file_set('['backend'', u']', u')', u'if', u"'metadata'", u'==', u"'backend'", u':', u'metadata', u'=', u'_getattr_to_data(', u',', u"'metadata'", u',', u'False', u')', u'or', u"'metadata'", u'==', u"'backend'", u':', u"'metadata'", u'=', u'_getattr_to_data()'', u'or', u'_getattr_to_data(', u'[', u"'metadata'", u']', u',', u"'metadata'", u',', u'False', u')', u'(', u'metadata', u')'] Returns : A Tuple<', u'the-cached-ssp> whose ssp was cached by the SSP(ssp_id=ssp_id is a Tuple<', u'the-cache-metadata>) or by the cache that contains the SSP, regardless of whether that SSP is in a SSP(ssp_id=ssp_id) or

[1100 | 6654.72] loss=1.21 avg=1.10
[1101 | 6659.84] loss=1.22 avg=1.10
[1102 | 6665.13] loss=0.58 avg=1.09
[1103 | 6670.45] loss=0.92 avg=1.09
[1104 | 6675.70] loss=0.68 avg=1.09
[1105 | 6680.92] loss=0.91 avg=1.08
[1106 | 6686.08] loss=0.91 avg=1.08
[1107 | 6691.26] loss=1.03 avg=1.08
[1108 | 6696.56] loss=1.02 avg=1.08
[1109 | 6701.80] loss=0.78 avg=1.08
[1110 | 6707.04] loss=1.56 avg=1.08
[1111 | 6713.13] loss=1.17 avg=1.08
[1112 | 6718.89] loss=1.32 avg=1.09
[1113 | 6724.40] loss=0.83 avg=1.08
[1114 | 6729.52] loss=1.50 avg=1.09
[1115 | 6734.80] loss=0.74 avg=1.08
[1116 | 6740.10] loss=0.93 avg=1.08
[1117 | 6745.42] loss=0.71 avg=1.08
[1118 | 6750.68] loss=1.59 avg=1.08
[1119 | 6755.82] loss=0.87 avg=1.08
[1120 | 6761.15] loss=0.98 avg=1.08
[1121 | 6766.74] loss=0.74 avg=1.08
[1122 | 6772.15] loss=0.93 avg=1.08
[1123 | 6777.47] loss=0.90 avg=1.07
[1124 | 6782.92] loss=0.67 avg=1.07
[1125 | 6788.20] loss=0.93 avg=1.07
[1126 | 6793.52] loss=1.17 avg=1.07
[1127 | 6798.89] loss=1.07 avg=1.07
[1128 | 6804.49] loss=0.90 avg=1.07
[1129 | 6809.90] loss=0.87 avg=1.07
[1130 | 6815.26] loss=0.79 avg=1.06
[1131 | 6820.75] loss=1.47 avg=1.07
[1132 | 6826.04] loss=0.92 avg=1.07
[1133 | 6831.33] loss=1.21 avg=1.07
[1134 | 6836.63] loss=1.09 avg=1.07
[1135 | 6841.88] loss=1.09 avg=1.07
[1136 | 6847.25] loss=1.11 avg=1.07
[1137 | 6852.57] loss=1.07 avg=1.07
[1138 | 6858.03] loss=1.10 avg=1.07
[1139 | 6863.62] loss=1.89 avg=1.08
[1140 | 6869.09] loss=1.18 avg=1.08
[1141 | 6874.52] loss=0.99 avg=1.08
[1142 | 6879.84] loss=1.18 avg=1.08
[1143 | 6885.18] loss=1.06 avg=1.08
[1144 | 6890.46] loss=2.08 avg=1.09
[1145 | 6895.77] loss=1.19 avg=1.09
[1146 | 6901.25] loss=1.38 avg=1.09
[1147 | 6906.70] loss=2.01 avg=1.10
[1148 | 6912.09] loss=0.74 avg=1.10
[1149 | 6917.40] loss=0.93 avg=1.10
[1150 | 6922.76] loss=1.05 avg=1.09
[1151 | 6928.10] loss=1.31 avg=1.10
[1152 | 6933.33] loss=1.78 avg=1.10
[1153 | 6938.55] loss=0.94 avg=1.10
[1154 | 6943.80] loss=1.44 avg=1.11
[1155 | 6949.15] loss=1.10 avg=1.11
[1156 | 6954.47] loss=1.35 avg=1.11
[1157 | 6959.65] loss=0.80 avg=1.11
[1158 | 6964.98] loss=1.36 avg=1.11
[1159 | 6970.37] loss=1.04 avg=1.11
[1160 | 6975.86] loss=0.60 avg=1.10
[1161 | 6981.18] loss=1.57 avg=1.11
[1162 | 6986.70] loss=0.95 avg=1.11
[1163 | 6992.10] loss=1.06 avg=1.10
[1164 | 6997.43] loss=1.62 avg=1.11
[1165 | 7002.77] loss=1.43 avg=1.11
[1166 | 7008.11] loss=1.32 avg=1.12
[1167 | 7013.54] loss=1.03 avg=1.11
[1168 | 7018.97] loss=0.89 avg=1.11
[1169 | 7024.35] loss=1.09 avg=1.11
[1170 | 7029.83] loss=0.93 avg=1.11
[1171 | 7035.31] loss=0.82 avg=1.11
[1172 | 7040.86] loss=1.40 avg=1.11
[1173 | 7046.26] loss=1.30 avg=1.11
[1174 | 7051.90] loss=0.90 avg=1.11
[1175 | 7057.28] loss=1.02 avg=1.11
[1176 | 7062.77] loss=1.09 avg=1.11
[1177 | 7068.05] loss=0.54 avg=1.10
[1178 | 7073.60] loss=1.01 avg=1.10
[1179 | 7079.16] loss=1.08 avg=1.10
[1180 | 7084.50] loss=1.12 avg=1.10
[1181 | 7089.93] loss=0.85 avg=1.10
[1182 | 7095.27] loss=1.38 avg=1.10
[1183 | 7100.82] loss=1.44 avg=1.11
[1184 | 7106.14] loss=1.33 avg=1.11
[1185 | 7111.40] loss=1.26 avg=1.11
[1186 | 7116.71] loss=1.02 avg=1.11
[1187 | 7122.02] loss=1.04 avg=1.11
[1188 | 7127.32] loss=1.11 avg=1.11
[1189 | 7132.64] loss=1.37 avg=1.11
[1190 | 7138.02] loss=1.80 avg=1.12
[1191 | 7143.21] loss=0.85 avg=1.11
[1192 | 7148.45] loss=0.60 avg=1.11
[1193 | 7153.75] loss=1.79 avg=1.12
[1194 | 7159.08] loss=0.93 avg=1.11
[1196 | 8.64] loss=0.91 avg=0.91
[1197 | 13.84] loss=1.22 avg=1.07
[1198 | 19.04] loss=1.26 avg=1.13
[1199 | 24.23] loss=1.10 avg=1.12
Generating samples...
======== SAMPLE 1 ========
entials:
                                       passwd                                                               authuser                                             passwd                                         # disable autofill if not enabled)
                                                           passy - 1 )
                                            **username, **password.strip if not isinstance(username, dict)):
                        passy = [username, passwd, salt['password'] if salt['auth'][username] == PASSWORD_USERAGENT if salt['passwd'][username] else passy, passwd
                                   passy.append(self.username, passwd)
                                         passy.append(passuid, salt['passwd'])

                            passy.append(passuid, authuser, salt['passwd']))

                           passy.append(passuid, authuser, # disable autofill if not enabled)

                               passy.append(passuid, passuiduser, salt['passwd'])
                               passy.append(passuid, passuiduser, authuiduser)
                               passy.append(passuid, passuiduser, authuiduser))
                            passy.append(passuid, passuiduser, # disable auto-admin if not enabled)
                              passy.append(passuid, passuiduser, authuiduser)

                           passy.append(passuid, passuiduser)
                                passy.append(passuid, passuiduser)
                                   passy.append(passuid, passuiduser)

                               passy.append(passuid, passuiduser)
                       

[1200 | 107.24] loss=0.93 avg=1.08
[1201 | 112.45] loss=1.23 avg=1.11
[1202 | 117.67] loss=1.20 avg=1.12
[1203 | 122.84] loss=1.03 avg=1.11
[1204 | 128.15] loss=0.88 avg=1.08
[1205 | 133.40] loss=0.98 avg=1.07
[1206 | 138.62] loss=0.95 avg=1.06
[1207 | 143.80] loss=1.17 avg=1.07
[1208 | 148.98] loss=0.99 avg=1.06
[1209 | 154.19] loss=0.79 avg=1.04
[1210 | 159.46] loss=0.99 avg=1.04
[1211 | 164.76] loss=1.37 avg=1.06
[1212 | 170.18] loss=0.97 avg=1.06
[1213 | 175.53] loss=0.79 avg=1.04
[1214 | 180.90] loss=1.10 avg=1.04
[1215 | 186.39] loss=1.19 avg=1.05
[1216 | 191.78] loss=0.59 avg=1.03
[1217 | 197.26] loss=1.32 avg=1.04
[1218 | 202.66] loss=1.14 avg=1.05
[1219 | 208.09] loss=0.85 avg=1.04
[1220 | 213.43] loss=1.06 avg=1.04
[1221 | 218.77] loss=1.36 avg=1.05
[1222 | 224.12] loss=1.62 avg=1.08
[1223 | 229.51] loss=1.76 avg=1.10
[1224 | 234.89] loss=1.33 avg=1.11
[1225 | 240.21] loss=0.63 avg=1.09
[1226 | 245.62] loss=1.40 avg=1.11
[1227 | 250.98] loss=1.04 avg=1.10
[1228 | 256.29] loss=1.40 avg=1.11
[1229 | 261.74] loss=0.37 avg=1.09
[1230 | 267.22] loss=1.07 avg=1.09
[1231 | 272.69] loss=1.30 avg=1.09
[1232 | 278.18] loss=1.43 avg=1.10
[1233 | 283.70] loss=1.20 avg=1.11
[1234 | 289.23] loss=0.85 avg=1.10
[1235 | 294.81] loss=0.88 avg=1.09
[1236 | 300.32] loss=1.61 avg=1.11
[1237 | 305.81] loss=1.61 avg=1.12
[1238 | 311.17] loss=1.65 avg=1.14
[1239 | 316.56] loss=1.02 avg=1.13
[1240 | 322.05] loss=1.20 avg=1.14
[1241 | 327.49] loss=0.89 avg=1.13
[1242 | 332.96] loss=0.76 avg=1.12
[1243 | 338.48] loss=0.83 avg=1.11
[1244 | 343.90] loss=1.81 avg=1.13
[1245 | 349.42] loss=1.14 avg=1.13
[1246 | 354.82] loss=0.91 avg=1.13
[1247 | 360.17] loss=1.35 avg=1.13
[1248 | 365.56] loss=1.40 avg=1.14
[1249 | 370.93] loss=0.90 avg=1.13
[1250 | 376.29] loss=0.88 avg=1.13
[1251 | 381.78] loss=1.11 avg=1.13
[1252 | 387.24] loss=0.68 avg=1.12
[1253 | 392.73] loss=1.53 avg=1.12
[1254 | 398.25] loss=0.71 avg=1.12
[1255 | 403.62] loss=0.67 avg=1.11
[1256 | 409.32] loss=1.16 avg=1.11
[1257 | 414.72] loss=1.16 avg=1.11
[1258 | 420.35] loss=0.88 avg=1.10
[1259 | 425.79] loss=1.33 avg=1.11
[1260 | 431.21] loss=0.79 avg=1.10
[1261 | 436.53] loss=1.07 avg=1.10
[1262 | 441.96] loss=1.86 avg=1.12
[1263 | 447.41] loss=0.76 avg=1.11
[1264 | 452.87] loss=1.13 avg=1.11
[1265 | 458.34] loss=0.89 avg=1.10
[1266 | 463.78] loss=1.31 avg=1.11
[1267 | 469.29] loss=1.88 avg=1.12
[1268 | 474.76] loss=0.76 avg=1.12
[1269 | 480.17] loss=0.95 avg=1.11
[1270 | 485.58] loss=0.75 avg=1.11
[1271 | 490.95] loss=0.96 avg=1.10
[1272 | 496.31] loss=1.84 avg=1.12
[1273 | 501.70] loss=0.84 avg=1.11
[1274 | 506.98] loss=0.96 avg=1.11
[1275 | 512.31] loss=1.11 avg=1.11
[1276 | 517.76] loss=0.79 avg=1.10
[1277 | 523.16] loss=0.85 avg=1.10
[1278 | 528.58] loss=1.30 avg=1.10
[1279 | 533.80] loss=0.98 avg=1.10
[1280 | 539.18] loss=1.68 avg=1.11
[1281 | 544.42] loss=1.25 avg=1.11
[1282 | 549.74] loss=1.19 avg=1.11
[1283 | 554.99] loss=0.77 avg=1.11
[1284 | 560.30] loss=0.78 avg=1.10
[1285 | 565.60] loss=1.36 avg=1.11
[1286 | 570.85] loss=1.51 avg=1.11
[1287 | 576.20] loss=0.68 avg=1.11
[1288 | 581.48] loss=1.03 avg=1.11
[1289 | 586.77] loss=0.86 avg=1.10
[1290 | 591.96] loss=1.36 avg=1.11
[1291 | 597.29] loss=0.55 avg=1.10
[1292 | 602.58] loss=1.64 avg=1.11
[1293 | 607.84] loss=0.79 avg=1.10
[1294 | 613.16] loss=0.90 avg=1.10
[1295 | 618.54] loss=1.39 avg=1.10
[1296 | 623.69] loss=0.85 avg=1.10
[1297 | 628.99] loss=1.34 avg=1.10
[1298 | 634.25] loss=1.11 avg=1.10
[1299 | 639.40] loss=0.88 avg=1.10
Generating samples...
======== SAMPLE 1 ========
' to a function, then return None.
    :type = 'string'
     :type = self.parse_string(lambda x : (string x)
     :type = 'dict').split().replace('
           '='', '.', '/*'
    :type = 'str'
    :type = self.parse_str(lambda x : (str x)
    :type = 'str'
    :type = 'int'

    :method = ['string'], args = {}

    :result = ['string'], 'result'
    :returns = {}

    :class = []

   :value = []

   :output = [True, False]

   :type = ['string'], [']
   :result = {'message': '{}', }'>
    :type = 'str'
   :type = self.parse_str(lambda x : (str x)
    :type = 'str'
    :type = 'string': '{}'}
    :type = 'int': '{}'

    :class = []

   :class = ['String'], [',', ', ,, , , , ]
  :output = {'message': '.message,}'>

   :type = 'int'

   :class = ['int']
  :class = 'int'

  :type = ['int'], [',],
  :output = {}

   :type = 'string'
  :type = self.parse_str(lambda x : (str x)
    :type = 'str'
   :type = 'string': '{}'}'
  :type = 'string': '{}'

   :type = '[', ] [', ],
  :output = [True, False]
  :class = []
  :class = ['String'], [', ] [/\s ]
  :output = [False, True]
  :class = ['int'],
  :class = 'int'

  :type = 'object'
  :type = 'object'

  :type = 'string', [])
  :type = `[object']`
  :class = [],
  :class = ['string']

  :type = '[object']`
  :class = [], [])
  :type = ['String'], [],
  :form = ["''', ''''],
  :param = ''

  :type = []['] [],
  :param = `[object]` [],
  :fname = [],
  :fname = [],
  :param = '''' [],
  :param = '''' [],
  :param = '''' [],
  :param = '''' [],
  :param = '''' [],
  :param = ''''
  :type = 'object'

  :type = ['object', 'string']

   :type = 'object' [])

  :type = `[object']` [],
  :group = []

  :group = ['string'], [
  ]

  class = 'string' [], [
  ]

  :group = ['int'], []
  :group = ['int'], [] [])

  :type = 'str'
  :type = {'message': '''',
  :format: str['{}']}'>
  :class = ['str'] [],
  :group = ['int']
  :group = ['int'], [] []]

  :type = 'class'

  :class = ['String'], [],
  :group = ['String'], [], _
  :group = ['int']
  :group = ['int'], []
  :group = ['int'], [] []]

  :type = 'int' [])

  :type = ['int'] [])
  :type = ['int'], [], _
  :group = ['int'], [] [], _
  :group = ['int'], [] [], _
  :group = ['int'], [] []]
  :type = 'int' [])
  :type = ['int'], [], _
  :group = ['int'], [] [], _

[1300 | 723.55] loss=1.71 avg=1.11
[1301 | 728.95] loss=1.58 avg=1.12
[1302 | 734.31] loss=1.06 avg=1.11
[1303 | 739.69] loss=0.63 avg=1.11
[1304 | 745.03] loss=1.21 avg=1.11
[1305 | 750.46] loss=0.90 avg=1.11
[1306 | 755.76] loss=0.68 avg=1.10
[1307 | 760.98] loss=1.30 avg=1.10
[1308 | 766.34] loss=0.74 avg=1.10
[1309 | 771.72] loss=1.26 avg=1.10
[1310 | 777.02] loss=1.18 avg=1.10
[1311 | 782.27] loss=1.09 avg=1.10
[1312 | 787.57] loss=1.34 avg=1.10
[1313 | 792.91] loss=1.51 avg=1.11
[1314 | 798.23] loss=0.99 avg=1.11
[1315 | 803.58] loss=1.43 avg=1.11
[1316 | 808.78] loss=1.00 avg=1.11
[1317 | 814.09] loss=1.55 avg=1.12
[1318 | 819.48] loss=1.11 avg=1.12
[1319 | 824.80] loss=1.20 avg=1.12
[1320 | 830.08] loss=1.49 avg=1.12
[1321 | 835.40] loss=1.44 avg=1.13
[1322 | 840.73] loss=0.79 avg=1.12
[1323 | 845.98] loss=0.60 avg=1.12
[1324 | 851.20] loss=1.03 avg=1.11
[1325 | 856.38] loss=1.04 avg=1.11
[1326 | 861.65] loss=1.04 avg=1.11
[1327 | 866.97] loss=0.70 avg=1.11
[1328 | 872.29] loss=1.42 avg=1.11
[1329 | 877.50] loss=0.71 avg=1.11
[1330 | 882.90] loss=0.65 avg=1.10
[1331 | 888.09] loss=0.93 avg=1.10
[1332 | 893.34] loss=1.55 avg=1.10
[1333 | 898.58] loss=1.49 avg=1.11
[1334 | 903.76] loss=0.80 avg=1.10
[1335 | 908.95] loss=0.45 avg=1.10
[1336 | 914.10] loss=1.69 avg=1.10
[1337 | 919.26] loss=0.81 avg=1.10
[1338 | 924.51] loss=1.33 avg=1.10
[1339 | 929.77] loss=1.14 avg=1.10
[1340 | 935.02] loss=0.93 avg=1.10
[1341 | 940.29] loss=0.79 avg=1.10
[1342 | 945.45] loss=1.35 avg=1.10
[1343 | 950.68] loss=1.07 avg=1.10
[1344 | 955.95] loss=0.83 avg=1.10
[1345 | 961.06] loss=1.33 avg=1.10
[1346 | 966.33] loss=1.79 avg=1.11
[1347 | 971.60] loss=0.70 avg=1.10
[1348 | 976.91] loss=0.86 avg=1.10
[1349 | 982.15] loss=1.22 avg=1.10
[1350 | 987.40] loss=1.20 avg=1.10
[1351 | 992.58] loss=1.33 avg=1.11
[1352 | 997.82] loss=0.91 avg=1.10
[1353 | 1003.03] loss=1.28 avg=1.11
[1354 | 1008.27] loss=1.44 avg=1.11
[1355 | 1013.48] loss=0.96 avg=1.11
[1356 | 1018.70] loss=1.14 avg=1.11
[1357 | 1023.88] loss=1.32 avg=1.11
[1358 | 1029.05] loss=1.15 avg=1.11
[1359 | 1034.21] loss=1.16 avg=1.11
[1360 | 1039.42] loss=1.61 avg=1.12
[1361 | 1044.69] loss=1.36 avg=1.12
[1362 | 1049.95] loss=1.70 avg=1.13
[1363 | 1055.21] loss=1.29 avg=1.13
[1364 | 1060.45] loss=0.96 avg=1.13
[1365 | 1065.67] loss=1.23 avg=1.13
[1366 | 1070.88] loss=2.33 avg=1.14
[1367 | 1076.04] loss=1.78 avg=1.15
[1368 | 1081.23] loss=0.91 avg=1.15
[1369 | 1086.54] loss=0.71 avg=1.14
[1370 | 1091.76] loss=1.15 avg=1.14
[1371 | 1096.89] loss=1.21 avg=1.14
[1372 | 1102.05] loss=0.69 avg=1.14
[1373 | 1107.24] loss=1.22 avg=1.14
[1374 | 1112.40] loss=1.09 avg=1.14
[1375 | 1117.63] loss=0.92 avg=1.14
[1376 | 1122.86] loss=1.33 avg=1.14
[1377 | 1128.11] loss=2.37 avg=1.15
[1378 | 1133.30] loss=0.72 avg=1.15
[1379 | 1138.53] loss=1.59 avg=1.15
[1380 | 1143.68] loss=0.96 avg=1.15
[1381 | 1148.88] loss=1.29 avg=1.15
[1382 | 1154.17] loss=0.85 avg=1.15
[1383 | 1159.41] loss=1.00 avg=1.15
[1384 | 1164.58] loss=1.65 avg=1.15
[1385 | 1169.84] loss=1.16 avg=1.15
[1386 | 1175.10] loss=1.36 avg=1.16
[1387 | 1180.32] loss=1.06 avg=1.16
[1388 | 1185.56] loss=1.16 avg=1.16
[1389 | 1190.74] loss=0.90 avg=1.15
[1390 | 1196.03] loss=1.02 avg=1.15
[1391 | 1201.31] loss=1.43 avg=1.15
[1392 | 1206.60] loss=0.99 avg=1.15
[1393 | 1211.76] loss=0.74 avg=1.15
[1394 | 1216.95] loss=0.77 avg=1.14
[1395 | 1222.15] loss=1.03 avg=1.14
[1396 | 1227.34] loss=0.91 avg=1.14
[1397 | 1232.45] loss=1.24 avg=1.14
[1398 | 1237.60] loss=0.76 avg=1.14
[1399 | 1242.80] loss=1.36 avg=1.14
Generating samples...
======== SAMPLE 1 ========
 u'.',
                              str(', %', '.',
                                           str(', %', '.',
                                       str(', %', '.', '.',
                                              ''');
                    str(', str(', %', '.', '.', str(', str(', str(', str(', str(', str(', str(', str( ', if str(', str( ', str( ', str')', '''') else string(', str(', str(', str( ', str')') ''')), ''')
              '''
                    str(', str(', str(', str(', str( ', str() * %') + '.01' ))), ''')
                str(', str(', str( ', str() * %', str( ', str() * %') + '.01' ))), ''')
              str()
               def str():
                  str(', str(', str(', str( ', str() * %' ))), ''')
                 str()

               str(_)
                str(', str(', str( ', str() * %' )), ''')
                str()

               def str():
                str(', str(', str( ', str() * %' )), ''')
               str()

             def _(self, str):
                 str(', str(', str( ', int(', str( ', str() *%' )), '''), '''),
             str(', str(', str( ', str() * %' )), ''')
              str(', str( ', str() * %' ), ''')
              str())
             return str

            def str_split(self, str, max):
              str(', str( ', str() * %' ), ''')
             str()
             return str

            def _split(self, str, max):
               str(', str( ', str() * %' ), ''')
              str()

            def str_split(self, str, max):
              str(', str( ', str() * %' ), ''')
             return str
             def _

[1400 | 1327.57] loss=2.19 avg=1.15
[1401 | 1332.96] loss=1.79 avg=1.16
[1402 | 1338.29] loss=0.31 avg=1.15
[1403 | 1343.55] loss=0.83 avg=1.14
[1404 | 1348.77] loss=1.27 avg=1.15
[1405 | 1353.96] loss=0.97 avg=1.14
[1406 | 1359.23] loss=1.34 avg=1.15
[1407 | 1364.50] loss=0.91 avg=1.14
[1408 | 1369.74] loss=1.28 avg=1.14
[1409 | 1375.04] loss=0.86 avg=1.14
[1410 | 1380.24] loss=0.88 avg=1.14
[1411 | 1385.44] loss=0.94 avg=1.14
[1412 | 1390.82] loss=1.44 avg=1.14
[1413 | 1396.06] loss=0.74 avg=1.14
[1414 | 1401.26] loss=1.52 avg=1.14
[1415 | 1406.57] loss=0.97 avg=1.14
[1416 | 1411.70] loss=0.95 avg=1.14
[1417 | 1416.89] loss=1.33 avg=1.14
[1418 | 1422.04] loss=1.64 avg=1.14
[1419 | 1427.33] loss=1.30 avg=1.15
[1420 | 1432.66] loss=0.92 avg=1.14
[1421 | 1437.90] loss=1.05 avg=1.14
[1422 | 1443.04] loss=1.35 avg=1.14
[1423 | 1448.29] loss=1.78 avg=1.15
[1424 | 1453.62] loss=1.96 avg=1.16
[1425 | 1458.99] loss=0.83 avg=1.16
[1426 | 1464.26] loss=1.58 avg=1.16
[1427 | 1469.46] loss=1.29 avg=1.16
[1428 | 1474.68] loss=1.07 avg=1.16
[1429 | 1479.95] loss=0.74 avg=1.16
[1430 | 1485.23] loss=0.97 avg=1.15
[1431 | 1490.57] loss=1.25 avg=1.16
[1432 | 1495.82] loss=1.09 avg=1.15
[1433 | 1501.10] loss=0.88 avg=1.15
[1434 | 1506.34] loss=1.07 avg=1.15
[1435 | 1511.56] loss=1.18 avg=1.15
[1436 | 1516.72] loss=1.50 avg=1.16
[1437 | 1521.97] loss=1.44 avg=1.16
[1438 | 1527.27] loss=1.66 avg=1.16
[1439 | 1532.56] loss=1.34 avg=1.17
[1440 | 1537.88] loss=1.03 avg=1.16
[1441 | 1543.10] loss=1.21 avg=1.16
[1442 | 1548.30] loss=1.66 avg=1.17
[1443 | 1553.44] loss=1.27 avg=1.17
[1444 | 1558.69] loss=1.47 avg=1.17
[1445 | 1564.01] loss=1.50 avg=1.18
[1446 | 1569.40] loss=1.27 avg=1.18
[1447 | 1574.72] loss=1.14 avg=1.18
[1448 | 1580.03] loss=1.73 avg=1.18
[1449 | 1585.42] loss=0.92 avg=1.18
[1450 | 1590.66] loss=0.94 avg=1.18
[1451 | 1596.02] loss=1.29 avg=1.18
[1452 | 1601.36] loss=1.08 avg=1.18
[1453 | 1606.55] loss=0.74 avg=1.17
[1454 | 1611.92] loss=1.07 avg=1.17
[1455 | 1617.30] loss=1.55 avg=1.18
[1456 | 1622.56] loss=1.02 avg=1.18
[1457 | 1627.80] loss=1.15 avg=1.18
[1458 | 1633.00] loss=0.91 avg=1.17
[1459 | 1638.21] loss=0.85 avg=1.17
[1460 | 1643.39] loss=0.85 avg=1.17
[1461 | 1648.61] loss=0.77 avg=1.16
[1462 | 1653.81] loss=1.84 avg=1.17
[1463 | 1658.98] loss=1.35 avg=1.17
[1464 | 1664.36] loss=0.74 avg=1.17
[1465 | 1669.58] loss=1.03 avg=1.16
[1466 | 1674.79] loss=1.29 avg=1.17
[1467 | 1680.04] loss=1.05 avg=1.16
[1468 | 1685.32] loss=1.01 avg=1.16
[1469 | 1690.67] loss=1.46 avg=1.17
[1470 | 1695.89] loss=0.69 avg=1.16
[1471 | 1701.17] loss=0.89 avg=1.16
[1472 | 1706.47] loss=1.11 avg=1.16
[1473 | 1711.68] loss=0.92 avg=1.16
[1474 | 1716.93] loss=1.26 avg=1.16
[1475 | 1722.09] loss=0.67 avg=1.15
[1476 | 1727.35] loss=0.86 avg=1.15
[1477 | 1732.56] loss=1.51 avg=1.15
[1478 | 1737.75] loss=0.65 avg=1.15
[1479 | 1742.97] loss=1.00 avg=1.14
[1480 | 1748.21] loss=0.71 avg=1.14
[1481 | 1753.42] loss=1.35 avg=1.14
[1482 | 1758.74] loss=0.84 avg=1.14
[1483 | 1764.03] loss=1.18 avg=1.14
[1484 | 1769.37] loss=0.74 avg=1.14
[1485 | 1774.68] loss=1.86 avg=1.14
[1486 | 1780.10] loss=1.16 avg=1.14
[1487 | 1785.32] loss=1.07 avg=1.14
[1488 | 1790.48] loss=1.01 avg=1.14
[1489 | 1795.67] loss=1.21 avg=1.14
[1490 | 1800.82] loss=1.27 avg=1.14
[1491 | 1806.06] loss=0.69 avg=1.14
[1492 | 1811.24] loss=0.90 avg=1.14
[1493 | 1816.49] loss=2.15 avg=1.15
[1494 | 1821.79] loss=1.85 avg=1.15
[1495 | 1827.08] loss=0.95 avg=1.15
[1496 | 1832.23] loss=0.94 avg=1.15
[1497 | 1837.40] loss=1.38 avg=1.15
[1498 | 1842.67] loss=0.88 avg=1.15
[1499 | 1847.92] loss=0.94 avg=1.15
Generating samples...
======== SAMPLE 1 ========
      "The data type:
                        "Pid",                       "Foo",                    "pops",                    "giant",     "penguin")
        self.bbox.copy_pid_data(False)
        self.c_vx_data = self.f_vx_data[1:]
        self.c_vx_data = c_vx_data[1:]
        if self.bbox.get_data is not None:
               self.bbox.copy_data = False
        self.c_vx_data = self.c_vx_data[1:]
        self.c_vx_data = c_vx_data[1:]
        return self<|endoftext|>From TheOLAP

1 Explicit #41 - Losing the day

The day that the Losing the day (Midsummer 2014) event is not being displayed is being marked by "Day to Day Midsummer 2014" in that the event date has been changed. This is done so that in all the event files and subfolders that are being added, the event may not be marked in that date. [1]


The day is not being displayed for some subfolders, such as in the following files:

Subfolder

Midsummer

The following files are being marked as being dropped as well when they are placed:

Midsummer's

Midsummer's.zip

/usr/lib/dcl/dcl/Midsummer.zip

Midsummer's_data.zip

/usr/lib/dcl/dcl/LosingOfTheDayMidsummer.zip

Losingtheday_day_scalar.png

/usr/lib/dcl/dcl/LosingOfTheDayMidsummer.png


From a list of subfolders that are being added to a list at a time, the day is not being displayed by any of those files. This is done so that in all the individual subfolders that are being shown, the day may not actually be displayed or drop by any directory that is under the subfolders in the folder in its file name and subfolders where it is, that the event will be displayed by the following file names for the subfolders/sub_folder as well:

Subfolder

Midsummer

The following files are being marked as being dropped and re-added as well when those subfolders/sub_folder are added. This is done so that all the subfolders or sub_folder that are being shown at any point will have the event appear on each of its subfolders/sub_folder's

Subfolder

Midsummer

For the sub directories that are being added to the list (i.e. their subfolders) by default, the day is being displayed by the following filenames:

Subfolder

Midsummer's


For the subfolders that do NOT yet exist within files in that subfolder, it will simply replace them when they are added, and will not even exist in the subsequent files. This allows for a very common nonfunctional day which simply has no event.


From a list of subfolders that are already in that folder, it will also replace them when they will be added, and will not even exist in the subsequent files. This allows for a very common nonfunctional day which simply has no event.


From a list of subfolders that are already in that folder, it will also replace them when they will be added, and will NOT even exist in the subsequent files. This allows for a very common nonfunctional day which simply has no event.


From a list of subfolders that are already in That subfolder, it will also replace it when it will appear there on each of its sub folder's:

Subfolder

Midsummer

The previous day that is being displayed in the Subfolder will be replaced with the day of the event which has been displayed for it. [2]


Losing the

[1500 | 1931.89] loss=0.80 avg=1.14
[1501 | 1937.12] loss=0.83 avg=1.14
[1502 | 1942.32] loss=1.13 avg=1.14
[1503 | 1947.58] loss=1.70 avg=1.15
[1504 | 1952.88] loss=1.29 avg=1.15
[1505 | 1958.20] loss=1.43 avg=1.15
[1506 | 1963.38] loss=1.06 avg=1.15
[1507 | 1968.67] loss=1.23 avg=1.15
[1508 | 1973.90] loss=1.02 avg=1.15
[1509 | 1979.15] loss=0.86 avg=1.15
[1510 | 1984.52] loss=1.15 avg=1.15
[1511 | 1989.82] loss=0.83 avg=1.14
[1512 | 1995.11] loss=0.71 avg=1.14
[1513 | 2000.30] loss=1.66 avg=1.14
[1514 | 2005.48] loss=1.47 avg=1.15
[1515 | 2010.60] loss=1.20 avg=1.15
[1516 | 2015.69] loss=0.95 avg=1.15
[1517 | 2020.85] loss=1.02 avg=1.14
[1518 | 2026.00] loss=1.39 avg=1.15
[1519 | 2031.21] loss=0.94 avg=1.14
[1520 | 2036.34] loss=0.88 avg=1.14
[1521 | 2041.48] loss=0.87 avg=1.14
[1522 | 2046.61] loss=1.16 avg=1.14
[1523 | 2051.74] loss=0.97 avg=1.14
[1524 | 2056.92] loss=1.15 avg=1.14
[1525 | 2062.10] loss=0.97 avg=1.14
[1526 | 2067.27] loss=1.47 avg=1.14
[1527 | 2072.39] loss=0.90 avg=1.14
[1528 | 2077.63] loss=0.37 avg=1.13
[1529 | 2082.84] loss=0.93 avg=1.13
[1530 | 2088.00] loss=1.44 avg=1.13
[1531 | 2093.20] loss=1.16 avg=1.13
[1532 | 2098.47] loss=0.73 avg=1.13
[1533 | 2103.75] loss=1.65 avg=1.13
[1534 | 2108.96] loss=1.04 avg=1.13
[1535 | 2114.30] loss=0.85 avg=1.13
[1536 | 2119.58] loss=0.98 avg=1.13
[1537 | 2124.87] loss=1.02 avg=1.13
[1538 | 2130.06] loss=3.66 avg=1.15
[1539 | 2135.19] loss=0.71 avg=1.15
[1540 | 2140.39] loss=1.62 avg=1.15
[1541 | 2145.65] loss=1.08 avg=1.15
[1542 | 2150.85] loss=1.48 avg=1.15
[1543 | 2156.05] loss=1.06 avg=1.15
[1544 | 2161.23] loss=1.10 avg=1.15
[1545 | 2166.51] loss=1.13 avg=1.15
[1546 | 2171.73] loss=2.05 avg=1.16
[1547 | 2176.89] loss=0.98 avg=1.16
[1548 | 2182.05] loss=1.64 avg=1.16
[1549 | 2187.22] loss=1.04 avg=1.16
[1550 | 2192.48] loss=1.10 avg=1.16
[1551 | 2197.84] loss=1.43 avg=1.17
[1552 | 2203.04] loss=0.79 avg=1.16
[1553 | 2208.33] loss=1.45 avg=1.16
[1554 | 2213.57] loss=0.76 avg=1.16
[1555 | 2218.92] loss=1.27 avg=1.16
[1556 | 2224.22] loss=1.04 avg=1.16
[1557 | 2229.53] loss=1.22 avg=1.16
[1558 | 2234.81] loss=1.86 avg=1.17
[1559 | 2240.16] loss=0.95 avg=1.17
[1560 | 2245.33] loss=1.18 avg=1.17
[1561 | 2250.50] loss=1.28 avg=1.17
[1562 | 2255.66] loss=1.10 avg=1.17
[1563 | 2260.76] loss=1.43 avg=1.17
[1564 | 2265.80] loss=0.37 avg=1.16
[1565 | 2270.92] loss=1.75 avg=1.17
[1566 | 2276.09] loss=1.08 avg=1.17
[1567 | 2281.22] loss=1.47 avg=1.17
[1568 | 2286.41] loss=0.78 avg=1.17
[1569 | 2291.58] loss=1.01 avg=1.16
[1570 | 2296.78] loss=1.64 avg=1.17
[1571 | 2301.96] loss=1.43 avg=1.17
[1572 | 2307.13] loss=1.08 avg=1.17
[1573 | 2312.35] loss=0.63 avg=1.16
[1574 | 2317.52] loss=0.89 avg=1.16
[1575 | 2322.68] loss=0.56 avg=1.16
[1576 | 2327.92] loss=1.17 avg=1.16
[1577 | 2333.11] loss=2.02 avg=1.16
[1578 | 2338.36] loss=0.51 avg=1.16
[1579 | 2343.54] loss=1.28 avg=1.16
[1580 | 2348.85] loss=0.80 avg=1.16
[1581 | 2354.02] loss=1.14 avg=1.16
[1582 | 2359.24] loss=1.17 avg=1.16
[1583 | 2364.49] loss=0.88 avg=1.15
[1584 | 2369.71] loss=0.94 avg=1.15
[1585 | 2374.95] loss=0.70 avg=1.15
[1586 | 2380.23] loss=1.48 avg=1.15
[1587 | 2385.40] loss=0.85 avg=1.15
[1588 | 2390.63] loss=1.71 avg=1.15
[1589 | 2395.84] loss=1.33 avg=1.15
[1590 | 2401.19] loss=1.10 avg=1.15
[1591 | 2406.47] loss=0.75 avg=1.15
[1592 | 2411.71] loss=1.27 avg=1.15
[1593 | 2417.00] loss=0.42 avg=1.14
[1594 | 2422.29] loss=1.62 avg=1.15
[1595 | 2427.48] loss=1.36 avg=1.15
[1596 | 2432.62] loss=1.23 avg=1.15
[1597 | 2437.79] loss=0.85 avg=1.15
[1598 | 2442.94] loss=0.95 avg=1.15
[1599 | 2448.15] loss=0.73 avg=1.14
Generating samples...
======== SAMPLE 1 ========
 want a more accurate model. To do this, the model is implemented with
        x + = x + y.
        For further details, see the wiki-source
        http://gizmo.wikidata.org/wiki/Generative_Model_with_Data_flow
    (http://gizmo.wikidata.org/wiki/Gradient_Layers, __________).
        The model is iteratively applied to the top of the data stack. This process will then
        yield additional variables which are not being returned in any future update.

        This model is a model that has been built on top of the
        data stack.

        Parameters
        ----------
        x - the value of x values (i.e. values in, e.g., 0.7, 0.5).
        y - the value of y values (i.e. values in, e.g., 0.6, 1.5).
        """
        y + = x + y.model.x
        y + = { 0:0, 1:0, 2:2, 3:3}
        x + = x + y.model.x + y.model.x + y.model.y + y.model.y + y.model.y + y.model.y + y.model.y + y.model.y + y.model.y + y.model.y + y.model.y + y.model.y +                                            +     3, 0, 2:3}
                x + = x + y.model.x + y.model.y + y.models.x + y.models.y + y.model.y +        y + =
              x + = x + y.model.x + ... + y.models.y + y.model.y + y.models.y + y.models.y + """
        y + = y + x + y.model.y + y +  x + y + y + y + y + x +     x + = x + y + ... + y
        y + = y + x + y + y + x + y + x + y + x + x +   y + =
              x + = x + x + y + x + z
        y + = y + x + x +   y + y + y +      x + = x +  x + z
        y - = x +  y +  x +  y +       x + = 0.6
             y + = x + ( y + y +  x + y + y +        x +
                                     y +                                                                  x +                                                     x +                                                       

[1600 | 2532.65] loss=0.83 avg=1.14
[1601 | 2537.98] loss=1.23 avg=1.14
[1602 | 2543.27] loss=1.66 avg=1.14
[1603 | 2548.55] loss=1.76 avg=1.15
[1604 | 2553.83] loss=1.05 avg=1.15
[1605 | 2559.13] loss=0.66 avg=1.14
[1606 | 2564.38] loss=1.41 avg=1.15
[1607 | 2569.74] loss=1.19 avg=1.15
[1608 | 2575.01] loss=1.88 avg=1.16
[1609 | 2580.29] loss=0.83 avg=1.15
[1610 | 2585.58] loss=0.67 avg=1.15
[1611 | 2590.76] loss=0.91 avg=1.14
[1612 | 2596.11] loss=1.25 avg=1.15
[1613 | 2601.40] loss=0.85 avg=1.14
[1614 | 2606.69] loss=1.10 avg=1.14
[1615 | 2612.07] loss=0.83 avg=1.14
[1616 | 2617.29] loss=1.37 avg=1.14
[1617 | 2622.57] loss=1.35 avg=1.14
[1618 | 2627.90] loss=1.92 avg=1.15
[1619 | 2633.23] loss=0.72 avg=1.15
[1620 | 2638.43] loss=1.29 avg=1.15
[1621 | 2643.67] loss=1.46 avg=1.15
[1622 | 2648.95] loss=2.43 avg=1.16
[1623 | 2654.15] loss=1.53 avg=1.17
[1624 | 2659.42] loss=0.98 avg=1.17
[1625 | 2664.68] loss=1.02 avg=1.17
[1626 | 2669.90] loss=0.54 avg=1.16
[1627 | 2675.19] loss=0.77 avg=1.15
[1628 | 2680.38] loss=1.08 avg=1.15
[1629 | 2685.68] loss=1.27 avg=1.16
[1630 | 2690.99] loss=0.74 avg=1.15
[1631 | 2696.27] loss=1.25 avg=1.15
[1632 | 2701.61] loss=0.98 avg=1.15
[1633 | 2706.88] loss=1.04 avg=1.15
[1634 | 2712.19] loss=0.81 avg=1.15
[1635 | 2717.50] loss=1.74 avg=1.15
[1636 | 2722.79] loss=1.29 avg=1.15
[1637 | 2728.07] loss=0.87 avg=1.15
[1638 | 2733.32] loss=0.99 avg=1.15
[1639 | 2738.75] loss=0.73 avg=1.14
[1640 | 2744.11] loss=0.50 avg=1.14
[1641 | 2749.45] loss=0.84 avg=1.14
[1642 | 2754.76] loss=1.58 avg=1.14
[1643 | 2760.00] loss=1.63 avg=1.14
[1644 | 2765.21] loss=1.21 avg=1.15
[1645 | 2770.37] loss=1.05 avg=1.14
[1646 | 2775.56] loss=1.21 avg=1.15
[1647 | 2780.73] loss=0.90 avg=1.14
[1648 | 2785.87] loss=1.51 avg=1.15
[1649 | 2790.99] loss=0.92 avg=1.14
[1650 | 2796.20] loss=0.83 avg=1.14
[1651 | 2801.41] loss=1.22 avg=1.14
[1652 | 2806.64] loss=1.30 avg=1.14
[1653 | 2811.85] loss=1.58 avg=1.15
[1654 | 2816.93] loss=1.02 avg=1.15
[1655 | 2822.16] loss=1.16 avg=1.15
[1656 | 2827.28] loss=0.96 avg=1.14
[1657 | 2832.37] loss=1.46 avg=1.15
[1658 | 2837.55] loss=1.11 avg=1.15
[1659 | 2842.74] loss=1.16 avg=1.15
[1660 | 2847.96] loss=1.12 avg=1.15
[1661 | 2853.15] loss=1.23 avg=1.15
[1662 | 2858.39] loss=0.96 avg=1.15
[1663 | 2863.62] loss=1.03 avg=1.15
[1664 | 2868.79] loss=0.99 avg=1.14
[1665 | 2874.03] loss=0.66 avg=1.14
[1666 | 2879.29] loss=1.07 avg=1.14
[1667 | 2884.52] loss=0.72 avg=1.13
[1668 | 2889.73] loss=0.86 avg=1.13
[1669 | 2894.98] loss=0.93 avg=1.13
[1670 | 2900.21] loss=1.20 avg=1.13
[1671 | 2905.35] loss=1.05 avg=1.13
[1672 | 2910.52] loss=0.66 avg=1.12
[1673 | 2915.75] loss=1.18 avg=1.12
[1674 | 2921.01] loss=1.10 avg=1.12
[1675 | 2926.30] loss=0.93 avg=1.12
[1676 | 2931.56] loss=1.09 avg=1.12
[1677 | 2936.83] loss=0.97 avg=1.12
[1678 | 2942.03] loss=1.30 avg=1.12
[1679 | 2947.25] loss=1.19 avg=1.12
[1680 | 2952.55] loss=0.61 avg=1.12
[1681 | 2957.82] loss=1.28 avg=1.12
[1682 | 2963.12] loss=1.06 avg=1.12
[1683 | 2968.71] loss=0.74 avg=1.12
[1684 | 2974.02] loss=1.07 avg=1.11
[1685 | 2979.29] loss=0.43 avg=1.11
[1686 | 2984.57] loss=0.89 avg=1.11
[1687 | 2989.83] loss=1.24 avg=1.11
[1688 | 2995.02] loss=0.75 avg=1.10
[1689 | 3000.17] loss=1.05 avg=1.10
[1690 | 3005.30] loss=1.94 avg=1.11
[1691 | 3010.54] loss=0.66 avg=1.11
[1692 | 3015.87] loss=1.21 avg=1.11
[1693 | 3021.20] loss=0.87 avg=1.11
[1694 | 3026.51] loss=1.05 avg=1.10
[1695 | 3031.69] loss=1.18 avg=1.11
[1696 | 3036.94] loss=1.20 avg=1.11
[1697 | 3042.11] loss=1.13 avg=1.11
[1698 | 3047.38] loss=0.75 avg=1.10
[1699 | 3052.64] loss=1.15 avg=1.10
Generating samples...
======== SAMPLE 1 ========
', 】 """
         self.__op_values_and_params = map(get_self(self),                                                                                                                                                                                                                                                )

         self.__values_and_params[:self.__op_values_and_params[:self.__op_values_and_params[:self.__op_values_and_params[:self.__op_values_and_params[:self.__op_values_and_params[:self.__op_values_and_params[:self.__op_values_and_params[:self.__op_values_and_params[:self.__op_values_and_params]]] = get_self(self)
        self.__values_and_params[0] = self.__op_values_and_params[1] [u'def', u'__op_values', u'(', u'self', u',', u'mtrs', u')', u':', u'kw', u'=', u'start_kw_get', u'(', u'kw', u')', u'# Set to the value at the left side
        # from the values at the bottom.
        values = self.__values_and_params[]:__op_values.get()
        values.pop()
        self.__values_and_params[_] = values.pop()
        self.__op_values_and_params[_] = self.__op_values_and_params[__op_values_and_params[_][':self.__op_values_and_params[0].get()-self.__op_values_and_params[0].get()-self.__op_values_and_params[1].get().value(self.__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and__params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_values_and_params[__op_

[1700 | 3138.28] loss=1.42 avg=1.11
[1701 | 3143.45] loss=1.14 avg=1.11
[1702 | 3148.70] loss=1.01 avg=1.11
[1703 | 3154.03] loss=0.91 avg=1.10
[1704 | 3159.40] loss=0.81 avg=1.10
[1705 | 3164.68] loss=0.85 avg=1.10
[1706 | 3169.94] loss=0.70 avg=1.09
[1707 | 3175.26] loss=1.00 avg=1.09
[1708 | 3180.53] loss=0.79 avg=1.09
[1709 | 3185.92] loss=0.85 avg=1.09
[1710 | 3191.27] loss=0.86 avg=1.09
[1711 | 3196.66] loss=1.24 avg=1.09
[1712 | 3201.99] loss=1.05 avg=1.09
[1713 | 3207.22] loss=1.26 avg=1.09
[1714 | 3212.38] loss=1.02 avg=1.09
[1715 | 3217.73] loss=1.06 avg=1.09
[1716 | 3223.07] loss=0.74 avg=1.08
[1717 | 3228.42] loss=1.23 avg=1.09
[1718 | 3233.85] loss=0.90 avg=1.08
[1719 | 3239.20] loss=1.41 avg=1.09
[1720 | 3244.58] loss=0.72 avg=1.08
[1721 | 3249.86] loss=1.58 avg=1.09
[1722 | 3255.14] loss=0.93 avg=1.09
[1723 | 3260.47] loss=1.21 avg=1.09
[1724 | 3265.79] loss=1.08 avg=1.09
[1725 | 3271.15] loss=1.34 avg=1.09
[1726 | 3276.45] loss=0.97 avg=1.09
[1727 | 3281.86] loss=0.89 avg=1.09
[1728 | 3287.19] loss=1.40 avg=1.09
[1729 | 3292.46] loss=1.72 avg=1.10
[1730 | 3298.07] loss=0.95 avg=1.10
[1731 | 3303.36] loss=1.17 avg=1.10
[1732 | 3308.63] loss=0.91 avg=1.09
[1733 | 3313.76] loss=0.61 avg=1.09
[1734 | 3318.96] loss=1.41 avg=1.09
[1735 | 3324.15] loss=0.81 avg=1.09
[1736 | 3329.32] loss=1.41 avg=1.09
[1737 | 3334.45] loss=0.92 avg=1.09
[1738 | 3339.58] loss=0.80 avg=1.09
[1739 | 3344.76] loss=0.80 avg=1.09
[1740 | 3349.91] loss=2.15 avg=1.10
[1741 | 3355.04] loss=0.64 avg=1.09
[1742 | 3360.26] loss=1.15 avg=1.09
[1743 | 3365.40] loss=1.49 avg=1.10
[1744 | 3370.68] loss=1.57 avg=1.10
[1745 | 3376.01] loss=1.39 avg=1.10
[1746 | 3381.14] loss=0.67 avg=1.10
[1747 | 3386.34] loss=1.15 avg=1.10
[1748 | 3391.65] loss=1.56 avg=1.10
[1749 | 3397.00] loss=1.09 avg=1.10
[1750 | 3402.24] loss=0.68 avg=1.10
[1751 | 3407.59] loss=1.05 avg=1.10
[1752 | 3412.83] loss=1.13 avg=1.10
[1753 | 3418.10] loss=1.09 avg=1.10
[1754 | 3423.44] loss=1.29 avg=1.10
[1755 | 3428.78] loss=1.56 avg=1.11
[1756 | 3434.05] loss=1.15 avg=1.11
[1757 | 3439.23] loss=1.25 avg=1.11
[1758 | 3444.45] loss=1.45 avg=1.11
[1759 | 3449.68] loss=1.40 avg=1.11
[1760 | 3454.99] loss=0.89 avg=1.11
[1761 | 3460.27] loss=0.84 avg=1.11
[1762 | 3465.63] loss=0.89 avg=1.11
[1763 | 3470.94] loss=0.68 avg=1.10
[1764 | 3476.21] loss=1.15 avg=1.10
[1765 | 3481.52] loss=0.95 avg=1.10
[1766 | 3486.76] loss=1.04 avg=1.10
[1767 | 3492.05] loss=0.66 avg=1.10
[1768 | 3497.25] loss=1.42 avg=1.10
[1769 | 3502.44] loss=0.32 avg=1.09
[1770 | 3507.69] loss=1.00 avg=1.09
[1771 | 3513.00] loss=1.24 avg=1.09
[1772 | 3518.21] loss=1.16 avg=1.09
[1773 | 3523.49] loss=0.71 avg=1.09
[1774 | 3528.75] loss=1.05 avg=1.09
[1775 | 3534.03] loss=1.65 avg=1.10
[1776 | 3539.22] loss=0.98 avg=1.09
[1777 | 3544.46] loss=0.79 avg=1.09
[1778 | 3549.58] loss=0.82 avg=1.09
[1779 | 3554.89] loss=0.99 avg=1.09
[1780 | 3560.01] loss=0.82 avg=1.08
[1781 | 3565.34] loss=0.87 avg=1.08
[1782 | 3570.65] loss=1.46 avg=1.09
[1783 | 3575.86] loss=1.03 avg=1.09
[1784 | 3581.19] loss=1.19 avg=1.09
[1785 | 3586.43] loss=1.74 avg=1.09
[1786 | 3591.67] loss=1.08 avg=1.09
[1787 | 3596.93] loss=1.24 avg=1.09
[1788 | 3602.19] loss=0.89 avg=1.09
[1789 | 3607.38] loss=2.77 avg=1.11
[1790 | 3612.59] loss=0.69 avg=1.11
[1791 | 3617.81] loss=0.93 avg=1.10
[1792 | 3623.02] loss=0.90 avg=1.10
[1793 | 3628.30] loss=1.07 avg=1.10
[1794 | 3633.56] loss=1.29 avg=1.10
[1795 | 3638.90] loss=1.53 avg=1.11
[1796 | 3644.16] loss=0.97 avg=1.11
[1797 | 3649.35] loss=0.82 avg=1.10
[1798 | 3654.63] loss=0.65 avg=1.10
[1799 | 3659.83] loss=1.27 avg=1.10
Generating samples...
======== SAMPLE 1 ========
 =    return x
        train_data.insert(p = np.cos(p) + 1)
        train_data.insert(p = np.sin(p))
        train_data.insert(p = np.sin((p - 1)) + 1)
        train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 2)) + 1) train_data.insert(p = np.tan((p - 2)) + 1) train_data.insert(p = np.tan(p)) train_data.insert(p = np.tan((p - 2)) + 1) train_data.insert(p = np.sin((p - 2)) + 1) train_data.insert(p = np.sin((p - 2)) + 1) train_data.insert(p = 0.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p=1.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p = 0.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) __id__ = np.cos(p) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = 0.0) train_data.insert(p = 0.0) train_data.insert(p = 0.0) train_data.insert(p = 0.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p = 0.0) train_data.insert(p = 0.0) __id__ = np.cos(p) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin((p - 1)) + 1) train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = 1.0) train_data.insert(p = 0.0) train_data.insert(p = 0.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) train_data.insert(p = 1.0) __id__ = np.cos(p) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np.sin(p)) + 1 train_data.insert(p = np

[1800 | 3743.74] loss=3.56 avg=1.12
[1801 | 3748.95] loss=0.89 avg=1.12
[1802 | 3754.32] loss=0.83 avg=1.12
[1803 | 3759.49] loss=1.41 avg=1.12
[1804 | 3764.67] loss=1.32 avg=1.12
[1805 | 3769.92] loss=0.86 avg=1.12
[1806 | 3775.19] loss=1.22 avg=1.12
[1807 | 3780.39] loss=0.51 avg=1.12
[1808 | 3785.68] loss=1.01 avg=1.12
[1809 | 3790.83] loss=0.87 avg=1.11
[1810 | 3796.09] loss=0.86 avg=1.11
[1811 | 3801.34] loss=1.50 avg=1.11
[1812 | 3806.53] loss=0.74 avg=1.11
[1813 | 3811.74] loss=1.94 avg=1.12
[1814 | 3816.97] loss=0.90 avg=1.12
[1815 | 3822.05] loss=0.85 avg=1.11
[1816 | 3827.20] loss=0.96 avg=1.11
[1817 | 3832.42] loss=1.29 avg=1.11
[1818 | 3837.70] loss=0.96 avg=1.11
[1819 | 3842.91] loss=1.07 avg=1.11
[1820 | 3848.14] loss=1.04 avg=1.11
[1821 | 3853.46] loss=1.03 avg=1.11
[1822 | 3858.75] loss=1.42 avg=1.11
[1823 | 3864.09] loss=2.13 avg=1.12
[1824 | 3869.32] loss=1.34 avg=1.13
[1825 | 3874.55] loss=1.12 avg=1.13
[1826 | 3879.82] loss=0.91 avg=1.12
[1827 | 3885.05] loss=0.67 avg=1.12
[1828 | 3890.42] loss=1.33 avg=1.12
[1829 | 3895.71] loss=1.05 avg=1.12
[1830 | 3900.99] loss=1.29 avg=1.12
[1831 | 3906.18] loss=1.05 avg=1.12
[1832 | 3911.39] loss=0.86 avg=1.12
[1833 | 3916.50] loss=0.95 avg=1.12
[1834 | 3921.68] loss=0.98 avg=1.12
[1835 | 3926.82] loss=1.53 avg=1.12
[1836 | 3932.05] loss=0.50 avg=1.11
[1837 | 3937.23] loss=1.21 avg=1.11
[1838 | 3942.51] loss=1.31 avg=1.12
[1839 | 3947.77] loss=1.22 avg=1.12
[1840 | 3953.03] loss=1.05 avg=1.12
[1841 | 3958.21] loss=1.01 avg=1.12
[1842 | 3963.28] loss=0.83 avg=1.11
[1843 | 3968.47] loss=1.15 avg=1.11
[1844 | 3973.81] loss=0.80 avg=1.11
[1845 | 3979.10] loss=0.86 avg=1.11
[1846 | 3984.28] loss=1.14 avg=1.11
[1847 | 3989.53] loss=1.05 avg=1.11
[1848 | 3994.73] loss=0.57 avg=1.10
[1849 | 3999.91] loss=0.91 avg=1.10
[1850 | 4005.35] loss=1.10 avg=1.10
[1851 | 4010.67] loss=1.17 avg=1.10
[1852 | 4016.09] loss=1.23 avg=1.10
[1853 | 4021.38] loss=1.44 avg=1.11
[1854 | 4026.59] loss=0.70 avg=1.10
[1855 | 4031.83] loss=1.13 avg=1.10
[1856 | 4037.11] loss=1.03 avg=1.10
[1857 | 4042.43] loss=1.69 avg=1.11
[1858 | 4047.81] loss=1.80 avg=1.11
[1859 | 4053.12] loss=0.99 avg=1.11
[1860 | 4058.39] loss=1.67 avg=1.12
[1861 | 4063.62] loss=1.63 avg=1.12
[1862 | 4068.91] loss=1.20 avg=1.12
[1863 | 4074.13] loss=1.66 avg=1.13
[1864 | 4079.29] loss=1.09 avg=1.13
[1865 | 4084.70] loss=0.58 avg=1.12
[1866 | 4089.96] loss=1.08 avg=1.12
[1867 | 4095.26] loss=0.96 avg=1.12
[1868 | 4100.48] loss=0.55 avg=1.12
[1869 | 4105.77] loss=0.75 avg=1.11
[1870 | 4111.02] loss=0.54 avg=1.11
[1871 | 4116.21] loss=1.21 avg=1.11
[1872 | 4121.43] loss=1.06 avg=1.11
[1873 | 4126.64] loss=0.90 avg=1.11
[1874 | 4131.88] loss=1.20 avg=1.11
[1875 | 4137.12] loss=1.39 avg=1.11
[1876 | 4142.37] loss=1.73 avg=1.12
[1877 | 4147.68] loss=1.48 avg=1.12
[1878 | 4153.00] loss=0.84 avg=1.12
[1879 | 4158.26] loss=0.63 avg=1.11
[1880 | 4163.54] loss=0.99 avg=1.11
[1881 | 4168.83] loss=0.84 avg=1.11
[1882 | 4174.26] loss=0.93 avg=1.11
[1883 | 4179.66] loss=0.81 avg=1.10
[1884 | 4184.96] loss=0.71 avg=1.10
[1885 | 4190.19] loss=0.68 avg=1.09
[1886 | 4195.43] loss=1.03 avg=1.09
[1887 | 4200.75] loss=1.07 avg=1.09
[1888 | 4206.11] loss=0.76 avg=1.09
[1889 | 4211.37] loss=0.99 avg=1.09
[1890 | 4216.67] loss=1.36 avg=1.09
[1891 | 4221.84] loss=1.13 avg=1.09
[1892 | 4227.05] loss=1.30 avg=1.09
[1893 | 4232.43] loss=0.75 avg=1.09
[1894 | 4237.71] loss=1.47 avg=1.09
[1895 | 4242.98] loss=0.95 avg=1.09
[1896 | 4248.27] loss=1.11 avg=1.09
[1897 | 4253.52] loss=0.67 avg=1.09
[1898 | 4258.77] loss=1.08 avg=1.09
[1899 | 4264.06] loss=0.93 avg=1.09
Generating samples...
======== SAMPLE 1 ========
id', u'[', u'0', u']', u')', u'if', u'id', u'not', u'==', u'__class__', u'(', u"'__class__':':', u'name', u',', u'isinstance', u'(', u'self', u'.', u'min', u')', u'and', u'__id', u'not', u'==', u'__class__', u'(', u"'__id':', u')', u']', u'self', u'[', u'__class__', u']', u'=', u'self', u'[', u'__id', u']', u'# This method should not be applied to the object returned via the set_instance() method, as you will need to return a dict of a form with the id you received from a model_id.', u'#
'except', u'TypeError', u'as', u'instance', u'(', u'self', u'.', u'=', u'_from_models', u'[', u'name', u']', u',', u'__class__', u'(', u'self', u'.', u'=', u'__class__', u',', u'__attr__', u'(', u'self', u'.', u'__id', u')', u',', u'model_id', u'.', u'id', u'=', u'__class__', u',', u'__id', u'(', u')', u'as', u'self', u'.', u'_from_models', u')'] Validate object for id for this parameter. Returns the self object. The resulting object is automatically returned to the model._method.
        Validate object for id for this parameter. Returns the self object. The resulting object is automatically returned to the model._method. [u'Validate', u'object', u'for', u'id', u'for', u'this', u'param', u'.'] Validate._method_id python def _get_object_from_models(self, id=None):
        """
        Validate object for id for this parameter. Returns the self object. The resulting object is automatically returned to the model._method.
        """
        assert isinstance(self) and self.id >= id
        self._id=self._id
        self._model_id =self._model_id
        self._id =self._id
        self._model_name=self._model_name
        self._id =self._id
        self._id =self._id
        self._model_id =self._id
        self._model_name =self._model_name
        # This method should not be applied to the object returned via the set_instance() method, as you will need to return a dict of a form with the id you received from a model_id.', u'#
        # This method should not be applied to the object returned via the set_instance() method, as you will need to return a dict of a form with the id you received from a model_id.', u'#
        # This method should not be applied to the object returned via the set_instance() method, as you will need to return a dict of a form with the id you received from a model_id.', u'#
         # This method should not be applied to the object returned via the set_instance() method, as you will need to return a dict of a form with the id you received from a model_id.', u'#
         # This method should not be applied to the object returned via the set_instance() method, as you will need to return a dict of a form with the id you received from a model_id.', u'#
         # This method should not be applied to the object returned via the set_instance() method, as you will need to return a dict of a form with the id you received from a model_id.', u'#


[1900 | 4352.11] loss=0.51 avg=1.08
[1901 | 4357.24] loss=1.04 avg=1.08
[1902 | 4362.50] loss=0.82 avg=1.08
[1903 | 4367.76] loss=0.80 avg=1.08
[1904 | 4372.99] loss=0.72 avg=1.07
[1905 | 4378.31] loss=1.58 avg=1.08
[1906 | 4383.41] loss=1.30 avg=1.08
[1907 | 4388.65] loss=0.85 avg=1.08
[1908 | 4393.95] loss=1.36 avg=1.08
[1909 | 4399.19] loss=0.68 avg=1.08
[1910 | 4404.59] loss=1.11 avg=1.08
[1911 | 4409.82] loss=0.69 avg=1.07
[1912 | 4415.04] loss=0.87 avg=1.07
[1913 | 4420.22] loss=0.96 avg=1.07
[1914 | 4425.56] loss=1.18 avg=1.07
[1915 | 4430.82] loss=1.23 avg=1.07
[1916 | 4436.04] loss=1.65 avg=1.08
[1917 | 4441.18] loss=1.95 avg=1.09
[1918 | 4446.25] loss=1.08 avg=1.09
[1919 | 4451.41] loss=1.07 avg=1.09
[1920 | 4456.50] loss=1.92 avg=1.09
[1921 | 4461.66] loss=0.89 avg=1.09
[1922 | 4466.81] loss=0.83 avg=1.09
[1923 | 4472.08] loss=1.56 avg=1.09
[1924 | 4477.30] loss=0.85 avg=1.09
[1925 | 4482.49] loss=0.86 avg=1.09
[1926 | 4487.65] loss=1.18 avg=1.09
[1927 | 4492.87] loss=1.32 avg=1.09
[1928 | 4498.01] loss=0.94 avg=1.09
[1929 | 4503.25] loss=0.97 avg=1.09
[1930 | 4508.45] loss=1.17 avg=1.09
[1931 | 4513.67] loss=1.01 avg=1.09
[1932 | 4519.07] loss=1.28 avg=1.09
[1933 | 4524.38] loss=0.83 avg=1.09
[1934 | 4529.59] loss=1.48 avg=1.09
[1935 | 4534.83] loss=0.85 avg=1.09
[1936 | 4540.14] loss=1.04 avg=1.09
[1937 | 4545.46] loss=0.86 avg=1.09
[1938 | 4550.69] loss=1.50 avg=1.09
[1939 | 4555.96] loss=0.77 avg=1.09
[1940 | 4561.14] loss=1.15 avg=1.09
[1941 | 4566.42] loss=1.16 avg=1.09
[1942 | 4571.52] loss=1.06 avg=1.09
[1943 | 4576.66] loss=1.58 avg=1.10
[1944 | 4581.88] loss=1.00 avg=1.09
[1945 | 4587.18] loss=0.83 avg=1.09
[1946 | 4592.34] loss=1.40 avg=1.09
[1947 | 4597.58] loss=1.20 avg=1.10
[1948 | 4602.82] loss=0.63 avg=1.09
[1949 | 4608.05] loss=0.80 avg=1.09
[1950 | 4613.33] loss=1.45 avg=1.09
[1951 | 4618.56] loss=1.16 avg=1.09
[1952 | 4623.65] loss=1.02 avg=1.09
[1953 | 4628.86] loss=1.48 avg=1.10
[1954 | 4634.03] loss=0.97 avg=1.09
[1955 | 4639.18] loss=1.21 avg=1.10
[1956 | 4644.43] loss=0.95 avg=1.09
[1957 | 4649.68] loss=1.31 avg=1.10
[1958 | 4654.94] loss=1.68 avg=1.10
[1959 | 4660.30] loss=1.18 avg=1.10
[1960 | 4665.63] loss=1.01 avg=1.10
[1961 | 4670.95] loss=0.91 avg=1.10
[1962 | 4676.20] loss=1.03 avg=1.10
[1963 | 4681.36] loss=1.37 avg=1.10
[1964 | 4686.62] loss=0.92 avg=1.10
[1965 | 4691.79] loss=0.70 avg=1.10
[1966 | 4696.97] loss=1.79 avg=1.10
[1967 | 4702.10] loss=0.83 avg=1.10
[1968 | 4707.28] loss=1.49 avg=1.10
[1969 | 4712.43] loss=2.09 avg=1.11
[1970 | 4717.66] loss=1.43 avg=1.12
[1971 | 4722.92] loss=1.11 avg=1.12
[1972 | 4728.13] loss=0.82 avg=1.11
[1973 | 4733.34] loss=1.46 avg=1.12
[1974 | 4738.47] loss=0.80 avg=1.11
[1975 | 4743.68] loss=1.46 avg=1.12
[1976 | 4748.96] loss=1.08 avg=1.12
[1977 | 4754.21] loss=0.78 avg=1.11
[1978 | 4759.42] loss=0.89 avg=1.11
[1979 | 4764.88] loss=1.23 avg=1.11
[1980 | 4770.22] loss=1.61 avg=1.12
[1981 | 4775.58] loss=1.54 avg=1.12
[1982 | 4780.94] loss=0.91 avg=1.12
[1983 | 4786.25] loss=1.41 avg=1.12
[1984 | 4791.52] loss=0.96 avg=1.12
[1985 | 4796.84] loss=0.99 avg=1.12
[1986 | 4802.11] loss=2.19 avg=1.13
[1987 | 4807.44] loss=1.73 avg=1.14
[1988 | 4812.69] loss=1.10 avg=1.14
[1989 | 4817.98] loss=0.86 avg=1.13
[1990 | 4823.34] loss=1.36 avg=1.14
[1991 | 4828.69] loss=0.82 avg=1.13
[1992 | 4833.77] loss=1.73 avg=1.14
[1993 | 4839.12] loss=0.96 avg=1.14
[1994 | 4844.45] loss=1.39 avg=1.14
[1995 | 4849.56] loss=0.92 avg=1.14
[1996 | 4854.84] loss=0.93 avg=1.14
[1997 | 4860.06] loss=1.17 avg=1.14
[1998 | 4865.28] loss=0.66 avg=1.13
[1999 | 4870.45] loss=1.11 avg=1.13
Saving checkpoint/run1/model-2000
Generating samples...
======== SAMPLE 1 ========
              n: int(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(str(dict(str(str(str(str(str(str(str(str(str(string(string(str(str(str(string(str(str( str (str(str(str(str(str(str(str(str(string(str(str(string(int(str(str(string(string(str(str(str(str(str(string(str(str(str(Str(string(string(string(str(name(string(str(str(str(string(string()str(str(string(str()id(string(new__class__id("type.class.name.str.name.name.name.name.name.desc.name.name.desc.name.desc.name.desc.name.desc.name.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.name.desc.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.xid.yid.yid.xid.xid.xid.yid.xid.xid.xid.xid.xid.xid.xid.xid.xid.winsix.yid.v2.xid.xid.xid.xid.xid.zid.zid.winsix.yid.v2.xid.xid.zid.zid.zid.winsix.yid.v2.xid.zid.zid.zid.zid.zid.widix.yid.widix.yid.widix.yid.widix.xids.xids.xids.xids.xids.xids.xids.yids.yids.xids.xids.xids.xids.yids.zid.zid.zid.zid.zid.zid.widik.widix.xids.xids.xids.xids.xids.xids.zids.widix.xids.xids.xids.yids.zids.widix.xids.xids.xids.xids.xids.yids.zids.widix.xids.xids.xids.zids.widix.xids.xids.zids.xids.zids.widix.xids.'

[2000 | 4957.07] loss=1.96 avg=1.14
[2001 | 4962.22] loss=0.89 avg=1.14
[2002 | 4967.48] loss=0.93 avg=1.13
[2003 | 4972.70] loss=1.04 avg=1.13
[2004 | 4977.83] loss=0.97 avg=1.13
[2005 | 4983.09] loss=0.73 avg=1.13
[2006 | 4988.39] loss=1.40 avg=1.13
[2007 | 4993.63] loss=0.92 avg=1.13
[2008 | 4998.84] loss=0.66 avg=1.12
[2009 | 5003.95] loss=0.81 avg=1.12
[2010 | 5009.09] loss=0.90 avg=1.12
[2011 | 5014.28] loss=1.24 avg=1.12
[2012 | 5019.42] loss=1.30 avg=1.12
[2013 | 5024.69] loss=1.40 avg=1.12
[2014 | 5029.97] loss=0.95 avg=1.12
[2015 | 5035.27] loss=1.16 avg=1.12
[2016 | 5040.48] loss=1.01 avg=1.12
[2017 | 5045.70] loss=1.19 avg=1.12
[2018 | 5050.91] loss=0.93 avg=1.12
[2019 | 5056.28] loss=1.57 avg=1.12
[2020 | 5061.62] loss=0.94 avg=1.12
[2021 | 5066.83] loss=1.16 avg=1.12
[2022 | 5072.01] loss=1.26 avg=1.12
[2023 | 5077.25] loss=0.65 avg=1.12
[2024 | 5082.54] loss=1.06 avg=1.12
[2025 | 5087.88] loss=0.93 avg=1.12
[2026 | 5093.13] loss=1.19 avg=1.12
[2027 | 5098.35] loss=0.83 avg=1.12
[2028 | 5103.48] loss=0.85 avg=1.11
[2029 | 5108.62] loss=0.63 avg=1.11
[2030 | 5113.82] loss=0.97 avg=1.11
[2031 | 5118.98] loss=0.84 avg=1.10
[2032 | 5124.20] loss=1.26 avg=1.11
[2033 | 5129.39] loss=0.81 avg=1.10
[2034 | 5134.61] loss=1.75 avg=1.11
[2035 | 5139.93] loss=0.72 avg=1.10
[2036 | 5145.20] loss=1.21 avg=1.11
[2037 | 5150.46] loss=1.37 avg=1.11
[2038 | 5155.78] loss=1.79 avg=1.12
[2039 | 5161.02] loss=0.98 avg=1.11
[2040 | 5166.26] loss=0.64 avg=1.11
[2041 | 5171.54] loss=0.84 avg=1.11
[2042 | 5177.09] loss=2.32 avg=1.12
[2043 | 5182.36] loss=1.61 avg=1.12
[2044 | 5187.61] loss=0.79 avg=1.12
[2045 | 5192.85] loss=1.00 avg=1.12
[2046 | 5198.13] loss=0.99 avg=1.12
[2047 | 5203.33] loss=1.17 avg=1.12
[2048 | 5208.51] loss=0.77 avg=1.11
[2049 | 5213.73] loss=0.98 avg=1.11
[2050 | 5219.08] loss=1.21 avg=1.11
[2051 | 5224.37] loss=0.87 avg=1.11
[2052 | 5229.75] loss=0.93 avg=1.11
[2053 | 5235.22] loss=0.70 avg=1.11
[2054 | 5240.59] loss=0.75 avg=1.10
[2055 | 5245.96] loss=1.46 avg=1.11
[2056 | 5251.45] loss=1.12 avg=1.11
[2057 | 5256.98] loss=1.99 avg=1.12
[2058 | 5262.28] loss=1.21 avg=1.12
[2059 | 5267.57] loss=1.39 avg=1.12
[2060 | 5272.82] loss=1.13 avg=1.12
[2061 | 5278.16] loss=0.94 avg=1.12
[2062 | 5283.42] loss=1.18 avg=1.12
[2063 | 5288.56] loss=0.83 avg=1.11
[2064 | 5293.97] loss=2.08 avg=1.12
[2065 | 5298.99] loss=0.95 avg=1.12
[2066 | 5304.30] loss=1.48 avg=1.13
[2067 | 5309.70] loss=1.87 avg=1.13
[2068 | 5315.08] loss=1.44 avg=1.14
[2069 | 5320.57] loss=0.82 avg=1.13
[2070 | 5325.86] loss=0.88 avg=1.13
[2071 | 5331.15] loss=0.62 avg=1.13
[2072 | 5336.66] loss=0.80 avg=1.12
[2073 | 5341.94] loss=1.76 avg=1.13
[2074 | 5347.32] loss=0.86 avg=1.13
[2075 | 5352.60] loss=0.63 avg=1.12
[2076 | 5357.87] loss=0.82 avg=1.12
[2077 | 5363.37] loss=1.19 avg=1.12
[2078 | 5368.88] loss=1.19 avg=1.12
[2079 | 5374.28] loss=0.81 avg=1.12
[2080 | 5379.58] loss=1.00 avg=1.12
[2081 | 5384.99] loss=0.69 avg=1.11
[2082 | 5390.28] loss=0.90 avg=1.11
[2083 | 5395.50] loss=1.54 avg=1.11
[2084 | 5400.98] loss=0.53 avg=1.11
[2085 | 5406.33] loss=0.91 avg=1.11
[2086 | 5411.73] loss=0.77 avg=1.10
[2087 | 5416.99] loss=1.70 avg=1.11
[2088 | 5422.45] loss=1.11 avg=1.11
[2089 | 5427.80] loss=0.68 avg=1.10
[2090 | 5432.98] loss=1.34 avg=1.11
[2091 | 5438.30] loss=1.33 avg=1.11
[2092 | 5443.78] loss=1.02 avg=1.11
[2093 | 5449.19] loss=1.18 avg=1.11
[2094 | 5454.57] loss=0.92 avg=1.11
[2095 | 5459.87] loss=0.86 avg=1.10
[2096 | 5465.35] loss=1.06 avg=1.10
[2097 | 5470.70] loss=0.96 avg=1.10
[2098 | 5476.12] loss=0.78 avg=1.10
[2099 | 5481.50] loss=2.25 avg=1.11
Generating samples...
======== SAMPLE 1 ========
()             if not self.is_a_todata(todata)    else       self.is_todata(todata)     
            self.has_a_todata(todata) )  
except (NoFaultError:
            raise ValidationError("Error getting todata of %s: {}") )
            if self.is_todata()   else self.has_a_todata()  else self.has_a_todata()  
         )      return self._send_callback(self.get_todata(todata)
            ), _get_result_from_callback(self, None, self.name, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,

[2100 | 5570.55] loss=0.82 avg=1.11
[2101 | 5575.76] loss=0.71 avg=1.10
[2102 | 5580.96] loss=1.32 avg=1.11
[2103 | 5586.20] loss=1.23 avg=1.11
[2104 | 5591.43] loss=1.14 avg=1.11
[2105 | 5596.99] loss=1.04 avg=1.11
[2106 | 5602.40] loss=1.32 avg=1.11
[2107 | 5607.95] loss=1.42 avg=1.11
[2108 | 5613.48] loss=0.87 avg=1.11
[2109 | 5618.99] loss=0.83 avg=1.11
[2110 | 5624.35] loss=1.26 avg=1.11
[2111 | 5629.66] loss=0.96 avg=1.11
[2112 | 5634.88] loss=0.97 avg=1.11
[2113 | 5640.19] loss=0.61 avg=1.10
[2114 | 5645.48] loss=1.19 avg=1.10
[2115 | 5650.76] loss=0.87 avg=1.10
[2116 | 5656.13] loss=0.78 avg=1.10
[2117 | 5661.45] loss=0.68 avg=1.09
[2118 | 5666.74] loss=0.66 avg=1.09
[2119 | 5671.89] loss=0.96 avg=1.09
[2120 | 5677.21] loss=1.13 avg=1.09
[2121 | 5682.52] loss=1.65 avg=1.09
[2122 | 5687.88] loss=0.55 avg=1.09
[2123 | 5693.20] loss=1.17 avg=1.09
[2124 | 5698.48] loss=0.58 avg=1.08
[2125 | 5703.64] loss=1.02 avg=1.08
[2126 | 5708.91] loss=0.91 avg=1.08
[2127 | 5714.27] loss=1.02 avg=1.08
[2128 | 5719.54] loss=1.55 avg=1.08
[2129 | 5724.84] loss=0.99 avg=1.08
[2130 | 5730.11] loss=1.71 avg=1.09
[2131 | 5735.26] loss=1.24 avg=1.09
[2132 | 5740.45] loss=1.27 avg=1.09
[2133 | 5745.75] loss=1.53 avg=1.10
[2134 | 5750.99] loss=1.57 avg=1.10
[2135 | 5756.16] loss=0.72 avg=1.10
[2136 | 5761.55] loss=0.77 avg=1.09
[2137 | 5766.91] loss=1.00 avg=1.09
[2138 | 5772.14] loss=1.08 avg=1.09
[2139 | 5777.29] loss=1.16 avg=1.09
[2140 | 5782.53] loss=0.79 avg=1.09
[2141 | 5787.71] loss=0.98 avg=1.09
[2142 | 5793.01] loss=0.94 avg=1.09
[2143 | 5798.42] loss=0.51 avg=1.08
[2144 | 5803.80] loss=0.69 avg=1.08
[2145 | 5809.24] loss=1.12 avg=1.08
[2146 | 5814.49] loss=1.08 avg=1.08
[2147 | 5819.84] loss=1.20 avg=1.08
[2148 | 5825.11] loss=0.84 avg=1.08
[2149 | 5830.51] loss=0.87 avg=1.08
[2150 | 5835.92] loss=1.37 avg=1.08
[2151 | 5841.27] loss=1.05 avg=1.08
[2152 | 5846.63] loss=1.41 avg=1.08
[2153 | 5851.98] loss=0.86 avg=1.08
[2154 | 5857.28] loss=1.37 avg=1.08
[2155 | 5862.48] loss=0.78 avg=1.08
[2156 | 5867.92] loss=1.08 avg=1.08
[2157 | 5873.34] loss=0.70 avg=1.08
[2158 | 5878.67] loss=1.19 avg=1.08
[2159 | 5884.13] loss=1.23 avg=1.08
[2160 | 5889.43] loss=1.07 avg=1.08
[2161 | 5894.93] loss=1.23 avg=1.08
[2162 | 5900.40] loss=1.33 avg=1.08
[2163 | 5905.75] loss=1.16 avg=1.08
[2164 | 5911.08] loss=2.18 avg=1.09
[2165 | 5916.53] loss=0.74 avg=1.09
[2166 | 5921.87] loss=0.80 avg=1.09
[2167 | 5927.18] loss=0.99 avg=1.09
[2168 | 5932.56] loss=1.00 avg=1.09
[2169 | 5937.92] loss=0.93 avg=1.08
[2170 | 5943.21] loss=0.66 avg=1.08
[2171 | 5948.50] loss=0.93 avg=1.08
[2172 | 5953.83] loss=1.11 avg=1.08
[2173 | 5958.99] loss=1.19 avg=1.08
[2174 | 5964.39] loss=1.43 avg=1.08
[2175 | 5969.81] loss=1.49 avg=1.09
[2176 | 5975.32] loss=0.71 avg=1.08
[2177 | 5980.70] loss=1.24 avg=1.09
[2178 | 5986.10] loss=0.76 avg=1.08
[2179 | 5991.72] loss=0.78 avg=1.08
[2180 | 5996.96] loss=0.84 avg=1.08
[2181 | 6002.26] loss=1.22 avg=1.08
[2182 | 6007.58] loss=1.51 avg=1.08
[2183 | 6012.93] loss=0.60 avg=1.08
[2184 | 6018.28] loss=0.82 avg=1.08
[2185 | 6023.73] loss=1.14 avg=1.08
[2186 | 6029.03] loss=1.21 avg=1.08
[2187 | 6034.34] loss=0.82 avg=1.07
[2188 | 6039.75] loss=1.02 avg=1.07
[2189 | 6045.00] loss=1.04 avg=1.07
[2190 | 6050.33] loss=0.94 avg=1.07
[2191 | 6055.79] loss=0.99 avg=1.07
[2192 | 6061.40] loss=1.14 avg=1.07
[2193 | 6066.94] loss=0.85 avg=1.07
[2194 | 6072.45] loss=1.60 avg=1.08
[2195 | 6077.82] loss=1.64 avg=1.08
[2196 | 6083.15] loss=1.10 avg=1.08
[2197 | 6088.42] loss=1.12 avg=1.08
[2198 | 6093.73] loss=1.27 avg=1.08
[2199 | 6099.00] loss=1.43 avg=1.09
Generating samples...
======== SAMPLE 1 ========


[u'SetUp', u'(', u'two', u')', u"'predict_calls_from_all_cancel_cancel'", u',', u'does', u'=', u'os', u'.', u'get', u'(', u'os', u')', u'.', u'predict_calls', u'[', u'does', u']', u')', u'or', u'f', u'=', u'os', u'.', u'predict_calls', u'[', u'does']', u'or', u'm', u'=', u'os', u'.', u'_query', u'.', u'get', u'(', u'os', u')', u'.', u'numpy_mesh2', u'(', u'v', u')', u')', u'and', u'r', u'=', u'0', u'if', u'os', u'.', u'is_numpy', u'(', u'r', u')', u':', u'if', u'n_range', u'==', u'0', u':', u'if', u'(', u'os', u'.', u'is_numpy', u'(', u'v', u')', u')', u':', u'os', u'.', u'assert_isinstance', u'(', u'v', u'a', u')', u')', u'if', u'(', u'two', u',', u'o', u'.', u'_numpy', u'(', u')', u')', u':', u'if', u'(', u'os', u'.', u'is_numpy', u'(', u'v', u')', u',', u'o', u'.', u'_numpy', u'(', u')', u')', u')', u':', u'else', u':', u'if', u'(', u'os', u'.', u'is_numpy', u'(', u'o', u')', u')', u':', u'if', u'(', u'>', u'cancel', u')', u':', u'if', u'f', u'=', u'os', u'.', u'predict_calls_to_cancel_f', u'[', u'f', u'>', u'0', u':', u'subclass', u'.', u'is_numpy', u'(', u'v', u')', u')', u')', u':', u'if', u'(', u'os', u'.', u'is_numpy', u'(', u'n', u'_range', u'==', u'0', u')', u')', u')', u':', u'os', u'.', u'assert_isinstance', u'(', u'v', u'a', u')', u'if', u'(', u'two', u',', u'o', u'.', u'_numpy', u'(', u')', u')', u')', u':', u'if', u'(', u'os', u'.', u'is_numpy', u'(', u'o', u')', u',', u'o', u')', u':', u'else', u':', u'if', u'(', u'os', u'.', u'is_numpy', u'(', u'v', u')', u',', u'o', u')', u')', u':', u'if', u'(', u'o', u'.', u'_numpy', u'(', u'v', u')', u')', u':', u'if', u'(', u'o', u'.', u'_numpy', u'(', u'n', u'_range', u'==', u'0', u')', u')', u')', u':', u'else', u':', u'if', u'(', u'two', u',', u'o', u'.', u'_numpy', u'(', u')', u')', u')', u':', u'if', u'(', u'two', u',', u'o', u'.', u'_n

[2200 | 6184.01] loss=1.11 avg=1.09
[2201 | 6189.18] loss=0.98 avg=1.09
[2202 | 6194.39] loss=0.73 avg=1.08
[2203 | 6199.69] loss=1.20 avg=1.08
[2204 | 6204.94] loss=0.84 avg=1.08
[2205 | 6210.22] loss=0.74 avg=1.08
[2206 | 6215.58] loss=0.74 avg=1.07
[2207 | 6221.17] loss=1.35 avg=1.08
[2208 | 6226.55] loss=1.37 avg=1.08
[2209 | 6231.87] loss=0.81 avg=1.08
[2210 | 6237.17] loss=0.73 avg=1.07
[2211 | 6242.48] loss=1.07 avg=1.07
[2212 | 6247.81] loss=1.19 avg=1.08
[2213 | 6253.11] loss=0.84 avg=1.07
[2214 | 6258.40] loss=0.43 avg=1.07
[2215 | 6263.77] loss=0.85 avg=1.06
[2216 | 6269.20] loss=1.21 avg=1.07
[2217 | 6274.40] loss=1.01 avg=1.07
[2218 | 6279.60] loss=0.50 avg=1.06
[2219 | 6284.92] loss=1.64 avg=1.07
[2220 | 6290.23] loss=1.20 avg=1.07
[2221 | 6295.45] loss=1.13 avg=1.07
[2222 | 6300.69] loss=0.80 avg=1.06
[2223 | 6305.92] loss=0.68 avg=1.06
[2224 | 6311.09] loss=0.55 avg=1.06
[2225 | 6316.24] loss=1.03 avg=1.06
[2226 | 6321.43] loss=1.03 avg=1.06
[2227 | 6326.69] loss=1.05 avg=1.06
[2228 | 6331.93] loss=0.87 avg=1.05
[2229 | 6337.12] loss=1.27 avg=1.06
[2230 | 6342.39] loss=0.99 avg=1.05
[2231 | 6347.57] loss=0.60 avg=1.05
[2232 | 6352.71] loss=1.56 avg=1.06
[2233 | 6357.91] loss=1.03 avg=1.06
[2234 | 6363.10] loss=1.36 avg=1.06
[2235 | 6368.26] loss=1.00 avg=1.06
[2236 | 6373.61] loss=0.94 avg=1.06
[2237 | 6378.81] loss=1.26 avg=1.06
[2238 | 6384.19] loss=1.11 avg=1.06
[2239 | 6389.56] loss=0.85 avg=1.06
[2240 | 6394.87] loss=1.85 avg=1.06
[2241 | 6400.14] loss=0.98 avg=1.06
[2242 | 6405.43] loss=0.91 avg=1.06
[2243 | 6410.64] loss=0.84 avg=1.06
[2244 | 6415.94] loss=0.71 avg=1.06
[2245 | 6421.18] loss=1.11 avg=1.06
[2246 | 6426.31] loss=1.35 avg=1.06
[2247 | 6431.49] loss=0.63 avg=1.06
[2248 | 6436.71] loss=1.75 avg=1.06
[2249 | 6442.03] loss=1.09 avg=1.06
[2250 | 6447.39] loss=1.10 avg=1.06
[2251 | 6452.62] loss=1.26 avg=1.07
[2252 | 6457.82] loss=0.66 avg=1.06
[2253 | 6463.20] loss=1.10 avg=1.06
[2254 | 6468.50] loss=1.05 avg=1.06
[2255 | 6473.68] loss=1.00 avg=1.06
[2256 | 6478.97] loss=1.06 avg=1.06
[2257 | 6484.19] loss=0.96 avg=1.06
[2258 | 6489.38] loss=0.84 avg=1.06
[2259 | 6494.57] loss=1.03 avg=1.06
[2260 | 6499.87] loss=0.82 avg=1.06
[2261 | 6505.18] loss=0.56 avg=1.05
[2262 | 6510.38] loss=1.10 avg=1.05
[2263 | 6515.69] loss=1.04 avg=1.05
[2264 | 6520.98] loss=0.88 avg=1.05
[2265 | 6526.37] loss=1.39 avg=1.05
[2266 | 6531.81] loss=1.57 avg=1.06
[2267 | 6537.12] loss=1.09 avg=1.06
[2268 | 6542.39] loss=1.00 avg=1.06
[2269 | 6547.77] loss=0.50 avg=1.05
[2270 | 6553.02] loss=0.95 avg=1.05
[2271 | 6558.26] loss=1.16 avg=1.05
[2272 | 6563.58] loss=1.13 avg=1.05
[2273 | 6568.92] loss=0.85 avg=1.05
[2274 | 6574.30] loss=1.30 avg=1.05
[2275 | 6579.73] loss=1.64 avg=1.06
[2276 | 6585.10] loss=0.90 avg=1.06
[2277 | 6590.31] loss=0.92 avg=1.06
[2278 | 6595.45] loss=0.71 avg=1.05
[2279 | 6600.72] loss=0.64 avg=1.05
[2280 | 6605.89] loss=1.17 avg=1.05
[2281 | 6611.17] loss=1.31 avg=1.05
[2282 | 6616.39] loss=1.21 avg=1.05
[2283 | 6621.55] loss=1.21 avg=1.06
[2284 | 6626.70] loss=1.23 avg=1.06
[2285 | 6631.90] loss=0.93 avg=1.06
[2286 | 6637.10] loss=1.18 avg=1.06
[2287 | 6642.33] loss=1.26 avg=1.06
[2288 | 6647.57] loss=0.73 avg=1.06
[2289 | 6652.83] loss=0.70 avg=1.05
[2290 | 6658.08] loss=0.70 avg=1.05
[2291 | 6663.38] loss=1.78 avg=1.06
[2292 | 6668.55] loss=0.65 avg=1.05
[2293 | 6673.82] loss=1.16 avg=1.05
[2294 | 6679.13] loss=1.08 avg=1.05
[2295 | 6684.42] loss=1.00 avg=1.05
[2296 | 6689.61] loss=0.90 avg=1.05
[2297 | 6694.71] loss=1.57 avg=1.06
[2298 | 6699.86] loss=0.65 avg=1.05
[2299 | 6705.10] loss=0.87 avg=1.05
Generating samples...
======== SAMPLE 1 ========
                                                          ) 
                                                            '(if (isNmutable_key_type(n_n): # not sure if n_type? if n_type is not an integer and n_type is None then # check if n_type is already a tuple of an integer and n_type has not been set and # notifies us if n_type is a tuple of non-zero values and # do not change whether it is a tuple of non-zero values or a tuple of any # n_types)
                                
                  except: 
                                                   "Error:
                                                                    "Could not find n_n_type, n_type or n_name".format(n_name, n_type) or '{' if n_n_type but is not a tuple from the string. # if it is a tuple of any number of ints or is an integer, that number #                      #                       #                                  #                                                                                                                                                                                                                                   )})                                       elif n_num >= 2 :
                                              'No argument passed.'
                                                       return (
                                                  (n_num, 0))
                              else :
        

[2300 | 6792.85] loss=0.73 avg=1.05
[2301 | 6798.17] loss=0.58 avg=1.04
[2302 | 6803.56] loss=0.97 avg=1.04
[2303 | 6808.94] loss=1.13 avg=1.04
[2304 | 6814.25] loss=0.58 avg=1.04
[2305 | 6819.60] loss=1.61 avg=1.04
[2306 | 6824.79] loss=0.99 avg=1.04
[2307 | 6830.04] loss=1.69 avg=1.05
[2308 | 6835.25] loss=0.77 avg=1.05
[2309 | 6840.51] loss=0.92 avg=1.05
[2310 | 6845.62] loss=0.87 avg=1.04
[2311 | 6850.87] loss=0.55 avg=1.04
[2312 | 6856.25] loss=1.02 avg=1.04
[2313 | 6861.53] loss=1.58 avg=1.04
[2314 | 6866.76] loss=0.61 avg=1.04
[2315 | 6871.84] loss=1.07 avg=1.04
[2316 | 6877.06] loss=0.84 avg=1.04
[2317 | 6882.25] loss=1.72 avg=1.04
[2318 | 6887.58] loss=0.74 avg=1.04
[2319 | 6892.75] loss=0.86 avg=1.04
[2320 | 6897.89] loss=1.02 avg=1.04
[2321 | 6903.13] loss=1.55 avg=1.04
[2322 | 6908.31] loss=0.55 avg=1.04
[2323 | 6913.58] loss=0.82 avg=1.04
[2324 | 6918.91] loss=1.14 avg=1.04
[2325 | 6924.20] loss=1.04 avg=1.04
[2326 | 6929.46] loss=1.09 avg=1.04
[2327 | 6934.77] loss=0.93 avg=1.04
[2328 | 6940.13] loss=1.04 avg=1.04
[2329 | 6945.54] loss=1.04 avg=1.04
[2330 | 6951.02] loss=0.74 avg=1.04
[2331 | 6956.40] loss=1.01 avg=1.03
[2332 | 6961.92] loss=1.72 avg=1.04
[2333 | 6967.25] loss=1.09 avg=1.04
[2334 | 6972.52] loss=1.45 avg=1.05
[2335 | 6977.88] loss=0.94 avg=1.05
[2336 | 6983.24] loss=0.81 avg=1.04
[2337 | 6988.71] loss=0.93 avg=1.04
[2338 | 6993.98] loss=1.23 avg=1.04
[2339 | 6999.33] loss=1.46 avg=1.05
[2340 | 7004.73] loss=0.78 avg=1.05
[2341 | 7010.02] loss=0.87 avg=1.04
[2342 | 7015.25] loss=1.59 avg=1.05
[2343 | 7020.50] loss=1.13 avg=1.05
[2344 | 7025.78] loss=1.54 avg=1.05
[2345 | 7031.07] loss=0.68 avg=1.05
[2346 | 7036.39] loss=0.89 avg=1.05
[2347 | 7041.70] loss=0.63 avg=1.05
[2348 | 7047.04] loss=1.11 avg=1.05
[2349 | 7052.23] loss=0.84 avg=1.04
[2350 | 7057.61] loss=1.10 avg=1.04
[2351 | 7063.09] loss=1.60 avg=1.05
[2352 | 7068.41] loss=0.71 avg=1.05
[2353 | 7073.77] loss=0.74 avg=1.04
[2354 | 7079.08] loss=2.05 avg=1.05
[2355 | 7084.49] loss=1.71 avg=1.06
[2356 | 7089.79] loss=1.65 avg=1.07
[2357 | 7095.17] loss=0.44 avg=1.06
[2358 | 7100.36] loss=0.92 avg=1.06
[2359 | 7105.56] loss=1.09 avg=1.06
[2360 | 7110.82] loss=1.48 avg=1.06
[2361 | 7115.97] loss=1.14 avg=1.06
[2362 | 7121.18] loss=0.61 avg=1.06
[2363 | 7126.35] loss=0.85 avg=1.06
[2364 | 7131.80] loss=1.02 avg=1.06
[2365 | 7137.14] loss=1.77 avg=1.06
[2366 | 7142.47] loss=0.80 avg=1.06
[2367 | 7147.77] loss=0.73 avg=1.06
[2368 | 7153.07] loss=1.12 avg=1.06
[2369 | 7158.48] loss=1.00 avg=1.06
[2370 | 7163.90] loss=0.98 avg=1.06
[2371 | 7169.19] loss=0.89 avg=1.05
[2372 | 7174.40] loss=0.91 avg=1.05
[2373 | 7179.75] loss=0.61 avg=1.05
[2374 | 7185.01] loss=0.71 avg=1.05
[2375 | 7190.40] loss=0.92 avg=1.04
[2376 | 7195.72] loss=1.01 avg=1.04
[2377 | 7201.04] loss=2.02 avg=1.05
[2378 | 7206.37] loss=1.08 avg=1.05
[2379 | 7211.71] loss=1.17 avg=1.06
[2380 | 7216.85] loss=1.42 avg=1.06
[2381 | 7222.17] loss=0.84 avg=1.06
[2382 | 7227.48] loss=0.78 avg=1.05
[2383 | 7232.74] loss=0.78 avg=1.05
[2384 | 7238.09] loss=2.06 avg=1.06
[2385 | 7243.39] loss=1.39 avg=1.06
[2386 | 7248.67] loss=1.37 avg=1.07
[2387 | 7253.88] loss=0.80 avg=1.07
[2388 | 7259.14] loss=1.98 avg=1.07
[2389 | 7264.32] loss=1.03 avg=1.07
[2390 | 7269.61] loss=0.96 avg=1.07
[2391 | 7274.82] loss=0.72 avg=1.07
[2392 | 7280.04] loss=0.75 avg=1.07
[2393 | 7285.32] loss=1.27 avg=1.07
[2394 | 7290.56] loss=0.77 avg=1.07
[2395 | 7295.76] loss=1.22 avg=1.07
[2396 | 7301.08] loss=0.87 avg=1.06
[2397 | 7306.40] loss=0.68 avg=1.06
[2398 | 7311.72] loss=1.50 avg=1.07
[2399 | 7317.06] loss=0.93 avg=1.06
Generating samples...
======== SAMPLE 1 ========
name.field
        """
        for k, v in enumerate(__file__):
            if v in None:
                if v in dict(k,  ):
                   return v [u'def', u'_file__', u'(', u'self', u',', u'__file__', u')', u':', u'for', u'k', u',', u'v', u'in', u'getattr', u'(', u'__file__', u')', u':', u'if', u'v', u'in', u'None', u':', u'if', u'v', u'in', u'zeros', u':', u'v', u'=', u'[', u']', u'self', u'.', u'_list_string', u'for', u'k', u'in', u'__file__', u'.', u'iteritems', u'(', u')', u':', u'while', u'not', u'v', u'.', u'exists', u':', u'v', u'=', u'self', u'.', u'_list_string', u'(', u'v', u'[', u"'name'", u']', u')', u'if', u':', u'v', u'[', u"'name'", u']', u'=', u'__name_get', u'(', u'v', u')', u'else', u':', u'v', u'=', u'"__name__"', u'for', u'k', u',', u'v', u',', u'v', u'in', u'getattr', u'(', u'__file__', u')', u':', u'if', u'k', u'in', u'getattr', u'(', u'k', u')', u':', u'return', u'v', u'[', u"'name'", u']', u'join', u'(', u'self', u'.', u'_list_string', u',', u'# Add name to dict. We'll use one for the future'] dir_file__:
        self.__FILE__.__name___.setvalue python def _file__(self, file=False):
        """
        for k, v, i in enumerate(__file__):
            if v in None:
                if v in dictionary(k,  ):
                   return v train dg/python/python3/python_3.py dg/python/python3 5e3c9f92c090bdb8c9bef6f08c9f8dccdc https://github.com/dg/python/python/blob/5e3c9f92c090bdb8c9bef6f08c9f8dccdc/dg/python/python3/python_3.py#L22-L32
def _path__(self, 'file__')):
        """
        Create all namespace paths to be handled by the DLL.

        :param nameswitch: Whether to handle paths in the directory: bool, default None.

        :method nameswitch: If False, then use pathswitch
          
        """
        if self._path_exists(self._path__):
           if self._path_exists(self._path__):
                self._dir = self._get_saveddir_path
              else:
                  self._dir_path_ex

[2400 | 7405.02] loss=0.91 avg=1.06
[2401 | 7410.21] loss=1.03 avg=1.06
[2402 | 7415.39] loss=1.04 avg=1.06
[2403 | 7420.65] loss=1.29 avg=1.06
[2404 | 7425.90] loss=1.02 avg=1.06
[2405 | 7431.16] loss=0.99 avg=1.06
[2406 | 7436.40] loss=0.76 avg=1.06
[2407 | 7441.62] loss=1.34 avg=1.06
[2408 | 7447.01] loss=1.22 avg=1.06
[2409 | 7452.33] loss=1.25 avg=1.07
[2410 | 7457.71] loss=0.80 avg=1.06
[2411 | 7462.98] loss=0.94 avg=1.06
[2412 | 7468.24] loss=1.03 avg=1.06
[2413 | 7473.49] loss=1.15 avg=1.06
[2414 | 7478.63] loss=0.98 avg=1.06
[2415 | 7483.96] loss=0.81 avg=1.06
[2416 | 7489.27] loss=0.59 avg=1.06
[2417 | 7494.67] loss=1.24 avg=1.06
[2418 | 7499.95] loss=1.02 avg=1.06
[2419 | 7505.29] loss=0.73 avg=1.05
[2420 | 7510.49] loss=0.60 avg=1.05
[2421 | 7515.71] loss=1.14 avg=1.05
[2422 | 7521.09] loss=0.86 avg=1.05
[2423 | 7526.27] loss=0.97 avg=1.05
[2424 | 7531.48] loss=1.19 avg=1.05
[2425 | 7536.82] loss=1.12 avg=1.05
[2426 | 7542.16] loss=0.73 avg=1.05
[2427 | 7547.63] loss=1.44 avg=1.05
[2428 | 7553.17] loss=0.51 avg=1.04
[2429 | 7558.71] loss=1.61 avg=1.05
[2430 | 7564.16] loss=1.97 avg=1.06
[2431 | 7569.46] loss=0.85 avg=1.06
[2432 | 7574.94] loss=2.04 avg=1.07
[2433 | 7580.23] loss=0.87 avg=1.06
[2434 | 7585.43] loss=1.20 avg=1.07
[2435 | 7590.57] loss=1.33 avg=1.07
[2436 | 7595.62] loss=1.21 avg=1.07
[2437 | 7600.74] loss=1.96 avg=1.08
[2438 | 7606.08] loss=1.12 avg=1.08
[2439 | 7611.36] loss=1.08 avg=1.08
[2440 | 7616.66] loss=0.89 avg=1.08
[2441 | 7622.07] loss=0.83 avg=1.08
[2442 | 7627.37] loss=1.24 avg=1.08
[2443 | 7632.74] loss=1.25 avg=1.08
[2444 | 7637.98] loss=1.91 avg=1.09
[2445 | 7643.32] loss=0.90 avg=1.09
[2446 | 7648.73] loss=1.11 avg=1.09
[2447 | 7654.01] loss=1.08 avg=1.09
[2448 | 7659.44] loss=0.66 avg=1.08
[2449 | 7664.72] loss=1.14 avg=1.08
[2450 | 7670.03] loss=1.08 avg=1.08
[2451 | 7675.28] loss=1.13 avg=1.08
[2452 | 7680.55] loss=1.33 avg=1.08
[2453 | 7685.91] loss=2.06 avg=1.09
[2454 | 7691.35] loss=0.83 avg=1.09
[2455 | 7696.63] loss=1.03 avg=1.09
[2456 | 7701.92] loss=1.25 avg=1.09
[2457 | 7707.21] loss=0.46 avg=1.09
[2458 | 7712.59] loss=0.84 avg=1.08
[2459 | 7717.93] loss=0.71 avg=1.08
[2460 | 7723.38] loss=1.27 avg=1.08
[2461 | 7728.80] loss=0.83 avg=1.08
[2462 | 7734.18] loss=1.08 avg=1.08
[2463 | 7739.31] loss=1.22 avg=1.08
[2464 | 7744.70] loss=0.78 avg=1.08
[2465 | 7750.07] loss=0.85 avg=1.08
[2466 | 7755.33] loss=0.90 avg=1.07
[2467 | 7760.50] loss=0.76 avg=1.07
[2468 | 7765.76] loss=0.78 avg=1.07
[2469 | 7771.03] loss=1.12 avg=1.07
[2470 | 7776.25] loss=1.04 avg=1.07
[2471 | 7781.54] loss=0.81 avg=1.07
[2472 | 7786.72] loss=1.26 avg=1.07
[2473 | 7792.02] loss=1.14 avg=1.07
[2474 | 7797.36] loss=0.91 avg=1.07
[2475 | 7802.67] loss=0.94 avg=1.07
[2476 | 7808.06] loss=1.16 avg=1.07
[2477 | 7813.46] loss=1.15 avg=1.07
[2478 | 7818.81] loss=0.96 avg=1.07
[2479 | 7824.16] loss=0.82 avg=1.06
[2480 | 7829.49] loss=1.34 avg=1.07
[2481 | 7834.82] loss=0.83 avg=1.06
[2482 | 7840.16] loss=0.99 avg=1.06
[2483 | 7845.49] loss=1.22 avg=1.06
[2484 | 7850.78] loss=0.58 avg=1.06
[2485 | 7856.06] loss=1.28 avg=1.06
[2486 | 7861.37] loss=1.30 avg=1.06
[2487 | 7866.51] loss=1.10 avg=1.06
[2488 | 7871.54] loss=0.92 avg=1.06
[2489 | 7876.80] loss=0.93 avg=1.06
[2490 | 7882.15] loss=0.96 avg=1.06
[2491 | 7887.30] loss=0.88 avg=1.06
[2492 | 7892.55] loss=0.74 avg=1.06
[2493 | 7897.80] loss=0.94 avg=1.06
[2494 | 7903.23] loss=1.06 avg=1.06
[2495 | 7908.55] loss=1.95 avg=1.06
[2496 | 7913.82] loss=0.73 avg=1.06
[2497 | 7919.12] loss=0.77 avg=1.06
[2498 | 7924.33] loss=0.74 avg=1.05
[2499 | 7929.53] loss=1.44 avg=1.06
Generating samples...
======== SAMPLE 1 ========
 u',', u'g_', u')', u'g_', u'=', u'.', u'concatenation', u'sort_to_value', u'if', u'sort_to_value', u'is', u'not', u'None', u':', u'sort_to_value', u'[', u'"value"', u']', u'sort_to_value', u'=', u'sort_to_value', u'[', u"'value'', u']', u'if', u'sort_to_value', u'is', u'not', u'sort_to_value', u':', u'sort_to_value', u'[', u"'value'", u']', u'=', u'sort', u'# Get value [u'Glyphs', u'.', u'register', u'>', u'None', u'if', u'sort_to_value', u'in', u'sort_to_value', u':', u'sort_to_value', u'[', u"'value'", u']', u'=', u'sort_to_value', u'sort_to_value', u'self', u'.', u'_add_concatenation_keys', u'[', u'register', u'!=', u'None', u']', u'try', u':', u'g_', u',', u'g_', u'=', u'.', u'concatenation', u'register', u'=', u'g_', u'.', u'sort_to_value', u'[', u"'value'", u']', u'try', u':', u'_', u'tory', u',', u'to_value', u'sort_to_value', u'[', u"'value'", u']', u'else', u':', u'g_', u',', u'sort_to_value', u'[', u"'value'", u']', u'try', u':', u'sort_to_value', u'[', u"'value'", u']', u'=', u'sort', u'# Get value for g_ for g_' if', u'g_', u'==', u'None', u'sort_to_value', u'[', u"'value'", u']', u'=', u'sort_to_value', u'sort_to_value'] Return a string of values that are the same as
        the `*` keys. Glyphs requires ``g_`` and `g_`` keys. If it exists
    but does not exist, Glyphs does not do anything.

        g_ = Glyphs.parse_value(g_,
        g_=None,
        g_=None,
        g_=None,
        g_=None,
        g_=None,
        g_=None,
        return g_ [u'def', u'dependency_to_value', u'(', u'sort_to_value', u',', u'register', u'=', u'sort_to_value', u')', u':', u'sort_to_value', u'if', u'sort_to_value', u'is', u'not', u'sort_to_value', u':', u'text_to_value', u'sort_to_value', u'[', u"'value'", u']', u'register', u'=', u'text_to_value', u'[', u"'value'", u']', u'if', u'sort_to_value', u'is', u'not', u'sort_to_value', u':', u'sort_to_value', u'[', u"'value'", u']', u'=', u'sort_to_value', u'sort_to_value'] Return a string of values that are the same as
        the `*` keys. Glyphs requires ``g_`` and `g_`` keys. If it exists
        then Glyphs does not do anything.

  

[2500 | 8017.57] loss=1.05 avg=1.06
[2501 | 8023.03] loss=0.70 avg=1.05
[2502 | 8028.31] loss=1.04 avg=1.05
[2503 | 8033.57] loss=0.84 avg=1.05
[2504 | 8038.88] loss=1.05 avg=1.05
[2505 | 8044.19] loss=0.81 avg=1.05
[2506 | 8049.49] loss=1.07 avg=1.05
[2507 | 8054.75] loss=0.85 avg=1.05
[2508 | 8060.08] loss=0.64 avg=1.04
[2509 | 8065.49] loss=1.20 avg=1.05
[2510 | 8070.81] loss=0.71 avg=1.04
[2511 | 8076.11] loss=0.78 avg=1.04
[2512 | 8081.45] loss=1.07 avg=1.04
[2513 | 8086.83] loss=0.77 avg=1.04
[2514 | 8092.08] loss=0.80 avg=1.03
[2515 | 8097.27] loss=0.54 avg=1.03
[2516 | 8102.55] loss=1.09 avg=1.03
[2517 | 8107.78] loss=0.81 avg=1.03
[2518 | 8112.92] loss=1.12 avg=1.03
[2519 | 8118.08] loss=1.25 avg=1.03
[2520 | 8123.25] loss=0.71 avg=1.03
[2521 | 8128.65] loss=0.95 avg=1.03
[2522 | 8134.12] loss=1.70 avg=1.03
[2523 | 8139.57] loss=1.59 avg=1.04
[2524 | 8144.91] loss=0.78 avg=1.04
[2525 | 8150.22] loss=0.90 avg=1.04
[2526 | 8155.59] loss=0.62 avg=1.03
[2527 | 8161.07] loss=1.12 avg=1.03
[2528 | 8166.36] loss=0.80 avg=1.03
[2529 | 8171.57] loss=1.71 avg=1.04
[2530 | 8177.05] loss=0.77 avg=1.03
[2531 | 8182.62] loss=1.28 avg=1.04
[2532 | 8188.05] loss=0.74 avg=1.03
[2533 | 8193.41] loss=1.12 avg=1.03
[2534 | 8198.74] loss=1.19 avg=1.04
[2535 | 8204.14] loss=1.17 avg=1.04
[2536 | 8209.61] loss=1.12 avg=1.04
[2537 | 8214.93] loss=0.80 avg=1.04
[2538 | 8220.14] loss=1.64 avg=1.04
[2539 | 8225.48] loss=0.95 avg=1.04
[2540 | 8230.75] loss=1.84 avg=1.05
[2541 | 8236.05] loss=1.26 avg=1.05
[2542 | 8241.31] loss=0.74 avg=1.05
[2543 | 8246.59] loss=1.38 avg=1.05
[2544 | 8251.91] loss=0.93 avg=1.05
[2545 | 8257.23] loss=1.84 avg=1.06
[2546 | 8262.43] loss=1.02 avg=1.06
[2547 | 8267.76] loss=0.87 avg=1.06
[2548 | 8273.06] loss=1.72 avg=1.06
[2549 | 8278.21] loss=2.07 avg=1.07
[2550 | 8283.49] loss=1.01 avg=1.07
[2551 | 8288.67] loss=0.90 avg=1.07
[2552 | 8293.95] loss=1.14 avg=1.07
[2553 | 8299.24] loss=0.96 avg=1.07
[2554 | 8304.68] loss=1.07 avg=1.07
[2555 | 8310.02] loss=1.74 avg=1.08
[2556 | 8315.23] loss=2.35 avg=1.09
[2557 | 8320.49] loss=1.47 avg=1.09
[2558 | 8325.68] loss=0.79 avg=1.09
[2559 | 8331.02] loss=1.11 avg=1.09
[2560 | 8336.19] loss=0.85 avg=1.09
[2561 | 8341.50] loss=0.74 avg=1.08
[2562 | 8346.78] loss=1.57 avg=1.09
[2563 | 8352.19] loss=2.16 avg=1.10
[2564 | 8357.71] loss=0.92 avg=1.10
[2565 | 8363.05] loss=0.41 avg=1.09
[2566 | 8368.36] loss=1.18 avg=1.09
[2567 | 8373.60] loss=1.51 avg=1.10
[2568 | 8379.10] loss=0.55 avg=1.09
[2569 | 8384.33] loss=1.11 avg=1.09
[2570 | 8389.58] loss=0.77 avg=1.09
[2571 | 8394.86] loss=1.58 avg=1.09
[2572 | 8400.08] loss=1.16 avg=1.09
[2573 | 8405.43] loss=1.50 avg=1.10
[2574 | 8410.84] loss=1.40 avg=1.10
[2575 | 8416.13] loss=1.62 avg=1.11
[2576 | 8421.37] loss=1.16 avg=1.11
[2577 | 8426.52] loss=0.69 avg=1.10
[2578 | 8431.79] loss=0.95 avg=1.10
[2579 | 8437.03] loss=0.87 avg=1.10
[2580 | 8442.25] loss=0.73 avg=1.09
[2581 | 8447.41] loss=1.62 avg=1.10
[2582 | 8452.69] loss=0.50 avg=1.09
[2583 | 8458.24] loss=1.29 avg=1.10
[2584 | 8463.43] loss=1.38 avg=1.10
[2585 | 8468.74] loss=1.14 avg=1.10
[2586 | 8474.01] loss=0.87 avg=1.10
[2587 | 8479.23] loss=0.90 avg=1.09
[2588 | 8484.55] loss=1.09 avg=1.09
[2589 | 8490.06] loss=0.94 avg=1.09
[2590 | 8495.40] loss=0.82 avg=1.09
[2591 | 8500.81] loss=1.10 avg=1.09
[2592 | 8506.16] loss=0.70 avg=1.09
[2593 | 8511.50] loss=1.70 avg=1.09
[2594 | 8516.85] loss=0.95 avg=1.09
[2595 | 8522.13] loss=1.17 avg=1.09
[2596 | 8527.41] loss=1.15 avg=1.09
[2597 | 8532.58] loss=0.66 avg=1.09
[2598 | 8537.71] loss=0.84 avg=1.09
[2599 | 8542.81] loss=1.68 avg=1.09
Generating samples...
======== SAMPLE 1 ========
', u"#'p', u"#', u'p', u'#', u'module', u'[u'', u"'sum'", u']', u'return', u'{', u"'strictly'", u':', u'ret', u',', u"'outbound'", u':', u'vett', u',', u"'outreach'", u':', u'ret', u',', u"'is_dynamic'", u':', u'vett', u',', u"'outbound'", u':', u'vett', u',', u"'is_dynamic_in_mesh'", u':', u'vett', u',', u"'inverse_nodes'", u':', u'drop', u'] [u'def', u'get_pydata_id', u'(', u'nclsx', u',', u'snd', u',', u'module', u',', u'dump', u',', u'ret', u')', u':', u'#', u'p', u'#', u'p', u'#', u'ret', u'#', u'module', u'[', u'"pydata"', u']', u'[', u'"pydata"', u']', u'#', u'module', u'[', u'"pydata"', u']', u'[', u'"pydata"', u']', u'drop', u'(', u"'pydata'", u')', u'dump', u'snd', u'=', u'{', u"'ncls'", u':', u'drop', u'.', u'module', u',', u'dump', u'}', u'else', u':', u'#', u'p', u'#', u'p', u'# ', u'p', u'#,' u'p', u'if', u"'pydata'", u':', u'drop', u'.', u'module', u'[', u'"pydata"', u']', u'[', u'"pydata"', u']', u'.', u'module', u'[', u'"pydata"', u']', u'.', u'module', u'ret', u'=', u'{', u"'pydata'", u':', u'drop', u',', u"'pydata'", u':', u'dump', u'}', u'if', u"'mdl'", u':', u'{', u"', u':", u'{', u"'mdl':', u'drop', u'}', u',', u"'(mmol_mdl)'", u']', u'is_dynamic_in_mesh', u':', u'vett', u',', u"'outbound'", u':', u'vett', u',', u"'is_dynamic_in_mesh'", u':', u'try', u':', u'vett', u',', u"'inverse_nodes'", u':', u'drop', u',', u"'inverse_nodes'", u':', u'drop', u',', u"'is_dynamic_in_mesh'", u':', u'vett', u',', u"'inverse_nodes'", u':', u'dump', u'}', u'if', u"'f2c'", u':', u'{', u'"", u':", u'{', u'"fc1'", u':', u'{', u'"f2c"'", u':', u'{', u'"f2c'", u':', u'{', u'"fc2'", u':', u'{', u'"fc"', u':', u"{", u'"fc"', u':', u'"f2c"'", u':', u'{', u'"fc"', u':', u'"f2c"'", u':', u'"f2c'"', u':', u'"f2c"', u':', u'"f2c"', u':', u'"f2c"', u':', u'"f2c"', u':', u'"fc3", u':', u'"fc3"', u':', u'"fc3"', u':', u'"f2c"', u':', u'"f3"', u':', u'"fc3"', u':', u'"fc"', u':', u

[2600 | 8628.97] loss=1.18 avg=1.09
[2601 | 8634.12] loss=0.97 avg=1.09
[2602 | 8639.44] loss=1.43 avg=1.09
[2603 | 8644.67] loss=1.13 avg=1.09
[2604 | 8649.90] loss=1.07 avg=1.09
[2605 | 8655.12] loss=1.41 avg=1.10
[2606 | 8660.35] loss=0.79 avg=1.09
[2607 | 8665.56] loss=1.19 avg=1.10
[2608 | 8670.81] loss=2.03 avg=1.11
[2609 | 8675.96] loss=0.70 avg=1.10
[2610 | 8681.07] loss=1.33 avg=1.10
[2611 | 8686.34] loss=0.67 avg=1.10
[2612 | 8691.60] loss=0.93 avg=1.10
[2613 | 8696.89] loss=0.77 avg=1.09
[2614 | 8702.19] loss=0.96 avg=1.09
[2615 | 8707.51] loss=0.92 avg=1.09
[2616 | 8712.72] loss=0.95 avg=1.09
[2617 | 8718.00] loss=1.31 avg=1.09
[2618 | 8723.36] loss=0.86 avg=1.09
[2619 | 8728.61] loss=0.68 avg=1.09
[2620 | 8733.96] loss=1.10 avg=1.09
[2621 | 8739.24] loss=1.07 avg=1.09
[2622 | 8744.41] loss=0.53 avg=1.08
[2623 | 8749.58] loss=1.08 avg=1.08
[2624 | 8754.83] loss=0.71 avg=1.08
[2625 | 8760.11] loss=1.33 avg=1.08
[2626 | 8765.32] loss=1.07 avg=1.08
[2627 | 8770.53] loss=0.78 avg=1.08
[2628 | 8775.78] loss=0.88 avg=1.07
[2629 | 8780.99] loss=0.89 avg=1.07
[2630 | 8786.25] loss=0.61 avg=1.07
[2631 | 8791.56] loss=0.73 avg=1.06
[2632 | 8796.64] loss=0.71 avg=1.06
[2633 | 8801.82] loss=1.24 avg=1.06
[2634 | 8806.95] loss=0.80 avg=1.06
[2635 | 8812.06] loss=1.15 avg=1.06
[2636 | 8817.30] loss=0.77 avg=1.06
[2637 | 8822.37] loss=1.60 avg=1.06
[2638 | 8827.44] loss=1.33 avg=1.07
[2639 | 8832.68] loss=1.95 avg=1.07
[2640 | 8837.80] loss=1.14 avg=1.08
[2641 | 8842.43] loss=0.59 avg=1.07
[2642 | 8847.02] loss=0.76 avg=1.07
[2643 | 8852.08] loss=1.34 avg=1.07
[2644 | 8857.25] loss=0.97 avg=1.07
[2645 | 8862.46] loss=0.94 avg=1.07
[2646 | 8867.69] loss=0.47 avg=1.06
[2647 | 8872.84] loss=0.68 avg=1.06
[2648 | 8877.99] loss=0.94 avg=1.06
[2649 | 8883.13] loss=1.36 avg=1.06
[2650 | 8888.34] loss=1.01 avg=1.06
[2651 | 8893.45] loss=1.32 avg=1.06
[2652 | 8898.53] loss=1.16 avg=1.06
[2653 | 8903.77] loss=1.32 avg=1.07
[2654 | 8909.00] loss=1.48 avg=1.07
[2655 | 8914.18] loss=0.67 avg=1.07
[2656 | 8919.46] loss=2.74 avg=1.08
[2657 | 8924.59] loss=2.12 avg=1.09
[2658 | 8929.64] loss=1.07 avg=1.09
[2659 | 8934.82] loss=0.74 avg=1.09
[2660 | 8939.93] loss=1.08 avg=1.09
[2661 | 8945.05] loss=0.66 avg=1.08
[2662 | 8950.25] loss=1.18 avg=1.09
[2663 | 8955.27] loss=0.82 avg=1.08
[2664 | 8960.40] loss=1.73 avg=1.09
[2665 | 8965.59] loss=1.05 avg=1.09
[2666 | 8970.73] loss=0.85 avg=1.09
[2667 | 8975.82] loss=0.63 avg=1.08
[2668 | 8980.95] loss=1.12 avg=1.08
[2669 | 8986.18] loss=1.29 avg=1.08
[2670 | 8991.41] loss=1.12 avg=1.08
[2671 | 8996.58] loss=1.14 avg=1.09
[2672 | 9001.74] loss=0.61 avg=1.08
[2673 | 9006.92] loss=0.82 avg=1.08
[2674 | 9012.10] loss=0.66 avg=1.07
[2675 | 9017.24] loss=0.83 avg=1.07
[2676 | 9022.46] loss=1.09 avg=1.07
[2677 | 9027.69] loss=1.31 avg=1.07
[2678 | 9032.84] loss=1.42 avg=1.08
[2679 | 9038.04] loss=1.42 avg=1.08
[2680 | 9043.21] loss=0.83 avg=1.08
[2681 | 9048.40] loss=1.25 avg=1.08
[2682 | 9053.71] loss=1.25 avg=1.08
[2683 | 9058.92] loss=0.69 avg=1.08
[2684 | 9064.01] loss=0.98 avg=1.08
[2685 | 9069.12] loss=0.77 avg=1.07
[2686 | 9074.19] loss=0.63 avg=1.07
[2687 | 9079.29] loss=0.79 avg=1.07
[2688 | 9084.44] loss=1.07 avg=1.07
[2689 | 9089.66] loss=1.29 avg=1.07
[2690 | 9094.76] loss=1.37 avg=1.07
[2691 | 9099.98] loss=1.27 avg=1.07
[2692 | 9105.19] loss=0.87 avg=1.07
[2693 | 9110.44] loss=0.55 avg=1.07
[2694 | 9115.61] loss=0.97 avg=1.07
[2695 | 9120.74] loss=1.67 avg=1.07
[2696 | 9125.89] loss=0.52 avg=1.07
[2697 | 9131.01] loss=0.96 avg=1.07
[2698 | 9136.20] loss=1.07 avg=1.07
[2699 | 9141.34] loss=0.97 avg=1.06
Generating samples...
======== SAMPLE 1 ========
   - *:
            r = get_r ( 'r' )
            if lst is None:
                lst = 'r'
        | lst | = ''
                  if r is None:
                           lst = ['s':', 'o':', 'u':', 'r':', '.', r, '.', r, '.', r, '.']
                       if lst is None:
                                                                  lst |      | lst |    | = '.'
                         else:
                                                        [_], i_, _]
               | r | = 'r'

                     if r is None:
                                    lst = ['s':', 'o':', 'u':', 'r':', '.', r, '.', r, '.', r, '.']
        | lst | = ''
                    if r is None:
                                    lst | = '.'
       | lst | = ''
       if lst is None:
           | lst | = ''
                   if r is None:
           | lst | = ''
                           if lst is None:
        |                                lst | = '.'
       elif r == '.':
                    if s is None:
                          | # do not delete if str.get('-'))
                          s = ''
               else:
                       if s is None:
                                                       
                               if s not in lst:
             return          | lst | = '.'
         

[2700 | 9227.53] loss=1.66 avg=1.07
[2701 | 9232.75] loss=1.20 avg=1.07
[2702 | 9238.02] loss=1.22 avg=1.07
[2703 | 9243.30] loss=0.74 avg=1.07
[2704 | 9248.50] loss=1.20 avg=1.07
[2705 | 9253.61] loss=0.89 avg=1.07
[2706 | 9258.82] loss=1.08 avg=1.07
[2707 | 9264.18] loss=1.21 avg=1.07
[2708 | 9269.34] loss=1.13 avg=1.07
[2709 | 9274.71] loss=1.16 avg=1.07
[2710 | 9279.89] loss=1.42 avg=1.08
[2711 | 9285.15] loss=1.51 avg=1.08
[2712 | 9290.50] loss=1.05 avg=1.08
[2713 | 9295.80] loss=0.95 avg=1.08
[2714 | 9300.98] loss=1.13 avg=1.08
[2715 | 9306.27] loss=0.87 avg=1.08
[2716 | 9311.55] loss=1.56 avg=1.08
[2717 | 9316.78] loss=1.44 avg=1.09
[2718 | 9322.13] loss=1.01 avg=1.08
[2719 | 9327.48] loss=0.59 avg=1.08
[2720 | 9332.70] loss=0.81 avg=1.08
[2721 | 9337.87] loss=0.71 avg=1.07
[2722 | 9343.05] loss=0.55 avg=1.07
[2723 | 9348.34] loss=1.20 avg=1.07
[2724 | 9353.54] loss=1.25 avg=1.07
[2725 | 9358.78] loss=1.14 avg=1.07
[2726 | 9364.07] loss=0.90 avg=1.07
[2727 | 9369.33] loss=1.92 avg=1.08
[2728 | 9374.59] loss=0.70 avg=1.07
[2729 | 9379.77] loss=1.53 avg=1.08
[2730 | 9385.04] loss=0.83 avg=1.08
[2731 | 9390.35] loss=1.16 avg=1.08
[2732 | 9395.53] loss=0.79 avg=1.07
[2733 | 9400.72] loss=1.04 avg=1.07
[2734 | 9405.97] loss=1.31 avg=1.08
[2735 | 9411.08] loss=1.46 avg=1.08
[2736 | 9416.24] loss=0.93 avg=1.08
[2737 | 9421.44] loss=1.08 avg=1.08
[2738 | 9426.57] loss=0.90 avg=1.08
[2739 | 9431.72] loss=0.94 avg=1.08
[2740 | 9436.94] loss=0.71 avg=1.07
[2741 | 9442.14] loss=0.67 avg=1.07
[2742 | 9447.40] loss=1.16 avg=1.07
[2743 | 9452.67] loss=0.90 avg=1.07
[2744 | 9457.97] loss=1.18 avg=1.07
[2745 | 9463.15] loss=1.10 avg=1.07
[2746 | 9468.39] loss=0.77 avg=1.07
[2747 | 9473.71] loss=1.58 avg=1.07
[2748 | 9478.86] loss=0.74 avg=1.07
[2749 | 9484.07] loss=1.60 avg=1.07
[2750 | 9489.28] loss=1.94 avg=1.08
[2751 | 9494.52] loss=1.11 avg=1.08
[2752 | 9499.63] loss=1.24 avg=1.08
[2753 | 9504.73] loss=1.20 avg=1.08
[2754 | 9509.93] loss=0.89 avg=1.08
[2755 | 9515.11] loss=1.03 avg=1.08
[2756 | 9520.34] loss=0.59 avg=1.08
[2757 | 9525.43] loss=0.67 avg=1.07
[2758 | 9530.68] loss=2.09 avg=1.08
[2759 | 9536.02] loss=1.30 avg=1.09
[2760 | 9541.30] loss=1.02 avg=1.08
[2761 | 9546.50] loss=1.09 avg=1.08
[2762 | 9551.77] loss=0.75 avg=1.08
[2763 | 9557.08] loss=0.58 avg=1.08
[2764 | 9562.42] loss=0.96 avg=1.08
[2765 | 9567.63] loss=0.74 avg=1.07
[2766 | 9572.86] loss=0.82 avg=1.07
[2767 | 9578.08] loss=1.10 avg=1.07
[2768 | 9583.27] loss=1.44 avg=1.07
[2769 | 9588.52] loss=1.26 avg=1.08
[2770 | 9593.77] loss=1.05 avg=1.08
[2771 | 9599.05] loss=1.28 avg=1.08
[2772 | 9604.28] loss=1.44 avg=1.08
[2773 | 9609.55] loss=0.73 avg=1.08
[2774 | 9614.77] loss=1.53 avg=1.08
[2775 | 9620.05] loss=1.68 avg=1.09
[2776 | 9625.46] loss=0.72 avg=1.08
[2777 | 9630.77] loss=0.99 avg=1.08
[2778 | 9636.03] loss=0.52 avg=1.08
[2779 | 9641.34] loss=1.15 avg=1.08
[2780 | 9646.46] loss=0.91 avg=1.08
[2781 | 9651.83] loss=1.15 avg=1.08
[2782 | 9657.12] loss=0.73 avg=1.07
[2783 | 9662.43] loss=0.98 avg=1.07
[2784 | 9667.76] loss=0.68 avg=1.07
[2785 | 9672.97] loss=1.32 avg=1.07
[2786 | 9678.20] loss=1.39 avg=1.07
[2787 | 9683.41] loss=0.75 avg=1.07
[2788 | 9688.74] loss=1.26 avg=1.07
[2789 | 9694.07] loss=0.89 avg=1.07
[2790 | 9699.35] loss=1.57 avg=1.08
[2791 | 9704.67] loss=1.15 avg=1.08
[2792 | 9709.93] loss=1.46 avg=1.08
[2793 | 9715.23] loss=0.67 avg=1.08
[2794 | 9720.51] loss=1.16 avg=1.08
[2795 | 9725.72] loss=1.18 avg=1.08
[2796 | 9730.90] loss=0.98 avg=1.08
[2797 | 9735.98] loss=0.57 avg=1.07
[2798 | 9741.16] loss=0.76 avg=1.07
[2799 | 9746.33] loss=0.82 avg=1.07
Generating samples...
======== SAMPLE 1 ========
f0: the object that is the root in the container
            the original class name.
         mv=mux_dict_from_name(mux),
                                     mv/classes.slice(idx=0)
                                  in = muv_dict_find(mux)
         # Use mv in the container to find a valid mux class.
        mv = mux_dict_from_name(mx)
        # Use mv in the container to find a mux class with multiple Mappings.
        assert_is_nonlocale_only(f0, __opts__[f0.m.mappings ])
        assert_isdirected(f0, __opts__[f0.m.mappings])
        if __opts__['moved_mx']:
            return mv.find_object(__opts__[f0.m.mappings]=mv.class_name,
                               in=f0, and not in=f1,
                                 mv=mux_dict_from_name(mx))
         else:
             for k in range(f1.m, f1.m.mappings):
                assert_isdirected(f1, __opts__[k.m.mappings]=f1.m.m.mappings))
            return mv
        return mv.find_object(__opts__)[key] train lm/api/mav.py kwizor/mlm f9a9ca2ccf9bf3ea9a0928d3d3a7d4e97bcf6 https://github.com/kwizor/mlm/blob/f9a9ca2ccf9bf3ea9a0928d3d3a7d4e97bcf6/mlm/api/mav.py#L98-L102
def _store(self, self._opts):
        """store of the new method
        """
        return self._opts.setup_method(self._opts[0]),
        return self._opts[self._opts] train lm/api/client/store.py vandex/mlm 0f7e98a983ef8ffee1a2fa8edee3a893e0cb0 https://github.com/vandex/mlm/blob/0f7e98a983ef8ffee1a2fa8edee3a893e0cb0/mlm/api/client/store.py#L10-L26
def _get_config_url(self, name):
        """Get the configuration from a
                Mav.config object

        :param name:
        :param name: the name of the config object
           :param {value: {value: ``None``}``
                   'default.':
            - ``None``: Default value of the Configuration object
                  'default

[2800 | 9832.76] loss=1.51 avg=1.07
[2801 | 9837.97] loss=0.85 avg=1.07
[2802 | 9843.24] loss=1.11 avg=1.07
[2803 | 9848.56] loss=0.94 avg=1.07
[2804 | 9853.83] loss=0.92 avg=1.07
[2805 | 9859.04] loss=0.98 avg=1.07
[2806 | 9864.31] loss=0.74 avg=1.06
[2807 | 9869.55] loss=1.66 avg=1.07
[2808 | 9874.76] loss=0.91 avg=1.07
[2809 | 9879.94] loss=1.92 avg=1.08
[2810 | 9885.09] loss=1.30 avg=1.08
[2811 | 9890.32] loss=0.89 avg=1.08
[2812 | 9895.64] loss=0.90 avg=1.07
[2813 | 9900.79] loss=1.07 avg=1.07
[2814 | 9906.00] loss=1.84 avg=1.08
[2815 | 9911.18] loss=1.40 avg=1.08
[2816 | 9916.44] loss=1.08 avg=1.08
[2817 | 9921.74] loss=0.75 avg=1.08
[2818 | 9926.98] loss=0.92 avg=1.08
[2819 | 9932.17] loss=1.35 avg=1.08
[2820 | 9937.28] loss=0.77 avg=1.08
[2821 | 9942.51] loss=1.15 avg=1.08
[2822 | 9947.78] loss=0.95 avg=1.08
[2823 | 9952.97] loss=0.84 avg=1.08
[2824 | 9958.22] loss=0.92 avg=1.08
[2825 | 9963.47] loss=1.70 avg=1.08
[2826 | 9968.66] loss=0.68 avg=1.08
[2827 | 9973.94] loss=1.10 avg=1.08
[2828 | 9979.23] loss=0.78 avg=1.07
[2829 | 9984.47] loss=0.56 avg=1.07
[2830 | 9989.73] loss=1.29 avg=1.07
[2831 | 9994.94] loss=1.41 avg=1.07
[2832 | 10000.21] loss=1.55 avg=1.08
[2833 | 10005.36] loss=0.79 avg=1.08
[2834 | 10010.60] loss=1.26 avg=1.08
[2835 | 10015.84] loss=1.32 avg=1.08
[2836 | 10021.06] loss=0.50 avg=1.08
[2837 | 10026.34] loss=1.64 avg=1.08
[2838 | 10031.65] loss=1.37 avg=1.08
[2839 | 10037.02] loss=1.15 avg=1.08
[2840 | 10042.27] loss=0.44 avg=1.08
[2841 | 10047.65] loss=1.16 avg=1.08
[2842 | 10052.85] loss=1.51 avg=1.08
[2843 | 10058.10] loss=1.33 avg=1.09
[2844 | 10063.34] loss=1.33 avg=1.09
[2845 | 10068.62] loss=0.49 avg=1.08
[2846 | 10073.90] loss=0.85 avg=1.08
[2847 | 10079.05] loss=1.47 avg=1.08
[2848 | 10084.26] loss=0.35 avg=1.08
[2849 | 10089.48] loss=0.78 avg=1.07
[2850 | 10094.62] loss=1.60 avg=1.08
[2851 | 10099.87] loss=0.95 avg=1.08
[2852 | 10105.19] loss=0.67 avg=1.07
[2853 | 10110.41] loss=1.17 avg=1.07
[2854 | 10115.53] loss=0.84 avg=1.07
[2855 | 10120.67] loss=1.27 avg=1.07
[2856 | 10125.81] loss=1.10 avg=1.07
[2857 | 10131.10] loss=1.46 avg=1.08
[2858 | 10136.25] loss=0.63 avg=1.07
[2859 | 10141.46] loss=1.26 avg=1.08
[2860 | 10146.68] loss=1.42 avg=1.08
[2861 | 10151.98] loss=1.16 avg=1.08
[2862 | 10157.21] loss=1.44 avg=1.08
[2863 | 10162.61] loss=0.73 avg=1.08
[2864 | 10167.85] loss=1.42 avg=1.08
[2865 | 10173.03] loss=0.87 avg=1.08
[2866 | 10178.30] loss=1.05 avg=1.08
[2867 | 10183.52] loss=0.77 avg=1.08
[2868 | 10188.71] loss=1.06 avg=1.08
[2869 | 10193.97] loss=0.66 avg=1.07
[2870 | 10199.22] loss=1.30 avg=1.08
[2871 | 10204.36] loss=0.72 avg=1.07
[2872 | 10209.59] loss=1.50 avg=1.08
[2873 | 10214.85] loss=0.75 avg=1.07
[2874 | 10220.04] loss=1.88 avg=1.08
[2875 | 10225.19] loss=1.06 avg=1.08
[2876 | 10230.36] loss=1.32 avg=1.08
[2877 | 10235.62] loss=0.58 avg=1.08
[2878 | 10240.83] loss=1.03 avg=1.08
[2879 | 10245.97] loss=1.33 avg=1.08
[2880 | 10251.15] loss=0.77 avg=1.08
[2881 | 10256.34] loss=0.69 avg=1.07
[2882 | 10261.57] loss=1.38 avg=1.08
[2883 | 10266.86] loss=0.92 avg=1.07
[2884 | 10272.04] loss=1.56 avg=1.08
[2885 | 10277.23] loss=1.32 avg=1.08
[2886 | 10282.55] loss=1.47 avg=1.09
[2887 | 10287.88] loss=1.60 avg=1.09
[2888 | 10293.10] loss=1.01 avg=1.09
[2889 | 10298.37] loss=1.76 avg=1.10
[2890 | 10303.67] loss=0.68 avg=1.09
[2891 | 10308.88] loss=0.81 avg=1.09
[2892 | 10314.32] loss=1.50 avg=1.09
[2893 | 10319.50] loss=0.88 avg=1.09
[2894 | 10324.68] loss=1.83 avg=1.10
[2895 | 10329.92] loss=0.95 avg=1.10
[2896 | 10335.13] loss=1.50 avg=1.10
[2897 | 10340.31] loss=1.67 avg=1.11
[2898 | 10345.45] loss=0.96 avg=1.11
[2899 | 10350.57] loss=1.10 avg=1.11
Generating samples...
======== SAMPLE 1 ========
               #              #            #                #                #             #                (#                     #                   #                 #                  ))))
    if not isinstance(path, list):
        return
      else:
        raise self.code_warning()
        return path.strip_path(path) train liblangx/core_api.py dtbarker/liblang-gl.py dtbarker 3d67f5fc92bcf6c5e6f84745a128958d7c0dd https://github.com/dtbarker/liblangx/blob/3d67f5fc92bcf6c5e6f84745a128958d7c0dd/liblangx/core_api.py#L19-L46
def _get_id_list(self, id):
    """Return a list of the unique identifier for one of the keys returned.
    """
    return len(self) == len(self._id):
    if id not in dictionary:
     else:
     self._list = id

    return dict(
        self._list[id:id]
    ) [u'def', u'_get_id_list', u'(', u'self', u',', u'id', u')', u':', u':', u'return', u'"''', u'len', u'(', u'self', u')', u'==', u'len', u'(', u'self', u'.', u'db', u')', u':', u'if', u'id', u'not', u'in', u'dict', u':', u'self', u'.', u'_list', u'=', u'id', u'else', u':', u'self', u'.', u'_list', u'=', u'dict', u'(', u')'] Return a list of the unique identifier for one of the keys returned. [u'Return', u'a', u'list', u'of', u'text', u'identifiers', u'for', u'two', u'.', u'ids'] _get_id_list python def _get_id_list(self, id):
    """Return a list of the unique identifier for one of the keys returned.
    """
    return len(self) == len(self._id):
    self._list = id

    return dict(
         self._list[id:id]
    ) train liblangx/core_api.py dtbarker/liblangx-gl.py dtbarker/liblang-gl.py dtbarker-stack-liblangx-gl https://github.com/dtbarker/liblangx/blob/dtbarker/liblangx-gl.py#L38-L42
def _get_name_list(self, name):
    """Return a list of the unique identifier for one of the keys returned.
    """
    return len(self) == len(self._id):
    if {'name':name} in dictionary:
       assert {}, name} in dict
        return dict(
          assert name(name), "''" [u'def', u'_get_name_list', u'(', u'self', u',', u'name',

[2900 | 10440.97] loss=1.22 avg=1.11
[2901 | 10446.16] loss=1.17 avg=1.11
[2902 | 10451.28] loss=1.94 avg=1.12
[2903 | 10456.43] loss=0.98 avg=1.11
[2904 | 10461.68] loss=0.92 avg=1.11
[2905 | 10467.01] loss=0.78 avg=1.11
[2906 | 10472.25] loss=1.37 avg=1.11
[2907 | 10477.37] loss=1.37 avg=1.11
[2908 | 10482.54] loss=0.98 avg=1.11
[2909 | 10487.89] loss=1.03 avg=1.11
[2910 | 10493.19] loss=1.27 avg=1.11
[2911 | 10498.48] loss=0.88 avg=1.11
[2912 | 10503.72] loss=0.68 avg=1.11
[2913 | 10508.96] loss=0.55 avg=1.10
[2914 | 10514.11] loss=0.73 avg=1.10
[2915 | 10519.35] loss=0.61 avg=1.09
[2916 | 10524.54] loss=0.94 avg=1.09
[2917 | 10529.81] loss=1.12 avg=1.09
[2918 | 10535.02] loss=1.16 avg=1.09
[2919 | 10540.30] loss=1.02 avg=1.09
[2920 | 10545.52] loss=0.87 avg=1.09
[2921 | 10550.72] loss=0.73 avg=1.09
[2922 | 10555.92] loss=1.20 avg=1.09
[2923 | 10561.16] loss=1.28 avg=1.09
[2924 | 10566.32] loss=0.66 avg=1.08
[2925 | 10571.61] loss=1.13 avg=1.08
[2926 | 10576.83] loss=0.90 avg=1.08
[2927 | 10582.22] loss=0.94 avg=1.08
[2928 | 10587.53] loss=0.92 avg=1.08
[2929 | 10592.96] loss=0.71 avg=1.08
[2930 | 10598.29] loss=0.92 avg=1.07
[2931 | 10603.59] loss=1.25 avg=1.08
[2932 | 10608.85] loss=0.88 avg=1.07
[2933 | 10614.10] loss=1.18 avg=1.08
[2934 | 10619.40] loss=1.07 avg=1.08
[2935 | 10624.55] loss=1.13 avg=1.08
[2936 | 10629.87] loss=0.68 avg=1.07
[2937 | 10635.08] loss=1.20 avg=1.07
[2938 | 10640.32] loss=0.72 avg=1.07
[2939 | 10645.64] loss=1.00 avg=1.07
[2940 | 10650.92] loss=0.78 avg=1.07
[2941 | 10656.10] loss=1.55 avg=1.07
[2942 | 10661.24] loss=0.52 avg=1.07
[2943 | 10666.35] loss=1.15 avg=1.07
[2944 | 10671.65] loss=0.81 avg=1.06
[2945 | 10676.92] loss=1.22 avg=1.07
[2946 | 10682.02] loss=0.82 avg=1.06
[2947 | 10687.21] loss=1.19 avg=1.06
[2948 | 10692.36] loss=0.87 avg=1.06
[2949 | 10697.66] loss=1.06 avg=1.06
[2950 | 10702.90] loss=0.86 avg=1.06
[2951 | 10708.18] loss=1.34 avg=1.06
[2952 | 10713.42] loss=0.93 avg=1.06
[2953 | 10718.68] loss=1.12 avg=1.06
[2954 | 10723.92] loss=1.34 avg=1.07
[2955 | 10729.16] loss=0.95 avg=1.06
[2956 | 10734.31] loss=0.68 avg=1.06
[2957 | 10739.46] loss=1.45 avg=1.06
[2958 | 10744.56] loss=0.59 avg=1.06
[2959 | 10749.67] loss=1.25 avg=1.06
[2960 | 10754.88] loss=0.85 avg=1.06
[2961 | 10760.07] loss=1.01 avg=1.06
[2962 | 10765.33] loss=1.52 avg=1.06
[2963 | 10770.60] loss=0.97 avg=1.06
[2964 | 10775.84] loss=0.75 avg=1.06
[2965 | 10781.09] loss=0.92 avg=1.06
[2966 | 10786.37] loss=0.97 avg=1.06
[2967 | 10791.70] loss=1.76 avg=1.06
[2968 | 10796.99] loss=1.30 avg=1.07
[2969 | 10802.22] loss=0.99 avg=1.07
[2970 | 10807.33] loss=0.73 avg=1.06
[2971 | 10812.59] loss=0.65 avg=1.06
[2972 | 10817.76] loss=1.19 avg=1.06
[2973 | 10822.90] loss=1.44 avg=1.06
[2974 | 10828.06] loss=1.47 avg=1.07
[2975 | 10833.36] loss=0.78 avg=1.06
[2976 | 10838.51] loss=0.65 avg=1.06
[2977 | 10843.72] loss=0.80 avg=1.06
[2978 | 10849.03] loss=0.61 avg=1.05
[2979 | 10854.36] loss=1.48 avg=1.06
[2980 | 10859.57] loss=0.98 avg=1.06
[2981 | 10864.75] loss=1.24 avg=1.06
[2982 | 10869.95] loss=1.38 avg=1.06
[2983 | 10875.11] loss=1.36 avg=1.06
[2984 | 10880.30] loss=1.17 avg=1.07
[2985 | 10885.51] loss=1.06 avg=1.07
[2986 | 10890.76] loss=0.77 avg=1.06
[2987 | 10896.05] loss=0.89 avg=1.06
[2988 | 10901.26] loss=0.91 avg=1.06
[2989 | 10906.53] loss=0.91 avg=1.06
[2990 | 10911.72] loss=0.76 avg=1.05
[2991 | 10916.98] loss=1.07 avg=1.05
[2992 | 10922.32] loss=0.56 avg=1.05
[2993 | 10927.57] loss=1.12 avg=1.05
[2994 | 10932.83] loss=1.17 avg=1.05
[2995 | 10938.07] loss=0.85 avg=1.05
[2996 | 10943.40] loss=0.85 avg=1.05
[2997 | 10948.67] loss=1.15 avg=1.05
[2998 | 10953.80] loss=0.79 avg=1.05
[2999 | 10958.92] loss=1.26 avg=1.05
Saving checkpoint/run1/model-3000
Generating samples...
======== SAMPLE 1 ========
 d2d3d4a3bef7c6cacdffc1ad8a36d7e4905 https://github.com/toby/lighthouse-bundle/blob/d2d3d4a3bef7c6cacdffc1ad8a36d7e4905/lighthouse-bundle/lighthouse-bundle.py#L643-L644
def main(self,
                                                    )):
         """ Main function."""
        self.__main__ = self.__main__
        self.__main_func_name = self.__main_func
        self.__main_name = self.__main_name
        return self.__main__ [u'def', u'main', u'(', u'self', u',', u'*', u'*', u'(', u'*', u')', u',', u'*', u'(', u"'__main__'", u',', u'*', u'*', u'(', u'*', u'u')', u',', u'*', u'(', u"'__main__'", u',', u'metadata', u'=', u'"Main_function"', u')', u',', u'*', u'(', u"'__main__'", u',', u'*', u'(', u'*', u'-', u'[', u'1', u',', u'])', u'for', u'i', u',', u'r', u',', u'metadata', u'in', u'(', u"'__main__'", u',', u"'Main_func'", u')', u']', u')', u')', u'# Initial function.', u'__main__', u'=', u'self', u'.', u'__main__', u'.', u'__main__', u'.', u'__main_func_name', u'=', u'self', u'.', u'__main_func', u'.', u'__main_func', u'=', u'metadata', u'=', u'"Main_function"', u'if', u'None', u'and', u'metadata', u':', u'print', u'(', u'"Main_function"', u',', u'"Initials"', u')', u'print', u'(', u'"Function"', u',', u'"Initials"', u')', u'# Initial funcName = ', self', u'.', u'__main_func_name', u'print', u'(', u'"Function"', u')', u'print', u'(', u'"Function"', u',', u'"Initials"', u')', u'# Initial functionName = ', self', u'.', u'__main_func_name', u'print', u'(', u'"Initials"', u')', u'print', u'(', u'"Function"', u',', u'"Initials"', u')', u'# Initial functionName = ', self', u'.', u'__main_func_name', u'print', u'(', u'"Initials"', u')', u'print', u'(', u'"Function"', u',', u'"Initials"', u')', u'print', u'(', u'"Function"', u',', u'"Initials"', u')', u'return', u'self', u'.', u'__main__'] Main function. [u'Main', u'function', u'.'] Main function. [u'main', u'(', u'*', u'*', u'(', u'*', u'*', u'(', u')', u',', u'*', u'(', u')', u',', u'*', u'(', u')', u'*', u'(', u')', u'*', u'(', u')', u'*', u'(', u')', u'return', u'self', u'.', u'__main__

[3000 | 11050.81] loss=1.52 avg=1.05
[3001 | 11055.99] loss=1.22 avg=1.05
[3002 | 11061.21] loss=0.75 avg=1.05
[3003 | 11066.32] loss=1.17 avg=1.05
[3004 | 11071.57] loss=0.52 avg=1.05
[3005 | 11076.76] loss=0.75 avg=1.04
[3006 | 11082.01] loss=1.10 avg=1.05
[3007 | 11087.14] loss=1.23 avg=1.05
[3008 | 11092.39] loss=1.10 avg=1.05
[3009 | 11097.64] loss=1.04 avg=1.05
[3010 | 11102.85] loss=0.99 avg=1.05
[3011 | 11108.09] loss=1.28 avg=1.05
[3012 | 11113.28] loss=1.12 avg=1.05
[3013 | 11118.60] loss=1.38 avg=1.05
[3014 | 11123.77] loss=0.67 avg=1.05
[3015 | 11128.92] loss=1.50 avg=1.05
[3016 | 11134.13] loss=1.32 avg=1.06
[3017 | 11139.30] loss=0.93 avg=1.06
[3018 | 11144.51] loss=0.89 avg=1.05
[3019 | 11149.77] loss=0.70 avg=1.05
[3020 | 11155.02] loss=1.88 avg=1.06
[3021 | 11160.33] loss=1.33 avg=1.06
[3022 | 11165.48] loss=0.98 avg=1.06
[3023 | 11170.79] loss=0.99 avg=1.06
[3024 | 11176.01] loss=0.90 avg=1.06
[3025 | 11181.26] loss=1.31 avg=1.06
[3026 | 11186.50] loss=1.16 avg=1.06
[3027 | 11191.79] loss=0.83 avg=1.06
[3028 | 11197.16] loss=0.77 avg=1.06
[3029 | 11202.36] loss=0.69 avg=1.05
[3030 | 11207.52] loss=0.89 avg=1.05
[3031 | 11212.81] loss=1.33 avg=1.05
[3032 | 11218.09] loss=1.22 avg=1.06
[3033 | 11223.46] loss=1.82 avg=1.06
[3034 | 11228.62] loss=1.00 avg=1.06
[3035 | 11233.96] loss=1.04 avg=1.06
[3036 | 11239.32] loss=1.28 avg=1.06
[3037 | 11244.66] loss=1.00 avg=1.06
[3038 | 11249.97] loss=0.91 avg=1.06
[3039 | 11255.14] loss=1.34 avg=1.07
[3040 | 11260.55] loss=1.39 avg=1.07
[3041 | 11265.78] loss=1.22 avg=1.07
[3042 | 11271.06] loss=0.87 avg=1.07
[3043 | 11276.34] loss=0.75 avg=1.06
[3044 | 11281.60] loss=1.15 avg=1.07
[3045 | 11286.87] loss=0.88 avg=1.06
[3046 | 11292.01] loss=1.89 avg=1.07
[3047 | 11297.24] loss=0.52 avg=1.07
[3048 | 11302.56] loss=0.94 avg=1.07
[3049 | 11307.75] loss=1.57 avg=1.07
[3050 | 11312.92] loss=1.05 avg=1.07
[3051 | 11318.08] loss=1.31 avg=1.07
[3052 | 11323.25] loss=1.17 avg=1.07
[3053 | 11328.47] loss=0.48 avg=1.07
[3054 | 11333.60] loss=1.47 avg=1.07
[3055 | 11338.80] loss=1.46 avg=1.08
[3056 | 11343.97] loss=1.28 avg=1.08
[3057 | 11349.18] loss=0.93 avg=1.08
[3058 | 11354.44] loss=1.30 avg=1.08
[3059 | 11359.64] loss=2.37 avg=1.09
[3060 | 11364.86] loss=1.79 avg=1.10
[3061 | 11370.24] loss=1.14 avg=1.10
[3062 | 11375.50] loss=0.66 avg=1.09
[3063 | 11380.69] loss=0.57 avg=1.09
[3064 | 11385.92] loss=1.42 avg=1.09
[3065 | 11391.24] loss=1.04 avg=1.09
[3066 | 11396.40] loss=0.77 avg=1.09
[3067 | 11401.61] loss=0.95 avg=1.09
[3068 | 11406.81] loss=1.16 avg=1.09
[3069 | 11411.98] loss=1.85 avg=1.10
[3070 | 11417.18] loss=1.25 avg=1.10
[3071 | 11422.39] loss=1.04 avg=1.10
[3072 | 11427.64] loss=1.33 avg=1.10
[3073 | 11432.88] loss=0.67 avg=1.09
[3074 | 11438.20] loss=0.87 avg=1.09
[3075 | 11443.39] loss=0.78 avg=1.09
[3076 | 11448.59] loss=0.71 avg=1.09
[3077 | 11453.90] loss=1.37 avg=1.09
[3078 | 11459.18] loss=1.17 avg=1.09
[3079 | 11464.36] loss=1.38 avg=1.09
[3080 | 11469.59] loss=0.66 avg=1.09
[3081 | 11474.81] loss=1.19 avg=1.09
[3082 | 11480.02] loss=0.85 avg=1.09
[3083 | 11485.26] loss=0.99 avg=1.09
[3084 | 11490.50] loss=0.57 avg=1.08
[3085 | 11495.67] loss=0.98 avg=1.08
[3086 | 11500.83] loss=1.65 avg=1.08
[3087 | 11506.01] loss=1.20 avg=1.09
[3088 | 11511.14] loss=0.80 avg=1.08
[3089 | 11516.38] loss=0.63 avg=1.08
[3090 | 11521.64] loss=0.83 avg=1.08
[3091 | 11526.87] loss=0.68 avg=1.07
[3092 | 11532.12] loss=0.89 avg=1.07
[3093 | 11537.34] loss=0.87 avg=1.07
[3094 | 11542.68] loss=0.47 avg=1.06
[3095 | 11547.97] loss=0.83 avg=1.06
[3096 | 11553.18] loss=0.92 avg=1.06
[3097 | 11558.58] loss=1.45 avg=1.06
[3098 | 11563.73] loss=1.24 avg=1.06
[3099 | 11569.08] loss=1.03 avg=1.06
Generating samples...
======== SAMPLE 1 ========
, u'(', u'h', u',', u"'type'", u',', u'h2', u',', u'h3', u',', u't', u'h', u',', u'marked', u'=', u'None', u')', u':', u'# If 'h', u',', u'1', u',', u"'stmt'", u',', u'1', u',', u"'tmt'", u',', u'1', u')', u':', u'if', u'1', u':', u"'strftime'", u',', u'marked', u'=', u'-', u'1', u'else', u'(', u'1', u',', u"'strftime'", u',', u'marked', u'=', u'-', u'1', u')', u':', u'# No other argument', u'(', u'h', u',', u"'type'", u',', u'marked', u')', u':', u''if', u'1', u':', u'marked', u'=', u'-', u'1', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'h2', u',', u'marked', u')', u'else', u'(', u'marked', u',', u'-', u'marked', u')', u':', u'if', u'1', u':', u'marked', u'=', u'-', u'1', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'marked', u')', u'if', u'1', u':', u'marked', u'=', u'-', u'1', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'marked', u')', u'else', u'(', u'marked', u',', u'-', u'1', u')', u':', u'verdim', u'=', u'None', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'marked', u')', u'if', u'1', u':', u'verdim', u'=', u'-', u'-', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'marked', u')', u'if', u'1', u':', u'verdim', u'=', u'-', u'request', u'and', u'request', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'tep', u',', u'1', u',, u'marked', u'=', u'1', u'or', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'2', u',, u'marked', u'=', u'1', u')', u':', u'verdim', u'=', u'-', u'1', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'versif', u'=', u'1', u'(', u')', u'and', u'tep', u',', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'request', u')', u',', u'marked', u'=', u'-', u'marked', u'and', u'self', u'.', u'tertexts', u'(', u'h', u',', u'self', u'.', u'tertexts', u'(', u'h', u',', u"' type'", u',', u'2', u',, u'marked', u'=', u'1', u')', u',', u'marked', u'=',

[3100 | 11658.47] loss=1.29 avg=1.07
[3101 | 11663.60] loss=0.90 avg=1.06
[3102 | 11668.79] loss=1.26 avg=1.07
[3103 | 11674.01] loss=1.03 avg=1.07
[3104 | 11679.20] loss=0.78 avg=1.06
[3105 | 11684.43] loss=1.00 avg=1.06
[3106 | 11689.66] loss=1.18 avg=1.06
[3107 | 11694.89] loss=1.06 avg=1.06
[3108 | 11700.18] loss=0.77 avg=1.06
[3109 | 11705.52] loss=1.10 avg=1.06
[3110 | 11710.66] loss=1.24 avg=1.06
[3111 | 11715.85] loss=1.22 avg=1.06
[3112 | 11721.05] loss=1.55 avg=1.07
[3113 | 11726.28] loss=1.06 avg=1.07
[3114 | 11731.41] loss=0.72 avg=1.07
[3115 | 11736.57] loss=0.70 avg=1.06
[3116 | 11741.76] loss=1.22 avg=1.06
[3117 | 11746.98] loss=0.88 avg=1.06
[3118 | 11752.20] loss=1.04 avg=1.06
[3119 | 11757.38] loss=0.87 avg=1.06
[3120 | 11762.55] loss=1.27 avg=1.06
[3121 | 11767.83] loss=1.00 avg=1.06
[3122 | 11772.92] loss=0.74 avg=1.06
[3123 | 11778.11] loss=1.21 avg=1.06
[3124 | 11783.33] loss=1.40 avg=1.06
[3125 | 11788.44] loss=1.41 avg=1.07
[3126 | 11793.62] loss=0.66 avg=1.06
[3127 | 11798.85] loss=1.03 avg=1.06
[3128 | 11804.13] loss=0.76 avg=1.06
[3129 | 11809.37] loss=1.04 avg=1.06
[3130 | 11814.52] loss=0.66 avg=1.05
[3131 | 11819.72] loss=1.20 avg=1.06
[3132 | 11824.84] loss=0.69 avg=1.05
[3133 | 11830.07] loss=0.89 avg=1.05
[3134 | 11835.20] loss=0.51 avg=1.05
[3135 | 11840.35] loss=0.83 avg=1.04
[3136 | 11845.52] loss=0.72 avg=1.04
[3137 | 11850.74] loss=1.65 avg=1.05
[3138 | 11855.93] loss=0.92 avg=1.05
[3139 | 11861.17] loss=0.82 avg=1.04
[3140 | 11866.29] loss=0.68 avg=1.04
[3141 | 11871.46] loss=0.99 avg=1.04
[3142 | 11876.61] loss=1.19 avg=1.04
[3143 | 11881.82] loss=1.93 avg=1.05
[3144 | 11887.01] loss=0.72 avg=1.05
[3145 | 11892.27] loss=1.79 avg=1.05
[3146 | 11897.45] loss=0.90 avg=1.05
[3147 | 11902.78] loss=1.41 avg=1.06
[3148 | 11908.01] loss=0.84 avg=1.05
[3149 | 11913.16] loss=0.28 avg=1.05
[3150 | 11918.35] loss=1.67 avg=1.05
[3151 | 11923.61] loss=1.20 avg=1.05
[3152 | 11928.79] loss=1.01 avg=1.05
[3153 | 11933.92] loss=0.97 avg=1.05
[3154 | 11939.12] loss=1.23 avg=1.05
[3155 | 11944.27] loss=0.45 avg=1.05
[3156 | 11949.41] loss=0.87 avg=1.05
[3157 | 11954.55] loss=0.53 avg=1.04
[3158 | 11959.80] loss=0.60 avg=1.04
[3159 | 11965.00] loss=1.24 avg=1.04
[3160 | 11970.18] loss=0.96 avg=1.04
[3161 | 11975.38] loss=0.79 avg=1.04
[3162 | 11980.56] loss=1.96 avg=1.04
[3163 | 11985.75] loss=1.41 avg=1.05
[3164 | 11990.96] loss=1.23 avg=1.05
[3165 | 11996.17] loss=1.75 avg=1.06
[3166 | 12001.35] loss=1.21 avg=1.06
[3167 | 12006.54] loss=0.72 avg=1.06
[3168 | 12011.71] loss=0.87 avg=1.05
[3169 | 12016.88] loss=1.10 avg=1.05
[3170 | 12022.13] loss=1.51 avg=1.06
[3171 | 12027.28] loss=2.15 avg=1.07
[3172 | 12032.52] loss=0.94 avg=1.07
[3173 | 12037.75] loss=1.23 avg=1.07
[3174 | 12042.97] loss=0.93 avg=1.07
[3175 | 12048.18] loss=1.36 avg=1.07
[3176 | 12053.34] loss=1.20 avg=1.07
[3177 | 12058.47] loss=1.13 avg=1.07
[3178 | 12063.66] loss=0.81 avg=1.07
[3179 | 12068.82] loss=1.26 avg=1.07
[3180 | 12074.00] loss=2.03 avg=1.08
[3181 | 12079.20] loss=0.98 avg=1.08
[3182 | 12084.62] loss=1.37 avg=1.08
[3183 | 12089.92] loss=0.79 avg=1.08
[3184 | 12095.13] loss=1.00 avg=1.08
[3185 | 12100.40] loss=0.94 avg=1.08
[3186 | 12105.67] loss=0.62 avg=1.07
[3187 | 12110.99] loss=1.05 avg=1.07
[3188 | 12116.26] loss=0.82 avg=1.07
[3189 | 12121.39] loss=0.82 avg=1.07
[3190 | 12126.65] loss=1.33 avg=1.07
[3191 | 12131.85] loss=1.20 avg=1.07
[3192 | 12137.01] loss=1.06 avg=1.07
[3193 | 12142.19] loss=1.56 avg=1.08
[3194 | 12147.30] loss=0.99 avg=1.08
[3195 | 12152.44] loss=1.10 avg=1.08
[3196 | 12157.70] loss=0.68 avg=1.07
[3197 | 12162.92] loss=1.18 avg=1.07
[3198 | 12168.02] loss=0.92 avg=1.07
[3199 | 12173.30] loss=1.18 avg=1.07
Generating samples...
======== SAMPLE 1 ========
('a':', u'(', u'a', u')', u'f', u')', u'c', u',', u'a', u',', u'f', u')', u'f', u'=', u'f', u'.', u'description', u'(', u"'A\u20c5\u20c6\u20a0\u20a1 (0,1,1,1)\n"'", u'.', u'lower', u')', u'time', u'=', u"'o'", u'%', u'time', u'+', u'1', u'.', u'normalize', u'(', u'time', u',', u'"n-o"', u')', u'%', u'time', u'*', u'a', u'.', u'description', u'(', u"'o'", u')', u'.', u'upper', u'time', u'=', u'time', u'+', u'"o"', u'%', u'time', u'*', u'about', u'.', u'decode', u'(', u'"r-o"', u')', u'time', u'=', u'o', u'[', u"'o'", u']', u'%', u'time', u'%', u'time', u'*', u'about', u'.', u'decode', u'(', u'"a-o"', u')', u'time', u'=', u'about', u'.', u'decode', u'(', u'"a-o"', u')', u'if', u'de', u'==', u'time', u'and', u'te', u'==', u'about_time', u'and', u'te', u'==', u'about_time_c', u':', u'about', u'=', u'about_time_c', u'c,', u'a', u',', u'about_time', u'=', u'about_time_c', u'if', u'de', u'==', u'time', u'and', u'about', u'==', u'about_time_c', u'and', u'string', u'==', u'about_time_b', u':-', u'1', u')', u'about', u'[', u'te', u'-', u'about_time_c', u']', u'=', u'about_time_b', u'+', u'te', u'about_time_c', u'abit', u'=', u'about_time', u'+', u'"o"', u'%', u'about', u'.', u'decode', u'(', u'"r-o"', u')', u'(', u'method', u'=', u'"about"', u'+', u'abit', u')', u'time', u'=', u'Abtimes', u'.', u'fast', u'(', u'time', u',', u'a', u',', u'den_time_c', u')', u'or', u'about', u'[', u'abit', u']', u'and', u'a', u'!=', u'about_time_c', u'.', u'fast', u'and', u'about', u'[', u'abit', u']', u'=', u'about_time_c', u'*', u'about_time_c', u'b', u'for', u'abbyll', u'in', u'about', u'.', u':', u'abbyll', u'=', u'abbyll', u'.', u'd', u'.', u'keys', u'(', u')', u'b', u'=', u'abbyll', u'.', u'bumpy', u'[', u'a', u']', u'# The same as abbyll', u'abbyll', u'=', u'abbyll', u'.', u'abi', u'if', u"ab"', u'==', u'abbyll', u'and', u'"b

[3200 | 12262.51] loss=0.86 avg=1.07
[3201 | 12267.73] loss=0.92 avg=1.07
[3202 | 12273.19] loss=1.41 avg=1.07
[3203 | 12278.51] loss=0.62 avg=1.07
[3204 | 12283.89] loss=0.81 avg=1.07
[3205 | 12289.14] loss=1.02 avg=1.07
[3206 | 12294.41] loss=0.62 avg=1.06
[3207 | 12299.62] loss=1.52 avg=1.07
[3208 | 12304.78] loss=0.46 avg=1.06
[3209 | 12310.11] loss=0.99 avg=1.06
[3210 | 12315.31] loss=0.93 avg=1.06
[3211 | 12320.53] loss=1.01 avg=1.06
[3212 | 12325.79] loss=1.07 avg=1.06
[3213 | 12331.07] loss=0.68 avg=1.05
[3214 | 12336.41] loss=1.01 avg=1.05
[3215 | 12341.61] loss=1.03 avg=1.05
[3216 | 12346.76] loss=1.04 avg=1.05
[3217 | 12352.03] loss=1.07 avg=1.05
[3218 | 12375.81] loss=1.35 avg=1.06
[3219 | 12381.06] loss=0.57 avg=1.05
[3220 | 12658.41] loss=0.60 avg=1.05
[3221 | 12664.14] loss=0.73 avg=1.04
[3222 | 12671.79] loss=0.95 avg=1.04
[3223 | 12686.37] loss=1.21 avg=1.04
[3224 | 12700.94] loss=0.92 avg=1.04
[3225 | 14947.99] loss=0.60 avg=1.04
[3226 | 14953.34] loss=0.80 avg=1.04
[3227 | 14962.00] loss=0.80 avg=1.03
[3228 | 14976.78] loss=0.88 avg=1.03
[3229 | 14991.67] loss=0.59 avg=1.03
[3230 | 15006.27] loss=1.19 avg=1.03
[3231 | 15020.95] loss=1.13 avg=1.03
[3232 | 15035.59] loss=1.16 avg=1.03
[3233 | 15050.01] loss=1.06 avg=1.03
[3234 | 15064.97] loss=1.49 avg=1.04
[3235 | 15079.84] loss=0.59 avg=1.03
[3236 | 15094.85] loss=0.96 avg=1.03
[3237 | 15109.38] loss=0.77 avg=1.03
[3238 | 15124.19] loss=1.44 avg=1.03
[3239 | 15139.60] loss=1.22 avg=1.03
[3240 | 15154.26] loss=0.94 avg=1.03
[3241 | 15168.93] loss=1.30 avg=1.04
[3242 | 15184.00] loss=0.91 avg=1.04
[3243 | 15198.76] loss=1.30 avg=1.04
[3244 | 15213.81] loss=1.49 avg=1.04
[3245 | 15228.32] loss=1.16 avg=1.04
[3246 | 15242.87] loss=1.20 avg=1.05
[3247 | 15257.42] loss=0.99 avg=1.04
[3248 | 15272.18] loss=0.81 avg=1.04
[3249 | 15286.90] loss=1.12 avg=1.04
[3250 | 15301.56] loss=1.58 avg=1.05
[3251 | 15316.26] loss=1.80 avg=1.06
[3252 | 15330.89] loss=1.02 avg=1.06
[3253 | 15345.35] loss=1.03 avg=1.06
[3254 | 15360.16] loss=0.70 avg=1.05
[3255 | 15374.66] loss=0.86 avg=1.05
[3256 | 15389.18] loss=1.17 avg=1.05
[3257 | 15404.15] loss=1.03 avg=1.05
[3258 | 15419.01] loss=1.56 avg=1.06
[3259 | 15433.75] loss=0.92 avg=1.05
[3260 | 15448.30] loss=0.95 avg=1.05
[3261 | 15462.70] loss=1.81 avg=1.06
[3262 | 15477.12] loss=1.01 avg=1.06
[3263 | 15491.84] loss=0.84 avg=1.06
[3264 | 15506.38] loss=1.46 avg=1.06
[3265 | 15521.27] loss=0.39 avg=1.06
[3266 | 15535.80] loss=1.01 avg=1.06
[3267 | 15550.31] loss=0.55 avg=1.05
[3268 | 15560.21] loss=0.63 avg=1.05
[3269 | 15565.42] loss=1.30 avg=1.05
[3270 | 15570.49] loss=0.69 avg=1.04
[3271 | 15575.64] loss=0.82 avg=1.04
[3272 | 15580.79] loss=0.86 avg=1.04
[3273 | 15585.95] loss=0.97 avg=1.04
[3274 | 15591.00] loss=0.73 avg=1.04
[3275 | 15596.08] loss=1.02 avg=1.04
[3276 | 15601.25] loss=0.63 avg=1.03
[3277 | 15606.42] loss=0.62 avg=1.03
[3278 | 15611.54] loss=1.95 avg=1.04
[3279 | 15616.71] loss=0.82 avg=1.04
[3280 | 15621.93] loss=0.86 avg=1.03
[3281 | 15670.46] loss=0.95 avg=1.03
[3282 | 15675.69] loss=0.83 avg=1.03
[3283 | 16657.70] loss=0.89 avg=1.03
[3284 | 16663.15] loss=1.06 avg=1.03
[3285 | 16668.49] loss=1.21 avg=1.03
[3286 | 16673.58] loss=1.33 avg=1.03
[3287 | 16678.87] loss=1.32 avg=1.04
[3288 | 16684.04] loss=0.63 avg=1.03
[3289 | 16689.08] loss=0.62 avg=1.03
[3290 | 16694.40] loss=1.09 avg=1.03
[3291 | 16699.73] loss=0.70 avg=1.03
[3292 | 16705.07] loss=1.13 avg=1.03
[3293 | 16710.29] loss=0.81 avg=1.03
[3294 | 16715.49] loss=0.65 avg=1.02
[3295 | 16720.94] loss=0.79 avg=1.02
[3296 | 16726.49] loss=1.69 avg=1.03
[3297 | 16731.74] loss=0.59 avg=1.02
[3298 | 16737.07] loss=1.25 avg=1.02
[3299 | 16742.40] loss=1.18 avg=1.03
Generating samples...
======== SAMPLE 1 ========
 private' and
       from os.curl_output(__file__, path)

        return cURL;
        }

        def send_file(self, path):
            file_path = os.path.bftype(path)

            if path:
                return sffile
        else:
            filename = sffile.open(filename)

            str = filename

            return cURL;
        }
        return self.send_file(path, file_path).read() [u'def', u'ste_file', u'(', u'self', u',', u'path', u')', u':', u'file_path', u'=', u'self', u'.', u'sync', u'(', u')', u'if', u'self', u'.', u'test', u'(', u'path', u')', u'!=', u'0', u':', u'response_file', u'=', u'self', u'.', u'symlink_file', u'(', u"'file_path'", u')', u'return', u'sffile', u'except', u'HTTPError', u':', u'# We had some bad client requests but didn't log them. Should', u'# be able to parse them. However, 's_file', u'# is still set.', u'if', u'response_file', u'not', u"'starts'", u':', u'# This may fail or make the process more verbose.', u'# This might have to be set, otherwise all are still readable, 's_file', u'# must_ be set', u'# is actually a file', u'('file_path', u')', u'start', u'=', u'self', u'.', u'logging_file', u'(', u'__file__', u',', u'path', u')', u'module_path', u'=', u'os', u'.', u'path', u'.', u'bftype', u'(', u'path', u')', u'if', u'module_path', u'is', u'None', u':', u'}', u'with', u'open', u'(', u'self', u'.', u'start', u')', u'as', u'file_path', u':', u'with', u'open', u'(', u'self', u'.', u'start', u')', u'as', u'srty', u'file_path', u':', u'sext', u'=', u'os', u'.', u'path', u'.', u'bftype', u'(', u'path', u')', u'if', u'srty', u'!=', u'version', u':', u'for', u'path', u'in', u'path', u'.', u'setdefault', u'(', u')', u'if', u'srty', u'==', u'3', u'and', u'path', u'.', u'shape', u'(', u'0', u')', u'.', u'self', u'.', u'module_path', u'and', u'srty', u'from', u'sftype_path', u'as', u'strty', u'.', u'(', u"'s_file'", u')', u')', u':', u'if', u'strty', u'not', u"'starts'", u':', u'# Make sure this file exists but there is already a file', u'start_file', u'=', u'os', u'.', u'path', u'.', u'test_setpath', u'(', u'strty', u'.', u'path', u')', u'self', u'.', u'logging

[3300 | 16830.05] loss=0.29 avg=1.02
[3301 | 16835.54] loss=0.90 avg=1.02
[3302 | 16840.90] loss=1.16 avg=1.02
[3303 | 16846.23] loss=0.71 avg=1.02
[3304 | 16851.47] loss=0.88 avg=1.01
[3305 | 16856.82] loss=0.98 avg=1.01
[3306 | 16862.16] loss=0.88 avg=1.01
[3307 | 16867.46] loss=1.02 avg=1.01
[3308 | 16872.66] loss=1.67 avg=1.02
[3309 | 16877.92] loss=1.16 avg=1.02
[3310 | 16883.07] loss=1.29 avg=1.02
[3311 | 16888.24] loss=1.46 avg=1.03
[3312 | 16893.34] loss=1.46 avg=1.03
[3313 | 16898.37] loss=0.60 avg=1.03
[3314 | 16903.60] loss=1.11 avg=1.03
[3315 | 16908.80] loss=1.50 avg=1.03
[3316 | 16914.16] loss=1.07 avg=1.03
[3317 | 16919.43] loss=0.63 avg=1.03
[3318 | 16924.78] loss=1.70 avg=1.04
[3319 | 16930.11] loss=1.13 avg=1.04
[3320 | 16935.40] loss=0.93 avg=1.04
[3321 | 16940.77] loss=0.56 avg=1.03
[3322 | 16946.09] loss=1.75 avg=1.04
[3323 | 16951.39] loss=1.40 avg=1.04
[3324 | 16956.69] loss=0.89 avg=1.04
[3325 | 16962.12] loss=0.68 avg=1.04
[3326 | 16967.54] loss=1.27 avg=1.04
[3327 | 16972.96] loss=1.16 avg=1.04
[3328 | 16978.33] loss=1.76 avg=1.05
[3329 | 16983.70] loss=0.88 avg=1.05
[3330 | 16989.08] loss=0.80 avg=1.04
[3331 | 16994.41] loss=1.05 avg=1.04
[3332 | 16999.86] loss=1.24 avg=1.05
[3333 | 17005.33] loss=0.58 avg=1.04
[3334 | 17010.78] loss=0.93 avg=1.04
[3335 | 17016.11] loss=1.12 avg=1.04
[3336 | 17021.43] loss=0.77 avg=1.04
[3337 | 17026.84] loss=1.24 avg=1.04
[3338 | 17032.16] loss=0.74 avg=1.04
[3339 | 17037.65] loss=1.20 avg=1.04
[3340 | 17043.03] loss=0.84 avg=1.04
[3341 | 17048.44] loss=1.08 avg=1.04
[3342 | 17053.92] loss=0.61 avg=1.03
[3343 | 17059.33] loss=1.30 avg=1.04
[3344 | 17064.63] loss=1.25 avg=1.04
[3345 | 17070.04] loss=1.93 avg=1.05
[3346 | 17075.28] loss=0.69 avg=1.04
[3347 | 17080.57] loss=0.80 avg=1.04
[3348 | 17085.96] loss=1.43 avg=1.04
[3349 | 17091.37] loss=1.28 avg=1.05
[3350 | 17096.72] loss=0.63 avg=1.04
[3351 | 17101.95] loss=1.17 avg=1.04
[3352 | 17107.24] loss=1.10 avg=1.04
[3353 | 17112.59] loss=1.62 avg=1.05
[3354 | 17117.84] loss=0.89 avg=1.05
[3355 | 17123.16] loss=0.64 avg=1.04
[3356 | 17128.37] loss=1.52 avg=1.05
[3357 | 17133.67] loss=1.00 avg=1.05
[3358 | 17138.95] loss=1.61 avg=1.05
[3359 | 17144.26] loss=1.11 avg=1.05
[3360 | 17149.63] loss=1.02 avg=1.05
[3361 | 17154.95] loss=1.43 avg=1.06
[3362 | 17160.24] loss=0.72 avg=1.05
[3363 | 17165.46] loss=1.45 avg=1.06
[3364 | 17170.65] loss=1.31 avg=1.06
[3365 | 17175.95] loss=1.36 avg=1.06
[3366 | 17181.05] loss=0.84 avg=1.06
[3367 | 17186.31] loss=1.05 avg=1.06
[3368 | 17191.58] loss=0.84 avg=1.06
[3369 | 17196.83] loss=2.10 avg=1.07
[3370 | 17202.00] loss=1.36 avg=1.07
[3371 | 17207.29] loss=1.02 avg=1.07
[3372 | 17212.52] loss=0.82 avg=1.07
[3373 | 17217.88] loss=1.41 avg=1.07
[3374 | 17223.16] loss=0.67 avg=1.07
[3375 | 17228.45] loss=1.04 avg=1.07
[3376 | 17233.61] loss=0.86 avg=1.07
[3377 | 17239.02] loss=0.59 avg=1.06
[3378 | 17244.26] loss=1.36 avg=1.07
[3379 | 17249.54] loss=0.64 avg=1.06
[3380 | 17254.78] loss=1.54 avg=1.07
[3381 | 17260.06] loss=0.95 avg=1.06
[3382 | 17265.32] loss=1.51 avg=1.07
[3383 | 17270.60] loss=0.75 avg=1.07
[3384 | 17275.85] loss=0.82 avg=1.06
[3385 | 17281.04] loss=1.31 avg=1.07
[3386 | 17286.31] loss=0.93 avg=1.06
[3387 | 17291.45] loss=0.80 avg=1.06
[3388 | 17296.59] loss=1.56 avg=1.07
[3389 | 17301.75] loss=1.15 avg=1.07
[3390 | 17306.95] loss=1.42 avg=1.07
[3391 | 17312.14] loss=0.63 avg=1.07
[3392 | 17317.32] loss=0.80 avg=1.06
[3393 | 17322.59] loss=0.94 avg=1.06
[3394 | 17327.85] loss=0.96 avg=1.06
[3395 | 17333.11] loss=0.95 avg=1.06
[3396 | 17338.37] loss=0.89 avg=1.06
[3397 | 17343.48] loss=1.71 avg=1.07
[3398 | 17348.73] loss=0.53 avg=1.06
[3399 | 17353.96] loss=1.59 avg=1.07
Generating samples...
======== SAMPLE 1 ========
                  if rnd_targets == nrnd_targets:
                    rnd_targets[self._idx] = None
                   if rnd_targets['q', ''] != self._idx:
        if                 fname:
                     self._names = None
                    if self._names:

                         rnd_targets['eq', ''] = fname
                       if len(self._names) != self._idx:
                          self._names[rnd_targets['q', '']] = rnd_targets['q', '']

                      fname = list(fname), ([
            'id', 'x', 'a','w', '_name'])
                      )

                   rnd_targets['q', 'id'] = rnd_targets
                    if rnd_targets['q'] != self._idx:
                         self._names = None
                       if self._names:
                         fname = list(fname), (
                               'name'):
                        if not fname:
                            self._names = [name]
                         fname = list(fname)

                            rnd_targets['q', 'name'] = [name]

                         if rnd_targets['q'] != self._idx:
                          self._names = None

                    if rnd_targets['q'] == self._idx:
                             self._names = list(fname), (
                               'name'):
                          if rnd_targets['q'] != self._idx:
                             if len(self._names) != self._idx:
                     

[3400 | 17442.50] loss=0.94 avg=1.06
[3401 | 17447.64] loss=0.63 avg=1.06
[3402 | 17452.84] loss=1.19 avg=1.06
[3403 | 17458.13] loss=0.67 avg=1.06
[3404 | 17463.36] loss=1.01 avg=1.06
[3405 | 17468.64] loss=0.87 avg=1.05
[3406 | 17473.89] loss=0.71 avg=1.05
[3407 | 17479.15] loss=1.20 avg=1.05
[3408 | 17484.31] loss=1.52 avg=1.06
[3409 | 17489.65] loss=0.58 avg=1.05
[3410 | 17494.88] loss=1.11 avg=1.05
[3411 | 17500.16] loss=0.73 avg=1.05
[3412 | 17505.39] loss=1.59 avg=1.06
[3413 | 17510.78] loss=1.01 avg=1.05
[3414 | 17516.11] loss=1.37 avg=1.06
[3415 | 17521.26] loss=1.35 avg=1.06
[3416 | 17526.46] loss=1.94 avg=1.07
[3417 | 17531.70] loss=0.61 avg=1.07
[3418 | 17536.93] loss=1.35 avg=1.07
[3419 | 17542.15] loss=1.16 avg=1.07
[3420 | 17547.33] loss=0.91 avg=1.07
[3421 | 17552.62] loss=0.65 avg=1.06
[3422 | 17557.83] loss=1.29 avg=1.07
[3423 | 17563.04] loss=1.35 avg=1.07
[3424 | 17568.34] loss=0.77 avg=1.07
[3425 | 17573.55] loss=0.70 avg=1.06
[3426 | 17578.77] loss=0.68 avg=1.06
[3427 | 17583.92] loss=0.72 avg=1.05
[3428 | 17589.11] loss=0.97 avg=1.05
[3429 | 17594.33] loss=0.66 avg=1.05
[3430 | 17599.53] loss=1.40 avg=1.05
[3431 | 17604.65] loss=1.21 avg=1.05
[3432 | 17609.85] loss=2.10 avg=1.07
[3433 | 17615.03] loss=0.96 avg=1.06
[3434 | 17620.15] loss=0.70 avg=1.06
[3435 | 17625.33] loss=0.87 avg=1.06
[3436 | 17630.55] loss=1.12 avg=1.06
[3437 | 17635.70] loss=0.82 avg=1.06
[3438 | 17640.94] loss=0.65 avg=1.05
[3439 | 17646.10] loss=1.28 avg=1.05
[3440 | 17651.25] loss=0.75 avg=1.05
[3441 | 17656.46] loss=0.80 avg=1.05
[3442 | 17661.71] loss=2.04 avg=1.06
[3443 | 17666.94] loss=1.49 avg=1.06
[3444 | 17672.08] loss=0.99 avg=1.06
[3445 | 17677.34] loss=0.89 avg=1.06
[3446 | 17682.51] loss=0.79 avg=1.06
[3447 | 17687.67] loss=1.31 avg=1.06
[3448 | 17692.91] loss=0.77 avg=1.06
[3449 | 17698.09] loss=1.12 avg=1.06
[3450 | 17703.31] loss=1.18 avg=1.06
[3451 | 17708.53] loss=1.52 avg=1.06
[3452 | 17713.69] loss=1.14 avg=1.07
[3453 | 17718.93] loss=1.19 avg=1.07
[3454 | 17724.15] loss=0.94 avg=1.07
[3455 | 17729.29] loss=1.08 avg=1.07
[3456 | 17734.48] loss=0.98 avg=1.06
[3457 | 17739.65] loss=1.20 avg=1.07
[3458 | 17744.85] loss=0.83 avg=1.06
[3459 | 17750.12] loss=1.40 avg=1.07
[3460 | 17755.31] loss=1.32 avg=1.07
[3461 | 17760.44] loss=0.66 avg=1.07
[3462 | 17765.59] loss=0.75 avg=1.06
[3463 | 17770.88] loss=0.87 avg=1.06
[3464 | 17776.05] loss=1.03 avg=1.06
[3465 | 17781.19] loss=1.16 avg=1.06
[3466 | 17786.44] loss=0.67 avg=1.06
[3467 | 17791.63] loss=1.03 avg=1.06
[3468 | 17796.88] loss=0.82 avg=1.05
[3469 | 17802.02] loss=0.71 avg=1.05
[3470 | 17807.22] loss=1.12 avg=1.05
[3471 | 17812.36] loss=0.75 avg=1.05
[3472 | 17817.49] loss=1.08 avg=1.05
[3473 | 17822.64] loss=1.77 avg=1.06
[3474 | 17827.89] loss=0.83 avg=1.05
[3475 | 17833.02] loss=0.76 avg=1.05
[3476 | 17838.18] loss=1.54 avg=1.06
[3477 | 17843.30] loss=0.92 avg=1.05
[3478 | 17848.51] loss=1.01 avg=1.05
[3479 | 17853.69] loss=1.37 avg=1.06
[3480 | 17858.83] loss=0.62 avg=1.05
[3481 | 17864.07] loss=0.56 avg=1.05
[3482 | 17869.25] loss=0.76 avg=1.04
[3483 | 17874.47] loss=1.21 avg=1.05
[3484 | 17879.66] loss=0.80 avg=1.04
[3485 | 17884.89] loss=0.59 avg=1.04
[3486 | 17890.22] loss=1.09 avg=1.04
[3487 | 17895.45] loss=0.88 avg=1.04
[3488 | 17900.66] loss=0.86 avg=1.04
[3489 | 17906.12] loss=0.56 avg=1.03
[3490 | 17911.28] loss=0.61 avg=1.03
[3491 | 17916.47] loss=1.32 avg=1.03
[3492 | 17921.85] loss=0.91 avg=1.03
[3493 | 17927.16] loss=1.04 avg=1.03
[3494 | 17932.43] loss=0.76 avg=1.03
[3495 | 17937.70] loss=1.57 avg=1.03
[3496 | 17942.79] loss=1.03 avg=1.03
[3497 | 17948.06] loss=1.09 avg=1.03
[3498 | 17953.20] loss=0.82 avg=1.03
[3499 | 17958.44] loss=1.15 avg=1.03
Generating samples...
======== SAMPLE 1 ========
, u'if', u'not', u'null', u'}', u'.', u'format', u'(', u'self', u'.', u'api', u')', u'if', u"'client_id'", u'in', u'[', u"'Clientid'", u']', u':', u'client_id', u'=', u'client_id', u'return', u'client_id'] Create a client.

         :func: if 'cidr_client' in ['ClientID', 'CIDr_ClientID']',
         :func: if 'cidr_client' in ['CIDr_ClientID']',
         :func: if 'cidr_client' in ['CIDr_ClientID']',
         :func: if 'cidr_client' in ['cidr_client_id']',
         :func: if '['.format(cidr_client', CIDr_ClientId))',
         :func: if '['.format(cidr_client_id', CIDr_ClientID)]", u'if', u'required', u'else', u'if', u"'client_id'", u'in', u'[', u"'Clientid'", u']', u':', u'client_id', u'=', u'client_id', u'return', u'client_id] Creates a client.

         :func: if 'cidr_client' in ['CIDr_ClientID']',
         :func: if '[.'format(cidr_client_id', CIDr_ClientId))',
         :func: if '[.'format(cidr_client_id', CIDr_ClientId)]", u'if', u'len', u'(', u'cidr_client_id', u'or', u'len', u'(', u'cidr_client_id', u'or', u'len', u'(', u'cidr_client_id', u'or', u'len', u'(', u'cidr_client_id', u'or', u'len', u'(', u'cidr_client_id', u'or', u'len', u'(', u'cidr_client_id', u'or', u'len', u'(', u'cidr_client_id', u')', u'or', u'len', u'(', u'cidr_client_id', u'or', u'len', u'(', u'cidr_client_id', u')', u'or', u'len', u'(', u'cidr_client_id', u')', u'or', u'len', u'(', u'cidr_client_id', u']', u')', u')', u'else', u'if', u"'cidr_client_id'", u'in', u'[', u"'CIDr_ClientId'", u']', u':', u'cidr_client_id', u'=', u'cidr_client_id', u'else', u'if', u"'cidr_client_id'", u'in', u'[', u"'CIDr_ClientID'", u']', u':', u'cidr_client_id', u'=', u'cidr_client_id'][', u"'cidr_connection'", u']', u'if', u"client_id"", u'in', u'[', u"'Clientid'", u']', u'(', u"'cidr_client_id'", u',', u"'cidr_client_id'", u')', u':', u'cidr_client_id', u'=', u'client_id', u'if', u'in', u"'client_id'", u':', u'client_id', u'=', u'client_id', u'or', u'self', u'.', u'api', u'.', u'get', u'(', u')', u'[

[3500 | 18046.46] loss=0.96 avg=1.03
[3501 | 18051.62] loss=0.63 avg=1.03
[3502 | 18057.00] loss=1.02 avg=1.03
[3503 | 18062.33] loss=0.73 avg=1.02
[3504 | 18067.61] loss=0.50 avg=1.02
[3505 | 18072.80] loss=1.49 avg=1.02
[3506 | 18078.03] loss=1.05 avg=1.02
[3507 | 18083.30] loss=1.51 avg=1.03
[3508 | 18088.68] loss=1.02 avg=1.03
[3509 | 18093.91] loss=0.93 avg=1.03
[3510 | 18099.07] loss=1.48 avg=1.03
[3511 | 18104.37] loss=1.87 avg=1.04
[3512 | 18109.66] loss=0.85 avg=1.04
[3513 | 18115.04] loss=0.85 avg=1.04
[3514 | 18120.21] loss=1.32 avg=1.04
[3515 | 18125.54] loss=1.60 avg=1.04
[3516 | 18130.74] loss=0.68 avg=1.04
[3517 | 18135.99] loss=1.03 avg=1.04
[3518 | 18141.21] loss=0.50 avg=1.04
[3519 | 18146.36] loss=0.82 avg=1.03
[3520 | 18151.57] loss=0.81 avg=1.03
[3521 | 18156.79] loss=1.49 avg=1.04
[3522 | 18161.96] loss=1.75 avg=1.04
[3523 | 18167.13] loss=0.76 avg=1.04
[3524 | 18172.32] loss=0.55 avg=1.04
[3525 | 18177.50] loss=0.99 avg=1.03
[3526 | 18182.73] loss=0.81 avg=1.03
[3527 | 18187.85] loss=0.98 avg=1.03
[3528 | 18193.03] loss=0.81 avg=1.03
[3529 | 18198.14] loss=0.77 avg=1.03
[3530 | 18203.31] loss=1.10 avg=1.03
[3531 | 18208.46] loss=0.56 avg=1.02
[3532 | 18213.61] loss=1.00 avg=1.02
[3533 | 18218.90] loss=1.07 avg=1.02
[3534 | 18224.06] loss=0.79 avg=1.02
[3535 | 18229.32] loss=1.53 avg=1.03
[3536 | 18234.49] loss=0.87 avg=1.02
[3537 | 18239.67] loss=0.76 avg=1.02
[3538 | 18244.94] loss=1.44 avg=1.03
[3539 | 18250.22] loss=0.93 avg=1.03
[3540 | 18255.45] loss=0.53 avg=1.02
[3541 | 18260.66] loss=1.03 avg=1.02
[3542 | 18265.85] loss=0.99 avg=1.02
[3543 | 18271.12] loss=0.82 avg=1.02
[3544 | 18276.34] loss=0.85 avg=1.02
[3545 | 18281.71] loss=1.21 avg=1.02
[3546 | 18287.06] loss=1.20 avg=1.02
[3547 | 18292.33] loss=0.62 avg=1.02
[3548 | 18297.60] loss=1.31 avg=1.02
[3549 | 18302.91] loss=0.70 avg=1.02
[3550 | 18308.05] loss=0.59 avg=1.01
[3551 | 18313.24] loss=1.22 avg=1.01
[3552 | 18318.42] loss=0.90 avg=1.01
[3553 | 18323.68] loss=0.91 avg=1.01
[3554 | 18329.01] loss=0.91 avg=1.01
[3555 | 18334.23] loss=0.98 avg=1.01
[3556 | 18339.53] loss=0.64 avg=1.01
[3557 | 18344.67] loss=1.45 avg=1.01
[3558 | 18349.93] loss=1.39 avg=1.02
[3559 | 18355.13] loss=0.71 avg=1.01
[3560 | 18360.27] loss=0.89 avg=1.01
[3561 | 18365.39] loss=0.99 avg=1.01
[3562 | 18370.53] loss=1.23 avg=1.01
[3563 | 18375.62] loss=1.18 avg=1.01
[3564 | 18380.78] loss=1.21 avg=1.02
[3565 | 18385.93] loss=0.71 avg=1.01
[3566 | 18391.09] loss=1.48 avg=1.02
[3567 | 18396.26] loss=1.33 avg=1.02
[3568 | 18401.54] loss=0.71 avg=1.02
[3569 | 18406.73] loss=0.46 avg=1.01
[3570 | 18411.94] loss=1.15 avg=1.01
[3571 | 18417.15] loss=1.55 avg=1.02
[3572 | 18422.55] loss=0.76 avg=1.02
[3573 | 18427.73] loss=0.95 avg=1.02
[3574 | 18432.85] loss=0.78 avg=1.01
[3575 | 18438.21] loss=0.87 avg=1.01
[3576 | 18443.52] loss=0.80 avg=1.01
[3577 | 18448.83] loss=0.83 avg=1.01
[3578 | 18454.01] loss=0.77 avg=1.01
[3579 | 18459.30] loss=0.93 avg=1.01
[3580 | 18464.48] loss=1.15 avg=1.01
[3581 | 18469.68] loss=1.03 avg=1.01
[3582 | 18474.89] loss=1.47 avg=1.01
[3583 | 18480.20] loss=0.82 avg=1.01
[3584 | 18485.40] loss=1.82 avg=1.02
[3585 | 18490.60] loss=0.80 avg=1.02
[3586 | 18495.83] loss=0.62 avg=1.01
[3587 | 18501.08] loss=1.32 avg=1.01
[3588 | 18506.28] loss=0.94 avg=1.01
[3589 | 18511.53] loss=1.37 avg=1.02
[3590 | 18516.65] loss=0.88 avg=1.02
[3591 | 18521.76] loss=2.08 avg=1.03
[3592 | 18527.00] loss=1.37 avg=1.03
[3593 | 18532.22] loss=1.19 avg=1.03
[3594 | 18537.52] loss=1.35 avg=1.03
[3595 | 18542.92] loss=1.63 avg=1.04
[3596 | 18548.05] loss=0.79 avg=1.04
[3597 | 18553.35] loss=0.94 avg=1.04
[3598 | 18558.67] loss=0.92 avg=1.04
[3599 | 18563.92] loss=0.74 avg=1.03
Generating samples...
======== SAMPLE 1 ========
', u'=', u'1', u'to', u'(', u'"\n"', u')', u'\n')', u'to', u'=', u'(', u'to', u'[', u"'name'", u']', u')', u'reverse', u'=', u'False', u'to', u'.', u'as', u'(', u"'n'", u')', u'is', u'a', u'or', u'n', u'=', u'0', u'to', u'.', u'to', u'if', u'n', u'is', u'None', u'else', u'strip', u'=', u'True', u'o', u'.', u'strip', u'.', u'to', u'max', u'(', u'strip', u'.', u'get_normal', u'(', u'strip', u')', u',', u'strip', u'.', u'dict', u')', u'except', u'False', u',', u'return', u'max_trials', u'(', u'trials', u',', u'i', u')', u'=', u'1', u'(', u'/', u'2', u')', u'\n', u'=', u'trials', u'max_trials', u'.', u'result', u'(', u'trials', u',', u'i', u')', u'=', u'max_trials', u'.', u'result', u'(', u'trials', u',', u'i'.', u'dict', u')', u'.', u'get', u'(', u'(', u')', u')', u'reverse', u'=', u'True', u'out', u'=', u'True', u'strip', u'.', u'to', u'.', u'as', u'(', u"'n'", u')', u'is', u'a', u'or', u'n', u'=', u'0', u'for', u'num', u'in', u'trials', u':', u'trials', u'.', u'result', u'(', u'num', u',', u'num', u')', u'=', u'num', u'trials', u'.', u'get', u'(', u'num', u',', u'reverse', u')', u'reverse', u'=', u'False', u'to', u'.', u'as', u'(', u"'n'", u')', u'is', u'a', u'or', u'n', u'=', u'0', u'mt', u'=', u'mt', u'reverse', u'.', u'result', u'(', u'num', u',', u'mt', u')', u'.', u'as', u'(', u'-', u'num', u')', u'if', u'm', u'is', u'None', u'else', u'strip', u'=', u'result', u'(', u'num', u',', u'num', u')', u'.', u'to', u'.', u'get', u'(', u'num', u',', u')', u'.', u'strip', u'.', u'reupt', u'(', u'strip', u',', u'strip', u')', u'.', u'max', u'(', u'num', u',', u'm', u')', u'strip', u'.', u'max', u'(', u'num', u',', u'm', u')', u'.', u'strip', u'.', u'max', u'(', u'strip', u',', u'strip', u')', u'.', u'ret', u'.', u'reupt', u'(', u'return', u'reverse', u')', u'if', u"'n'", u'in', u'strip', u':', u'raise', u'(', u'"Invalid trialled: "', u'if', u"'n'", u'is', u'not', u'given', u'and', u'num', u'<=', u'0", u')', u

[3600 | 18653.53] loss=0.57 avg=1.03
[3601 | 18658.82] loss=0.80 avg=1.03
[3602 | 18664.10] loss=1.01 avg=1.03
[3603 | 18669.34] loss=0.91 avg=1.02
[3604 | 18674.49] loss=1.10 avg=1.03
[3605 | 18679.57] loss=0.86 avg=1.02
[3606 | 18684.77] loss=0.92 avg=1.02
[3607 | 18689.86] loss=0.67 avg=1.02
[3608 | 18694.97] loss=0.97 avg=1.02
[3609 | 18700.18] loss=0.69 avg=1.02
[3610 | 18705.31] loss=1.90 avg=1.02
[3611 | 18710.51] loss=0.73 avg=1.02
[3612 | 18715.74] loss=0.88 avg=1.02
[3613 | 18720.85] loss=0.86 avg=1.02
[3614 | 18726.01] loss=1.50 avg=1.02
[3615 | 18731.29] loss=1.20 avg=1.02
[3616 | 18736.45] loss=0.86 avg=1.02
[3617 | 18741.64] loss=0.77 avg=1.02
[3618 | 18746.83] loss=0.89 avg=1.02
[3619 | 18752.04] loss=1.57 avg=1.02
[3620 | 18757.18] loss=1.32 avg=1.03
[3621 | 18762.45] loss=1.02 avg=1.03
[3622 | 18767.58] loss=0.65 avg=1.02
[3623 | 18772.82] loss=0.82 avg=1.02
[3624 | 18778.02] loss=0.41 avg=1.02
[3625 | 18783.21] loss=1.67 avg=1.02
[3626 | 18788.37] loss=1.40 avg=1.03
[3627 | 18793.46] loss=0.68 avg=1.02
[3628 | 18798.67] loss=0.89 avg=1.02
[3629 | 18803.78] loss=1.71 avg=1.03
[3630 | 18808.97] loss=1.41 avg=1.03
[3631 | 18814.15] loss=0.99 avg=1.03
[3632 | 18819.34] loss=0.82 avg=1.03
[3633 | 18824.56] loss=1.81 avg=1.04
[3634 | 18829.71] loss=1.04 avg=1.04
[3635 | 18834.84] loss=1.00 avg=1.04
[3636 | 18839.99] loss=1.14 avg=1.04
[3637 | 18845.04] loss=1.05 avg=1.04
[3638 | 18850.25] loss=1.04 avg=1.04
[3639 | 18855.48] loss=0.65 avg=1.03
[3640 | 18860.60] loss=0.90 avg=1.03
[3641 | 18865.85] loss=1.49 avg=1.04
[3642 | 18871.04] loss=1.21 avg=1.04
[3643 | 18876.23] loss=1.25 avg=1.04
[3644 | 18881.34] loss=1.93 avg=1.05
[3645 | 18886.54] loss=0.66 avg=1.05
[3646 | 18891.78] loss=1.52 avg=1.05
[3647 | 18896.99] loss=0.94 avg=1.05
[3648 | 18902.35] loss=1.07 avg=1.05
[3649 | 18907.49] loss=1.22 avg=1.05
[3650 | 18912.77] loss=0.80 avg=1.05
[3651 | 18917.94] loss=1.08 avg=1.05
[3652 | 18923.24] loss=1.25 avg=1.05
[3653 | 18928.52] loss=0.99 avg=1.05
[3654 | 18933.67] loss=1.33 avg=1.05
[3655 | 18938.89] loss=1.11 avg=1.05
[3656 | 18944.14] loss=0.76 avg=1.05
[3657 | 18949.41] loss=1.07 avg=1.05
[3658 | 18954.75] loss=0.96 avg=1.05
[3659 | 18959.99] loss=0.71 avg=1.05
[3660 | 18965.26] loss=0.72 avg=1.04
[3661 | 18970.49] loss=1.38 avg=1.05
[3662 | 18975.66] loss=0.72 avg=1.04
[3663 | 18980.88] loss=0.57 avg=1.04
[3664 | 18986.08] loss=1.18 avg=1.04
[3665 | 18991.33] loss=1.38 avg=1.04
[3666 | 18996.59] loss=1.18 avg=1.05
[3667 | 19001.66] loss=0.54 avg=1.04
[3668 | 19006.79] loss=1.02 avg=1.04
[3669 | 19012.04] loss=0.61 avg=1.04
[3670 | 19017.24] loss=1.02 avg=1.04
[3671 | 19022.49] loss=0.68 avg=1.03
[3672 | 19027.57] loss=1.43 avg=1.04
[3673 | 19032.88] loss=0.70 avg=1.03
[3674 | 19038.06] loss=1.16 avg=1.03
[3675 | 19043.14] loss=0.85 avg=1.03
[3676 | 19048.39] loss=1.36 avg=1.04
[3677 | 19053.64] loss=0.90 avg=1.03
[3678 | 19058.85] loss=0.59 avg=1.03
[3679 | 19064.09] loss=0.96 avg=1.03
[3680 | 19069.23] loss=0.97 avg=1.03
[3681 | 19074.39] loss=1.36 avg=1.03
[3682 | 19079.55] loss=1.05 avg=1.03
[3683 | 19084.72] loss=0.59 avg=1.03
[3684 | 19089.90] loss=0.62 avg=1.02
[3685 | 19095.05] loss=1.48 avg=1.03
[3686 | 19100.31] loss=0.75 avg=1.03
[3687 | 19105.47] loss=0.85 avg=1.02
[3688 | 19110.66] loss=1.24 avg=1.03
[3689 | 19115.86] loss=0.76 avg=1.02
[3690 | 19121.07] loss=0.85 avg=1.02
[3691 | 19126.28] loss=1.11 avg=1.02
[3692 | 19131.57] loss=1.52 avg=1.03
[3693 | 19136.88] loss=1.01 avg=1.03
[3694 | 19142.04] loss=0.49 avg=1.02
[3695 | 19147.30] loss=0.92 avg=1.02
[3696 | 19152.53] loss=0.83 avg=1.02
[3697 | 19157.76] loss=1.27 avg=1.02
[3698 | 19162.95] loss=1.11 avg=1.02
[3699 | 19168.11] loss=0.88 avg=1.02
Generating samples...
======== SAMPLE 1 ========
.dst_node_offset,
                                                                                                                                                                                                                                                                                                                                                                                                                                            )],
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         

[3700 | 19255.91] loss=1.03 avg=1.02
[3701 | 19261.07] loss=0.80 avg=1.02
[3702 | 19266.33] loss=0.96 avg=1.02
[3703 | 19271.48] loss=1.00 avg=1.02
[3704 | 19276.51] loss=0.83 avg=1.02
[3705 | 19281.66] loss=0.67 avg=1.01
[3706 | 19286.78] loss=1.02 avg=1.01
[3707 | 19291.91] loss=1.00 avg=1.01
[3708 | 19297.13] loss=0.75 avg=1.01
[3709 | 19302.26] loss=0.76 avg=1.01
[3710 | 19307.47] loss=0.93 avg=1.01
[3711 | 19312.66] loss=0.60 avg=1.00
[3712 | 19317.80] loss=1.63 avg=1.01
[3713 | 19323.03] loss=1.14 avg=1.01
[3714 | 19328.25] loss=0.51 avg=1.01
[3715 | 19333.34] loss=1.16 avg=1.01
[3716 | 19338.46] loss=1.30 avg=1.01
[3717 | 19343.63] loss=1.38 avg=1.01
[3718 | 19348.95] loss=0.83 avg=1.01
[3719 | 19354.07] loss=1.14 avg=1.01
[3720 | 19359.15] loss=0.80 avg=1.01
[3721 | 19364.38] loss=1.27 avg=1.01
[3722 | 19369.51] loss=0.68 avg=1.01
[3723 | 19374.78] loss=0.90 avg=1.01
[3724 | 19380.08] loss=1.37 avg=1.01
[3725 | 19385.31] loss=1.04 avg=1.01
[3726 | 19390.55] loss=1.10 avg=1.01
[3727 | 19395.64] loss=0.60 avg=1.01
[3728 | 19400.84] loss=1.30 avg=1.01
[3729 | 19406.10] loss=1.02 avg=1.01
[3730 | 19411.35] loss=1.23 avg=1.01
[3731 | 19416.50] loss=0.62 avg=1.01
[3732 | 19421.70] loss=0.86 avg=1.01
[3733 | 19426.89] loss=0.86 avg=1.01
[3734 | 19432.08] loss=0.59 avg=1.00
[3735 | 19437.32] loss=1.09 avg=1.00
[3736 | 19442.55] loss=1.47 avg=1.01
[3737 | 19447.76] loss=0.76 avg=1.01
[3738 | 19452.93] loss=0.59 avg=1.00
[3739 | 19458.30] loss=1.62 avg=1.01
[3740 | 19463.49] loss=1.52 avg=1.01
[3741 | 19468.66] loss=0.62 avg=1.01
[3742 | 19473.81] loss=0.72 avg=1.01
[3743 | 19478.93] loss=1.14 avg=1.01
[3744 | 19484.21] loss=1.15 avg=1.01
[3745 | 19489.61] loss=0.61 avg=1.01
[3746 | 19494.90] loss=1.28 avg=1.01
[3747 | 19500.27] loss=1.15 avg=1.01
[3748 | 19505.48] loss=0.58 avg=1.01
[3749 | 19510.89] loss=0.53 avg=1.00
[3750 | 19516.07] loss=1.18 avg=1.00
[3751 | 19521.32] loss=0.88 avg=1.00
[3752 | 19526.47] loss=0.81 avg=1.00
[3753 | 19531.67] loss=0.94 avg=1.00
[3754 | 19536.86] loss=1.27 avg=1.00
[3755 | 19542.14] loss=1.15 avg=1.00
[3756 | 19547.34] loss=0.63 avg=1.00
[3757 | 19552.58] loss=1.08 avg=1.00
[3758 | 19557.83] loss=0.82 avg=1.00
[3759 | 19563.12] loss=1.62 avg=1.00
[3760 | 19568.31] loss=1.13 avg=1.01
[3761 | 19573.56] loss=1.87 avg=1.01
[3762 | 19578.81] loss=1.46 avg=1.02
[3763 | 19584.10] loss=1.15 avg=1.02
[3764 | 19589.34] loss=0.59 avg=1.02
[3765 | 19594.60] loss=1.16 avg=1.02
[3766 | 19599.84] loss=0.98 avg=1.02
[3767 | 19604.93] loss=1.07 avg=1.02
[3768 | 19610.17] loss=1.06 avg=1.02
[3769 | 19615.29] loss=1.47 avg=1.02
[3770 | 19620.45] loss=1.76 avg=1.03
[3771 | 19625.58] loss=2.13 avg=1.04
[3772 | 19630.76] loss=0.74 avg=1.04
[3773 | 19635.93] loss=1.45 avg=1.04
[3774 | 19641.07] loss=1.63 avg=1.05
[3775 | 19646.36] loss=1.32 avg=1.05
[3776 | 19651.54] loss=1.64 avg=1.06
[3777 | 19656.76] loss=0.94 avg=1.06
[3778 | 19661.92] loss=1.21 avg=1.06
[3779 | 19667.13] loss=1.16 avg=1.06
[3780 | 19672.37] loss=0.70 avg=1.05
[3781 | 19677.58] loss=1.00 avg=1.05
[3782 | 19682.76] loss=0.74 avg=1.05
[3783 | 19687.95] loss=0.80 avg=1.05
[3784 | 19693.19] loss=0.86 avg=1.05
[3785 | 19698.35] loss=1.94 avg=1.06
[3786 | 19703.54] loss=1.00 avg=1.05
[3787 | 19708.75] loss=1.03 avg=1.05
[3788 | 19713.98] loss=1.53 avg=1.06
[3789 | 19719.13] loss=0.72 avg=1.06
[3790 | 19724.19] loss=0.88 avg=1.05
[3791 | 19729.54] loss=1.06 avg=1.05
[3792 | 19734.85] loss=0.54 avg=1.05
[3793 | 19740.10] loss=1.21 avg=1.05
[3794 | 19745.44] loss=0.89 avg=1.05
[3795 | 19750.75] loss=1.48 avg=1.05
[3796 | 19755.91] loss=1.19 avg=1.05
[3797 | 19761.06] loss=0.81 avg=1.05
[3798 | 19766.25] loss=0.89 avg=1.05
[3799 | 19771.41] loss=0.34 avg=1.04
Generating samples...
======== SAMPLE 1 ========
 Uses' :param kw_kw_kw_kw: Thekw instance to use in the model, and
                   the model path. The model path must match the path for
                    any of the other arguments.
         Raises Exception if kw_kw_kw is None: If kw_kw_kw does not match the path for
                    any of the other arguments.   
         Exception if we are missing any of or the model path.  If we are missing None,
                    raise Exception again in cases of a failure.
                if the model path is missing any of the other arguments (e.g. kw_kw_kw must not
                                                                     it is missing None,
                                                                            not None):
               raise Exception again if the model path matches any of the other arguments.
            """
        if not hasattr (kw):
          raise Exception (kw.__getattribute__, 'kw_kw_kw').
         if (isinstance(kw, ModelModel) and kw):
            kw = kw.toList()
            if kw:
                if kw == 'model':
                   kw = kw.toList()

       
        if None(kw) == kw:
            raise Exception, 'kw has not been given'.
       
        if kw == 'kwargs':
            raise Exception, 'kw args cannot be given'.
       
       
       
       
        # the model is to-be-changed
        # kw_kw is the model object
        kw = kw.toList()
       
       
        return kw_kw [u'def', u'get_model', u'(', u'self', u',', u'kw_kw', u',', u'matter', u')', u':', u'kw_kw', u'=', u'kw', u'.', u'toList', u'(', u')', u'kw_kw', u'=', u'kw_kw', u'.', u'replace', u'(', u'kw', u')', u'kw_kw', u'except', u':', u'kw_kw', u'=', u'None', u'# the model is to-be-changed', u'matter', u'=', u'kw_kw', u'.', u'toList', u'(', u')', u'matter', u'=', u'kw', u'.', u'toList', u'kw_kw', u'kw_kw', u''=', u'kw'] Get the new model object and return the following

[3800 | 19860.12] loss=1.29 avg=1.05
[3801 | 19865.45] loss=0.57 avg=1.04
[3802 | 19870.75] loss=1.53 avg=1.05
[3803 | 19875.95] loss=1.54 avg=1.05
[3804 | 19881.16] loss=1.11 avg=1.05
[3805 | 19886.39] loss=1.46 avg=1.06
[3806 | 19891.62] loss=0.82 avg=1.05
[3807 | 19896.81] loss=0.68 avg=1.05
[3808 | 19902.04] loss=0.94 avg=1.05
[3809 | 19907.25] loss=0.84 avg=1.05
[3810 | 19912.41] loss=0.92 avg=1.05
[3811 | 19917.68] loss=1.88 avg=1.05
[3812 | 19922.87] loss=0.78 avg=1.05
[3813 | 19927.98] loss=1.09 avg=1.05
[3814 | 19933.13] loss=2.26 avg=1.06
[3815 | 19938.37] loss=0.77 avg=1.06
[3816 | 19943.56] loss=1.05 avg=1.06
[3817 | 19948.74] loss=0.97 avg=1.06
[3818 | 19953.95] loss=0.95 avg=1.06
[3819 | 19959.20] loss=1.59 avg=1.06
[3820 | 19964.32] loss=1.05 avg=1.06
[3821 | 19969.50] loss=1.36 avg=1.07
[3822 | 19974.76] loss=0.69 avg=1.06
[3823 | 19979.95] loss=0.71 avg=1.06
[3824 | 19985.10] loss=1.13 avg=1.06
[3825 | 19990.45] loss=0.98 avg=1.06
[3826 | 19995.68] loss=1.12 avg=1.06
[3827 | 20000.93] loss=0.99 avg=1.06
[3828 | 20006.11] loss=1.13 avg=1.06
[3829 | 20011.37] loss=0.74 avg=1.06
[3830 | 20016.46] loss=0.64 avg=1.05
[3831 | 20021.69] loss=0.85 avg=1.05
[3832 | 20026.83] loss=1.19 avg=1.05
[3833 | 20031.96] loss=0.66 avg=1.05
[3834 | 20037.20] loss=1.17 avg=1.05
[3835 | 20042.35] loss=1.02 avg=1.05
[3836 | 20047.54] loss=0.82 avg=1.05
[3837 | 20052.75] loss=1.18 avg=1.05
[3838 | 20058.06] loss=0.77 avg=1.04
[3839 | 20063.30] loss=1.06 avg=1.04
[3840 | 20068.42] loss=1.11 avg=1.05
[3841 | 20073.53] loss=1.50 avg=1.05
[3842 | 20078.70] loss=1.24 avg=1.05
[3843 | 20083.76] loss=0.88 avg=1.05
[3844 | 20088.89] loss=1.11 avg=1.05
[3845 | 20094.09] loss=1.23 avg=1.05
[3846 | 20099.45] loss=0.82 avg=1.05
[3847 | 20104.63] loss=1.03 avg=1.05
[3848 | 20109.81] loss=0.90 avg=1.05
[3849 | 20115.04] loss=0.76 avg=1.05
[3850 | 20120.19] loss=0.86 avg=1.04
[3851 | 20125.39] loss=1.55 avg=1.05
[3852 | 20130.64] loss=0.77 avg=1.05
[3853 | 20135.90] loss=1.06 avg=1.05
[3854 | 20141.11] loss=0.83 avg=1.04
[3855 | 20146.24] loss=0.92 avg=1.04
[3856 | 20151.34] loss=0.74 avg=1.04
[3857 | 20156.60] loss=0.86 avg=1.04
[3858 | 20161.80] loss=0.71 avg=1.04
[3859 | 20167.01] loss=0.82 avg=1.03
[3860 | 20172.34] loss=0.78 avg=1.03
[3861 | 20177.58] loss=1.54 avg=1.04
[3862 | 20182.79] loss=0.68 avg=1.03
[3863 | 20188.07] loss=1.32 avg=1.03
[3864 | 20193.36] loss=1.37 avg=1.04
[3865 | 20198.59] loss=1.42 avg=1.04
[3866 | 20203.81] loss=0.88 avg=1.04
[3867 | 20209.04] loss=1.13 avg=1.04
[3868 | 20214.21] loss=0.95 avg=1.04
[3869 | 20219.43] loss=1.07 avg=1.04
[3870 | 20224.62] loss=1.56 avg=1.05
[3871 | 20229.78] loss=1.06 avg=1.05
[3872 | 20234.94] loss=1.04 avg=1.05
[3873 | 20240.09] loss=0.96 avg=1.04
[3874 | 20245.30] loss=0.71 avg=1.04
[3875 | 20250.49] loss=0.69 avg=1.04
[3876 | 20255.63] loss=1.33 avg=1.04
[3877 | 20260.87] loss=0.86 avg=1.04
[3878 | 20266.13] loss=0.78 avg=1.04
[3879 | 20271.35] loss=1.02 avg=1.04
[3880 | 20276.51] loss=0.66 avg=1.03
[3881 | 20281.77] loss=0.79 avg=1.03
[3882 | 20286.94] loss=1.15 avg=1.03
[3883 | 20292.15] loss=1.11 avg=1.03
[3884 | 20297.30] loss=1.65 avg=1.04
[3885 | 20302.55] loss=1.61 avg=1.04
[3886 | 20307.79] loss=0.83 avg=1.04
[3887 | 20312.90] loss=1.05 avg=1.04
[3888 | 20318.08] loss=1.93 avg=1.05
[3889 | 20323.33] loss=1.02 avg=1.05
[3890 | 20328.56] loss=0.75 avg=1.05
[3891 | 20333.69] loss=0.90 avg=1.05
[3892 | 20338.96] loss=1.16 avg=1.05
[3893 | 20344.21] loss=1.12 avg=1.05
[3894 | 20349.45] loss=0.63 avg=1.04
[3895 | 20354.56] loss=1.37 avg=1.05
[3896 | 20359.68] loss=0.64 avg=1.04
[3897 | 20364.78] loss=0.76 avg=1.04
[3898 | 20369.96] loss=2.19 avg=1.05
[3899 | 20375.10] loss=1.03 avg=1.05
Generating samples...
======== SAMPLE 1 ========
elta_index', u'=', u'len', u'(', u'self', u'series', u')', u'items', u'=', u'None', u'self', u'series', u'=', u'self', u'.', u'troutes', u'[', u'x_index', u']', u'=', u'self', u'.', u'list', u'(', u'troutes', u')', u'try', u':', u'for', u'x_index', u',', u'self', u'series', u',', u'troutes', u']', u'self', u'.', u'get', u'(', u'x_index', u',', u'self']', u'=', u'self', u'.', u'more_list', u'(', u'troutes', u')', u'if', u'x_index', u'==', u"'end'", u':', u'for', u'v_index', u',', u'troutes', u',', u'v_index', u'=', u'self', u'.', u'more_list', u'(', u'self', u'.', u'more_list', u',', u'troutes', u')', u'ret', u'=', u'x_index', u'y_index', u'=', u'y_index', u'# if we have too many variables', u'y_lst_index', u'=', u'x_index', u'+', u'y_lst_index', u'self', u'.', u'more_list', u'(', u'self', u'.', u'more_list', u',', u'v_index', u',', u'self', u'.', u'more_list', u',', u'v_index', u',', u'self', u'.', u'more_list', u',', u'troutes', u')', u'self', u'.', u'more_list', u'(', u'x_index', u')', u'st', u'=', u'v_index', u'if', u'self', u'.', u'list_for', u'key', u'(', u'self', u'.', u'more_list', u')', u':', u'try', u':', u'for', u'key', u',', u'self', u'.', u'more_list', u',', u'self', u'.', u'more_list', u'(', u'self', u'.', u'more_list', u',', u'troutes', u',', u'v_index', u',', u'self', u'.', u'more_list', u',', u'v_index', u',', u'self', u'.', u'more_list', u',', u'troutes', u')', u'except', u':', u'ret', u'=', u''v_index', u'self', u'.', u'more_list', u'(', u'self', u'.', u'more_list', u',', u"''", u')', u'self', u'.', u'more_list', u'(', u'self', u'.', u'more_list', u',', u'q_index', u',', u'self', u'.', u'more_list', u',', u'self', u'.', u'more_list', u',', u'lst_index', u',', u'self', u'.', u'more_list', u',', u'hint_index', u',', u'self', u'.', u'more_list', u',', u'w_index', u',', u'self', u'.', u'more_list', u',', u'self', u'.', u'more_list', u',', u'self', u'.', u'more_list', u']', u'self', u'.', u'list_for', u'key', u'(', u'self', u'.', u'more_list', u')', u'=', u'self', u'.', u'troutes', u'[', u'x_index',

[3900 | 20461.28] loss=1.38 avg=1.05
[3901 | 20466.55] loss=0.63 avg=1.05
[3902 | 20471.86] loss=1.25 avg=1.05
[3903 | 20477.25] loss=1.34 avg=1.06
[3904 | 20482.59] loss=1.85 avg=1.06
[3905 | 20487.84] loss=0.52 avg=1.06
[3906 | 20493.02] loss=0.75 avg=1.05
[3907 | 20498.23] loss=0.38 avg=1.05
[3908 | 20503.46] loss=0.76 avg=1.05
[3909 | 20508.79] loss=0.98 avg=1.04
[3910 | 20513.97] loss=1.47 avg=1.05
[3911 | 20519.23] loss=1.13 avg=1.05
[3912 | 20524.49] loss=0.63 avg=1.05
[3913 | 20529.85] loss=1.24 avg=1.05
[3914 | 20535.00] loss=0.61 avg=1.04
[3915 | 20540.45] loss=1.70 avg=1.05
[3916 | 20545.72] loss=1.45 avg=1.05
[3917 | 20550.96] loss=0.90 avg=1.05
[3918 | 20556.26] loss=0.77 avg=1.05
[3919 | 20561.46] loss=1.34 avg=1.05
[3920 | 20566.67] loss=0.97 avg=1.05
[3921 | 20571.91] loss=1.03 avg=1.05
[3922 | 20577.14] loss=0.86 avg=1.05
[3923 | 20582.45] loss=0.72 avg=1.05
[3924 | 20587.74] loss=0.83 avg=1.04
[3925 | 20592.96] loss=0.78 avg=1.04
[3926 | 20598.31] loss=0.79 avg=1.04
[3927 | 20603.69] loss=0.98 avg=1.04
[3928 | 20608.96] loss=0.63 avg=1.03
[3929 | 20614.26] loss=1.36 avg=1.04
[3930 | 20619.49] loss=1.51 avg=1.04
[3931 | 20624.74] loss=1.33 avg=1.04
[3932 | 20630.02] loss=1.04 avg=1.04
[3933 | 20635.36] loss=0.95 avg=1.04
[3934 | 20640.62] loss=1.02 avg=1.04
[3935 | 20645.78] loss=1.04 avg=1.04
[3936 | 20651.04] loss=0.63 avg=1.04
[3937 | 20656.34] loss=0.74 avg=1.04
[3938 | 20661.69] loss=0.69 avg=1.03
[3939 | 20666.94] loss=0.76 avg=1.03
[3940 | 20672.27] loss=1.12 avg=1.03
[3941 | 20677.49] loss=1.61 avg=1.04
[3942 | 20682.79] loss=1.04 avg=1.04
[3943 | 20688.06] loss=0.83 avg=1.03
[3944 | 20693.30] loss=1.30 avg=1.04
[3945 | 20698.54] loss=0.80 avg=1.04
[3946 | 20703.70] loss=0.93 avg=1.03
[3947 | 20708.91] loss=1.06 avg=1.03
[3948 | 20714.14] loss=0.84 avg=1.03
[3949 | 20719.31] loss=1.01 avg=1.03
[3950 | 20724.54] loss=1.10 avg=1.03
[3951 | 20729.78] loss=0.87 avg=1.03
[3952 | 20735.10] loss=1.91 avg=1.04
[3953 | 20740.28] loss=1.13 avg=1.04
[3954 | 20745.40] loss=0.56 avg=1.04
[3955 | 20750.64] loss=0.68 avg=1.03
[3956 | 20755.94] loss=0.72 avg=1.03
[3957 | 20761.10] loss=1.73 avg=1.04
[3958 | 20766.40] loss=0.79 avg=1.03
[3959 | 20771.67] loss=0.72 avg=1.03
[3960 | 20776.91] loss=0.97 avg=1.03
[3961 | 20782.13] loss=0.91 avg=1.03
[3962 | 20787.46] loss=0.85 avg=1.03
[3963 | 20792.67] loss=0.44 avg=1.02
[3964 | 20797.80] loss=0.87 avg=1.02
[3965 | 20803.01] loss=0.88 avg=1.02
[3966 | 20808.24] loss=1.38 avg=1.02
[3967 | 20813.52] loss=1.49 avg=1.03
[3968 | 20818.74] loss=0.95 avg=1.03
[3969 | 20823.96] loss=0.51 avg=1.02
[3970 | 20829.20] loss=1.59 avg=1.03
[3971 | 20834.30] loss=0.76 avg=1.02
[3972 | 20839.61] loss=0.80 avg=1.02
[3973 | 20844.84] loss=1.30 avg=1.02
[3974 | 20849.96] loss=1.50 avg=1.03
[3975 | 20855.20] loss=0.67 avg=1.03
[3976 | 20860.39] loss=1.01 avg=1.03
[3977 | 20865.63] loss=1.02 avg=1.03
[3978 | 20870.86] loss=0.82 avg=1.02
[3979 | 20876.12] loss=0.36 avg=1.02
[3980 | 20881.37] loss=1.08 avg=1.02
[3981 | 20886.56] loss=1.04 avg=1.02
[3982 | 20891.86] loss=1.35 avg=1.02
[3983 | 20897.24] loss=0.94 avg=1.02
[3984 | 20902.52] loss=1.60 avg=1.03
[3985 | 20907.77] loss=1.86 avg=1.03
[3986 | 20913.10] loss=0.78 avg=1.03
[3987 | 20918.44] loss=1.22 avg=1.03
[3988 | 20923.75] loss=1.47 avg=1.04
[3989 | 20929.05] loss=0.73 avg=1.03
[3990 | 20934.40] loss=0.83 avg=1.03
[3991 | 20939.69] loss=0.76 avg=1.03
[3992 | 20944.89] loss=0.97 avg=1.03
[3993 | 20950.13] loss=0.89 avg=1.03
[3994 | 20955.41] loss=1.00 avg=1.03
[3995 | 20960.66] loss=0.81 avg=1.03
[3996 | 20965.95] loss=0.70 avg=1.02
[3997 | 20971.16] loss=0.93 avg=1.02
[3998 | 20976.36] loss=0.72 avg=1.02
[3999 | 20981.69] loss=0.57 avg=1.01
Saving checkpoint/run1/model-4000
Generating samples...
======== SAMPLE 1 ========
       self.add_element(
                                 )
        else:
             self.add_element(
                                dg.get_dict(self.dict_str).append(item)
        self.add_element(
                                  )
        self.remove_element(
                                 dg.get_dict(self.dict_str).append(item)

        self.add_element(
                                 dg.get_element_str).append(item)
        ]

    # Get the
    # content of the item self._item._id [u'def', u'item', u'(', u'self', u',', u'retriniter', u'=', u'True', u')', u':', u'self', u'.', u'settings', u'=', u'None', u'self', u'.', u'make_store_content_and_store_element_and_store_element_and_store_element', u'=', u'direct', u'(', u"'items'", u',', u'self', u'.', u'settings', u')', u'ret', u'.', u'append', u'(', u'item', u')', u'self', u'.', u'add_item_object', u'(', u'self', u'.', u'name', u',', u'"a-v", u')', u'id', u'=', u'id', u'ret', u'.', u'append', u'(', u'id', u')', u'ret', u'.', u'remove', u'(', u')', u'self', u'.', u'remove_element', u'(', u'id', u')', u'self', u'.', u'remove_element', u'(', u'id', u')'] Adds a store object to a dict. This will only
    # provide the store element or dict in the object.
    #    :param self.store_object: _store_object
              The store object to make available.
             Returns True if store_object is a dict and 'value'
              is specified as the store element or function in the
    # `dict`.

               #                    self._store_object: dict
                                                                             self, name=id, dg=dg)
              #                                  self._store_object_id: dict
                                                  self._store_element_and_store_element_and_store_element,           

[4000 | 21073.95] loss=0.60 avg=1.01
[4001 | 21079.08] loss=0.81 avg=1.01
[4002 | 21084.23] loss=1.14 avg=1.01
[4003 | 21089.50] loss=1.27 avg=1.01
[4004 | 21094.68] loss=1.30 avg=1.01
[4005 | 21099.98] loss=0.70 avg=1.01
[4006 | 21105.17] loss=0.91 avg=1.01
[4007 | 21110.39] loss=1.11 avg=1.01
[4008 | 21115.53] loss=0.70 avg=1.01
[4009 | 21120.65] loss=0.77 avg=1.01
[4010 | 21125.81] loss=1.25 avg=1.01
[4011 | 21130.95] loss=1.99 avg=1.02
[4012 | 21136.05] loss=2.28 avg=1.03
[4013 | 21141.17] loss=1.47 avg=1.04
[4014 | 21146.34] loss=1.07 avg=1.04
[4015 | 21151.44] loss=0.73 avg=1.03
[4016 | 21156.61] loss=1.28 avg=1.04
[4017 | 21161.71] loss=0.87 avg=1.03
[4018 | 21166.96] loss=0.98 avg=1.03
[4019 | 21172.14] loss=1.14 avg=1.03
[4020 | 21177.36] loss=0.83 avg=1.03
[4021 | 21182.48] loss=1.47 avg=1.04
[4022 | 21187.64] loss=0.63 avg=1.03
[4023 | 21192.93] loss=0.62 avg=1.03
[4024 | 21198.08] loss=1.33 avg=1.03
[4025 | 21203.28] loss=1.22 avg=1.03
[4026 | 21208.56] loss=1.15 avg=1.03
[4027 | 21213.74] loss=0.98 avg=1.03
[4028 | 21218.95] loss=1.26 avg=1.04
[4029 | 21224.13] loss=1.24 avg=1.04
[4030 | 21229.33] loss=1.49 avg=1.04
[4031 | 21234.50] loss=0.71 avg=1.04
[4032 | 21239.62] loss=0.74 avg=1.04
[4033 | 21244.90] loss=0.58 avg=1.03
[4034 | 21250.13] loss=0.97 avg=1.03
[4035 | 21255.36] loss=0.79 avg=1.03
[4036 | 21260.53] loss=0.59 avg=1.02
[4037 | 21265.80] loss=0.79 avg=1.02
[4038 | 21270.92] loss=0.98 avg=1.02
[4039 | 21276.10] loss=0.89 avg=1.02
[4040 | 21281.28] loss=1.12 avg=1.02
[4041 | 21286.43] loss=1.14 avg=1.02
[4042 | 21291.68] loss=0.59 avg=1.02
[4043 | 21296.84] loss=0.80 avg=1.02
[4044 | 21301.94] loss=0.73 avg=1.01
[4045 | 21307.15] loss=0.95 avg=1.01
[4046 | 21312.30] loss=1.15 avg=1.01
[4047 | 21317.51] loss=1.20 avg=1.02
[4048 | 21322.69] loss=0.69 avg=1.01
[4049 | 21327.84] loss=1.34 avg=1.02
[4050 | 21333.12] loss=1.06 avg=1.02
[4051 | 21338.29] loss=0.93 avg=1.02
[4052 | 21343.48] loss=1.14 avg=1.02
[4053 | 21348.68] loss=0.88 avg=1.02
[4054 | 21353.97] loss=1.18 avg=1.02
[4055 | 21359.10] loss=0.95 avg=1.02
[4056 | 21364.19] loss=0.80 avg=1.01
[4057 | 21369.34] loss=0.68 avg=1.01
[4058 | 21374.51] loss=0.75 avg=1.01
[4059 | 21379.72] loss=1.03 avg=1.01
[4060 | 21384.96] loss=0.83 avg=1.01
[4061 | 21390.13] loss=1.30 avg=1.01
[4062 | 21395.27] loss=1.57 avg=1.02
[4063 | 21400.40] loss=1.02 avg=1.02
[4064 | 21405.55] loss=0.68 avg=1.01
[4065 | 21410.79] loss=0.81 avg=1.01
[4066 | 21416.19] loss=0.97 avg=1.01
[4067 | 21421.47] loss=1.27 avg=1.01
[4068 | 21426.76] loss=1.25 avg=1.01
[4069 | 21431.92] loss=1.79 avg=1.02
[4070 | 21437.04] loss=0.72 avg=1.02
[4071 | 21442.27] loss=0.57 avg=1.01
[4072 | 21447.43] loss=0.78 avg=1.01
[4073 | 21452.55] loss=1.09 avg=1.01
[4074 | 21457.72] loss=0.63 avg=1.01
[4075 | 21462.90] loss=1.11 avg=1.01
[4076 | 21468.03] loss=1.52 avg=1.02
[4077 | 21473.18] loss=0.74 avg=1.01
[4078 | 21478.37] loss=1.02 avg=1.01
[4079 | 21483.49] loss=1.42 avg=1.02
[4080 | 21488.70] loss=1.30 avg=1.02
[4081 | 21493.85] loss=0.78 avg=1.02
[4082 | 21499.01] loss=0.99 avg=1.02
[4083 | 21504.15] loss=1.38 avg=1.02
[4084 | 21509.42] loss=0.72 avg=1.02
[4085 | 21514.55] loss=0.77 avg=1.02
[4086 | 21519.66] loss=0.82 avg=1.01
[4087 | 21524.94] loss=0.99 avg=1.01
[4088 | 21530.24] loss=0.71 avg=1.01
[4089 | 21535.40] loss=0.56 avg=1.01
[4090 | 21540.47] loss=1.31 avg=1.01
[4091 | 21545.75] loss=1.07 avg=1.01
[4092 | 21550.98] loss=1.30 avg=1.01
[4093 | 21556.17] loss=0.79 avg=1.01
[4094 | 21561.26] loss=1.13 avg=1.01
[4095 | 21566.52] loss=1.05 avg=1.01
[4096 | 21571.70] loss=1.12 avg=1.01
[4097 | 21576.87] loss=0.61 avg=1.01
[4098 | 21582.09] loss=1.26 avg=1.01
[4099 | 21587.31] loss=1.05 avg=1.01
Generating samples...
======== SAMPLE 1 ========
 available for an object-safe method.

    :param pd:
    :cls:
    :str: A list of names suitable for creating a model file.

    :raises:
    :cg:
    :type: list

    :param pd:
    :type: [string]

    :param name:
    :type: string

    :param pd:
    :type: list

    :return: A data object

    Returns the pd as a list of the names of the model names.

    If the model has only one name
   """

    s = model.read()
    models = list(s)

    model_id = nameset.keys()
    pd = pd.PdName() for pd in models [u'add', u'a', u'distribution', u'of', u'source', u',', u'samples'] AddModel.objects_by_value python def objects_by_value(name, pd):
    """add a distribution of source samples of content
    .

    Args:
        name: str (for example, "Pd Name" or something similar).

       
       
        
        This method generates a dictionary containing a list of names suitable for creating a model file.

    :param name:
    :type: str
    :param pd:
    :type: list

    :param name:
    :type: string

     :return: A data object

    """

    if name == 'pds' and pd.part_name != name:
        return model_id, pd.data

    if pd.part_name is None:
        return model_id

    if pd.part_name is None:
        return model_id

    if pd.part_name is not None:
        return model_id

    assert name is name

    if name is not None:
        return model_id

    assert pd.part_name if name.lower() in models else None

    # pd.part_name is the first argument of name
    # except from the above
    # pd.part_name is the first argument of name
    if name is None:
        return model_id

    if not pd.part_part_name:
        return model_id

    return pd, name

   else:
        return None train pd.models/trender.py kmcc/pd-python f8bd77a1bff2baeb5a9bef0e0b3f0a8b4e0d2c https://github.com/kmcc/pd-python/blob/fd8bd77a1bff2baeb5a9bef0e0b3f0a8b4e0d2c/pd.models/trender.py#L2221-L2308
def to_trending_models(self):
    """ to_trending_models(n,s):
    """
    Pd for Model is created if name given (i.e._TrendingModel, i.e.__TrendingModel) == _TrendingModel(self):
    if s._trending_models() == _TrendingModel(self):
        return Pd(name)
        else:
            self._trending_models() == _TrendingModel
        raise ValueError("to_trending_models has to-do: "
            "p:\\n".format(

[4100 | 21676.33] loss=0.60 avg=1.01
[4101 | 21681.53] loss=1.17 avg=1.01
[4102 | 21686.81] loss=1.46 avg=1.01
[4103 | 21691.92] loss=0.95 avg=1.01
[4104 | 21697.23] loss=1.32 avg=1.02
[4105 | 21702.67] loss=0.71 avg=1.01
[4106 | 21708.00] loss=1.41 avg=1.02
[4107 | 21713.38] loss=1.19 avg=1.02
[4108 | 21718.87] loss=0.82 avg=1.02
[4109 | 21724.10] loss=0.94 avg=1.02
[4110 | 21729.41] loss=1.39 avg=1.02
[4111 | 21734.69] loss=0.84 avg=1.02
[4112 | 21739.93] loss=0.87 avg=1.02
[4113 | 21745.18] loss=1.02 avg=1.02
[4114 | 21750.46] loss=0.75 avg=1.01
[4115 | 21755.80] loss=1.17 avg=1.02
[4116 | 21761.08] loss=1.33 avg=1.02
[4117 | 21766.33] loss=1.04 avg=1.02
[4118 | 21771.62] loss=1.23 avg=1.02
[4119 | 21776.87] loss=0.81 avg=1.02
[4120 | 21782.10] loss=0.81 avg=1.02
[4121 | 21787.27] loss=0.89 avg=1.02
[4122 | 21792.49] loss=0.98 avg=1.01
[4123 | 21797.72] loss=0.82 avg=1.01
[4124 | 21802.89] loss=1.12 avg=1.01
[4125 | 21808.21] loss=1.22 avg=1.02
[4126 | 21813.40] loss=0.88 avg=1.01
[4127 | 21818.70] loss=1.13 avg=1.02
[4128 | 21823.95] loss=2.00 avg=1.03
[4129 | 21829.23] loss=0.85 avg=1.02
[4130 | 21834.64] loss=0.79 avg=1.02
[4131 | 21839.97] loss=1.38 avg=1.03
[4132 | 21845.31] loss=0.93 avg=1.02
[4133 | 21850.53] loss=0.72 avg=1.02
[4134 | 21855.86] loss=1.33 avg=1.02
[4135 | 21861.19] loss=2.14 avg=1.04
[4136 | 21866.47] loss=0.94 avg=1.03
[4137 | 21871.91] loss=0.35 avg=1.03
[4138 | 21877.14] loss=1.22 avg=1.03
[4139 | 21882.30] loss=0.93 avg=1.03
[4140 | 21887.55] loss=0.74 avg=1.03
[4141 | 21892.95] loss=0.90 avg=1.02
[4142 | 21898.21] loss=0.58 avg=1.02
[4143 | 21903.38] loss=1.38 avg=1.02
[4144 | 21908.61] loss=0.93 avg=1.02
[4145 | 21913.85] loss=0.93 avg=1.02
[4146 | 21919.00] loss=0.60 avg=1.02
[4147 | 21924.22] loss=1.16 avg=1.02
[4148 | 21929.56] loss=0.82 avg=1.02
[4149 | 21934.88] loss=0.75 avg=1.01
[4150 | 21940.21] loss=1.25 avg=1.02
[4151 | 21945.49] loss=0.73 avg=1.01
[4152 | 21950.72] loss=0.80 avg=1.01
[4153 | 21956.06] loss=0.60 avg=1.01
[4154 | 21961.29] loss=1.64 avg=1.01
[4155 | 21966.61] loss=0.99 avg=1.01
[4156 | 21971.90] loss=0.84 avg=1.01
[4157 | 21977.21] loss=1.06 avg=1.01
[4158 | 21982.64] loss=1.60 avg=1.02
[4159 | 21987.98] loss=1.12 avg=1.02
[4160 | 21993.27] loss=0.83 avg=1.02
[4161 | 21998.54] loss=0.58 avg=1.01
[4162 | 22003.89] loss=0.86 avg=1.01
[4163 | 22009.18] loss=1.50 avg=1.02
[4164 | 22014.52] loss=0.89 avg=1.01
[4165 | 22019.77] loss=1.00 avg=1.01
[4166 | 22025.01] loss=1.13 avg=1.02
[4167 | 22030.23] loss=1.19 avg=1.02
[4168 | 22035.39] loss=0.66 avg=1.01
[4169 | 22040.60] loss=0.87 avg=1.01
[4170 | 22045.90] loss=1.56 avg=1.02
[4171 | 22051.09] loss=1.31 avg=1.02
[4172 | 22056.33] loss=0.99 avg=1.02
[4173 | 22061.63] loss=0.85 avg=1.02
[4174 | 22066.87] loss=1.39 avg=1.02
[4175 | 22072.21] loss=0.63 avg=1.02
[4176 | 22077.41] loss=1.36 avg=1.02
[4177 | 22082.65] loss=1.13 avg=1.02
[4178 | 22087.87] loss=1.04 avg=1.02
[4179 | 22093.16] loss=0.98 avg=1.02
[4180 | 22098.40] loss=0.90 avg=1.02
[4181 | 22103.75] loss=1.01 avg=1.02
[4182 | 22109.08] loss=0.95 avg=1.02
[4183 | 22114.42] loss=0.68 avg=1.02
[4184 | 22119.70] loss=0.62 avg=1.01
[4185 | 22125.02] loss=0.65 avg=1.01
[4186 | 22130.27] loss=0.70 avg=1.01
[4187 | 22135.51] loss=0.92 avg=1.01
[4188 | 22140.65] loss=0.86 avg=1.00
[4189 | 22145.88] loss=0.56 avg=1.00
[4190 | 22151.08] loss=0.59 avg=1.00
[4191 | 22156.18] loss=0.60 avg=0.99
[4192 | 22161.34] loss=0.88 avg=0.99
[4193 | 22166.61] loss=1.16 avg=0.99
[4194 | 22171.96] loss=1.12 avg=0.99
[4195 | 22177.14] loss=1.14 avg=1.00
[4196 | 22182.39] loss=1.00 avg=1.00
[4197 | 22187.54] loss=1.13 avg=1.00
[4198 | 22192.82] loss=1.08 avg=1.00
[4199 | 22198.21] loss=0.74 avg=1.00
Generating samples...
======== SAMPLE 1 ========
': u'#''
    logger.info('Logging on/off')
    # If we are sure to go ahead, we do it anyway'
    if err != 'failure':
         raise ValueError('Failure - logging on '
                                                                                                                                                                                                                                                                    #'' 
                           logger.info('Logging on/off')
        else:
               logger.info('Logging on')
        # If we have failed to log on already, log on again. #''
        if err != 'failure':
         raise ValueError('failure - logging on '
                                                                                                                      
                            logger.info('Logging on',
                                                                                                                                                                                                                                                                                                                                                                                                  key=None)
                   if len (

[4200 | 22286.03] loss=0.65 avg=0.99
[4201 | 22291.23] loss=1.06 avg=0.99
[4202 | 22296.43] loss=1.30 avg=1.00
[4203 | 22301.76] loss=0.73 avg=0.99
[4204 | 22307.07] loss=0.60 avg=0.99
[4205 | 22312.36] loss=1.25 avg=0.99
[4206 | 22317.66] loss=0.76 avg=0.99
[4207 | 22322.92] loss=0.54 avg=0.98
[4208 | 22328.17] loss=1.08 avg=0.99
[4209 | 22333.37] loss=1.02 avg=0.99
[4210 | 22338.64] loss=1.05 avg=0.99
[4211 | 22343.90] loss=0.70 avg=0.98
[4212 | 22349.14] loss=1.78 avg=0.99
[4213 | 22354.34] loss=0.78 avg=0.99
[4214 | 22359.69] loss=1.02 avg=0.99
[4215 | 22364.99] loss=0.47 avg=0.98
[4216 | 22370.24] loss=0.58 avg=0.98
[4217 | 22375.57] loss=1.04 avg=0.98
[4218 | 22380.78] loss=0.88 avg=0.98
[4219 | 22385.91] loss=0.94 avg=0.98
[4220 | 22391.12] loss=1.09 avg=0.98
[4221 | 22396.45] loss=1.50 avg=0.99
[4222 | 22401.65] loss=0.64 avg=0.98
[4223 | 22406.91] loss=1.40 avg=0.99
[4224 | 22412.11] loss=1.50 avg=0.99
[4225 | 22417.48] loss=0.92 avg=0.99
[4226 | 22422.65] loss=0.80 avg=0.99
[4227 | 22427.82] loss=0.73 avg=0.99
[4228 | 22432.96] loss=1.82 avg=0.99
[4229 | 22438.25] loss=1.46 avg=1.00
[4230 | 22443.54] loss=0.91 avg=1.00
[4231 | 22448.80] loss=1.33 avg=1.00
[4232 | 22453.93] loss=0.93 avg=1.00
[4233 | 22459.08] loss=0.58 avg=1.00
[4234 | 22464.32] loss=0.71 avg=0.99
[4235 | 22469.53] loss=0.88 avg=0.99
[4236 | 22474.70] loss=0.45 avg=0.99
[4237 | 22479.94] loss=0.88 avg=0.99
[4238 | 22485.14] loss=0.67 avg=0.98
[4239 | 22490.38] loss=0.89 avg=0.98
[4240 | 22495.53] loss=1.43 avg=0.99
[4241 | 22500.75] loss=0.88 avg=0.99
[4242 | 22506.01] loss=0.77 avg=0.98
[4243 | 22511.21] loss=0.75 avg=0.98
[4244 | 22516.38] loss=0.80 avg=0.98
[4245 | 22521.60] loss=0.74 avg=0.98
[4246 | 22526.78] loss=1.32 avg=0.98
[4247 | 22531.99] loss=1.75 avg=0.99
[4248 | 22537.19] loss=0.86 avg=0.99
[4249 | 22542.37] loss=1.63 avg=0.99
[4250 | 22547.60] loss=1.26 avg=1.00
[4251 | 22552.77] loss=0.94 avg=1.00
[4252 | 22558.07] loss=1.20 avg=1.00
[4253 | 22563.31] loss=0.81 avg=1.00
[4254 | 22568.53] loss=1.28 avg=1.00
[4255 | 22573.71] loss=1.19 avg=1.00
[4256 | 22578.96] loss=0.69 avg=1.00
[4257 | 22584.13] loss=0.92 avg=1.00
[4258 | 22589.29] loss=0.49 avg=0.99
[4259 | 22594.55] loss=1.10 avg=0.99
[4260 | 22599.79] loss=1.51 avg=1.00
[4261 | 22604.99] loss=1.07 avg=1.00
[4262 | 22610.21] loss=0.69 avg=1.00
[4263 | 22615.49] loss=0.77 avg=0.99
[4264 | 22620.74] loss=1.38 avg=1.00
[4265 | 22625.91] loss=0.70 avg=0.99
[4266 | 22631.09] loss=0.89 avg=0.99
[4267 | 22636.30] loss=1.49 avg=1.00
[4268 | 22641.42] loss=0.75 avg=1.00
[4269 | 22646.68] loss=0.65 avg=0.99
[4270 | 22651.87] loss=0.70 avg=0.99
[4271 | 22657.08] loss=0.96 avg=0.99
[4272 | 22662.24] loss=0.77 avg=0.99
[4273 | 22667.37] loss=1.93 avg=1.00
[4274 | 22672.55] loss=1.85 avg=1.00
[4275 | 22677.72] loss=1.00 avg=1.00
[4276 | 22682.85] loss=1.15 avg=1.01
[4277 | 22688.06] loss=1.14 avg=1.01
[4278 | 22693.36] loss=1.06 avg=1.01
[4279 | 22698.70] loss=1.12 avg=1.01
[4280 | 22703.95] loss=0.67 avg=1.01
[4281 | 22709.25] loss=0.89 avg=1.00
[4282 | 22714.44] loss=1.28 avg=1.01
[4283 | 22719.69] loss=1.05 avg=1.01
[4284 | 22724.90] loss=0.96 avg=1.01
[4285 | 22730.14] loss=1.46 avg=1.01
[4286 | 22735.40] loss=1.43 avg=1.02
[4287 | 22740.59] loss=0.96 avg=1.02
[4288 | 22745.86] loss=1.03 avg=1.02
[4289 | 22751.11] loss=1.39 avg=1.02
[4290 | 22756.35] loss=0.89 avg=1.02
[4291 | 22761.51] loss=1.02 avg=1.02
[4292 | 22766.75] loss=0.88 avg=1.02
[4293 | 22771.99] loss=1.08 avg=1.02
[4294 | 22777.30] loss=0.62 avg=1.01
[4295 | 22782.48] loss=0.71 avg=1.01
[4296 | 22787.88] loss=1.69 avg=1.02
[4297 | 22793.22] loss=0.64 avg=1.01
[4298 | 22798.47] loss=0.71 avg=1.01
[4299 | 22803.71] loss=0.61 avg=1.01
Generating samples...
======== SAMPLE 1 ========
dos uid=uid(uid), nd=nd("uid")
        return (dtype=UUID, uid=uid, uid=uid, uid=uid, uid=uid.uid
        def create_nuid():
            if nd is not None:
                f, pg_ids = (uuid, uuid, uuid_dirs, uuid_dirs, uuid_uid_directory, uuid_dir)

           dtype_id = UUID(uid, uuid), nd = nd(dtype=dtype, nuid=dtype)
           uid_dir = (uuid, uuid_dir, uuid_dir)

           uids_file = f.read_uri().read(nid_name=uid_name, uids_file=uids_file)
           rd=fs.read_uri('file.r', rd.read, 0)
            rda = getattr(r, None, rd)
            rids_file.read(nid_name=rid_name)

        raise ValueError('nums: ' + nid_name + ' no one could write: ' +
                      ld/uuid_uid', unescape(nid_name)
        ) [u'def', u'create_nuid', u'(', u'n_uid', u')', u':', u'u_uid_path', u'(', u'n_uid', u')', u'=', u'sys', u'.', u'io', u'.', u'IO_HANDLE', u'(', u'sys', u'.', u'IO_RUN', u'(', u'0', u',', u'0', u')', u',', u'0', u',', u'1', u')', u'+', u'1', u',', u'2', u',', u'3', u'.', u'r', u'n_w', u'+=', u'1', u'w', u'or', u'de', u'not', u'(', u'(', u'w', u'=', u'1', u',', u'2', u',', u'3', u'.', u'r', u'+=', u'2', u',', u'2', u'.', u'r', u'+', u'3', u'.', u'nr', u'+', u'3', u'.', u'nr', u')', u'or', u'te', u'n', u'=', u'0', u',', u'n', u'.', u'def', u'_create_id', u'(', u'n_uid', u')', u'for', u'sys', u'.', u'io', u'.', u'IO_DELAY', u'in', u'range', u'(', u'len', u'(', u'u_uid_dir', u'+', u'sys', u"'w'", u'-', u'1', u')', u',', u'te', u'=', u'None', u')', u':', u'to_r', u'=', u'sys', u'.', u'getattr', u'(', u'r', u',', u'sys', u'.', u'getattr', u'(', u"r"', u'+', u'sys', u'.', u'getattr', u'(', u')', u',', u'te', u')', u'sys', u'.', u'getattr', u'(', u'r', u'+', u'sys', u'.', u'getattr', u'(', u')', u',', u'sys', u'.', u'getattr',

[4300 | 22893.78] loss=0.99 avg=1.01
[4301 | 22899.07] loss=0.66 avg=1.00
[4302 | 22904.41] loss=0.93 avg=1.00
[4303 | 22909.66] loss=1.83 avg=1.01
[4304 | 22914.97] loss=0.76 avg=1.01
[4305 | 22920.17] loss=1.16 avg=1.01
[4306 | 22925.41] loss=0.91 avg=1.01
[4307 | 22930.76] loss=0.80 avg=1.01
[4308 | 22936.08] loss=1.46 avg=1.01
[4309 | 22941.43] loss=0.85 avg=1.01
[4310 | 22946.72] loss=0.84 avg=1.01
[4311 | 22951.94] loss=0.46 avg=1.00
[4312 | 22957.23] loss=1.38 avg=1.01
[4313 | 22962.46] loss=0.91 avg=1.00
[4314 | 22967.73] loss=1.16 avg=1.01
[4315 | 22972.92] loss=0.63 avg=1.00
[4316 | 22978.12] loss=1.00 avg=1.00
[4317 | 22983.43] loss=1.23 avg=1.00
[4318 | 22988.76] loss=0.75 avg=1.00
[4319 | 22994.15] loss=1.12 avg=1.00
[4320 | 22999.48] loss=0.63 avg=1.00
[4321 | 23004.76] loss=1.06 avg=1.00
[4322 | 23010.05] loss=1.03 avg=1.00
[4323 | 23015.28] loss=1.45 avg=1.01
[4324 | 23020.53] loss=1.81 avg=1.01
[4325 | 23025.81] loss=0.90 avg=1.01
[4326 | 23030.95] loss=0.87 avg=1.01
[4327 | 23036.25] loss=0.77 avg=1.01
[4328 | 23041.49] loss=1.22 avg=1.01
[4329 | 23046.75] loss=1.41 avg=1.01
[4330 | 23051.95] loss=1.13 avg=1.02
[4331 | 23057.18] loss=0.75 avg=1.01
[4332 | 23062.47] loss=1.26 avg=1.02
[4333 | 23067.79] loss=1.10 avg=1.02
[4334 | 23073.01] loss=1.07 avg=1.02
[4335 | 23078.15] loss=0.93 avg=1.02
[4336 | 23083.44] loss=1.74 avg=1.02
[4337 | 23088.65] loss=0.73 avg=1.02
[4338 | 23093.97] loss=0.64 avg=1.02
[4339 | 23099.23] loss=0.92 avg=1.02
[4340 | 23104.43] loss=0.72 avg=1.01
[4341 | 23109.64] loss=0.75 avg=1.01
[4342 | 23114.98] loss=0.70 avg=1.01
[4343 | 23120.31] loss=0.78 avg=1.00
[4344 | 23125.74] loss=1.21 avg=1.01
[4345 | 23130.98] loss=0.80 avg=1.00
[4346 | 23136.27] loss=1.00 avg=1.00
[4347 | 23141.67] loss=1.18 avg=1.01
[4348 | 23146.96] loss=0.95 avg=1.01
[4349 | 23152.24] loss=0.49 avg=1.00
[4350 | 23157.50] loss=1.00 avg=1.00
[4351 | 23162.69] loss=1.38 avg=1.00
[4352 | 23167.99] loss=1.29 avg=1.01
[4353 | 23173.28] loss=0.76 avg=1.00
[4354 | 23178.53] loss=0.70 avg=1.00
[4355 | 23183.77] loss=1.19 avg=1.00
[4356 | 23188.99] loss=1.14 avg=1.00
[4357 | 23194.22] loss=1.20 avg=1.01
[4358 | 23199.64] loss=0.83 avg=1.01
[4359 | 23204.83] loss=1.25 avg=1.01
[4360 | 23210.15] loss=0.94 avg=1.01
[4361 | 23215.44] loss=1.02 avg=1.01
[4362 | 23220.86] loss=1.16 avg=1.01
[4363 | 23226.09] loss=0.58 avg=1.00
[4364 | 23231.37] loss=1.40 avg=1.01
[4365 | 23236.65] loss=2.23 avg=1.02
[4366 | 23242.01] loss=0.87 avg=1.02
[4367 | 23247.40] loss=1.48 avg=1.02
[4368 | 23252.76] loss=0.75 avg=1.02
[4369 | 23257.99] loss=0.91 avg=1.02
[4370 | 23263.25] loss=0.23 avg=1.01
[4371 | 23268.55] loss=1.41 avg=1.02
[4372 | 23273.92] loss=0.93 avg=1.01
[4373 | 23279.18] loss=0.88 avg=1.01
[4374 | 23284.44] loss=0.89 avg=1.01
[4375 | 23289.95] loss=1.54 avg=1.02
[4376 | 23295.35] loss=1.23 avg=1.02
[4377 | 23300.65] loss=0.93 avg=1.02
[4378 | 23305.83] loss=0.72 avg=1.02
[4379 | 23311.04] loss=1.12 avg=1.02
[4380 | 23316.36] loss=1.25 avg=1.02
[4381 | 23321.56] loss=0.70 avg=1.02
[4382 | 23326.79] loss=1.20 avg=1.02
[4383 | 23332.04] loss=1.40 avg=1.02
[4384 | 23337.40] loss=1.36 avg=1.02
[4385 | 23342.68] loss=0.52 avg=1.02
[4386 | 23348.07] loss=0.85 avg=1.02
[4387 | 23353.33] loss=1.70 avg=1.03
[4388 | 23358.57] loss=1.19 avg=1.03
[4389 | 23363.79] loss=2.06 avg=1.04
[4390 | 23369.10] loss=0.69 avg=1.03
[4391 | 23374.20] loss=1.69 avg=1.04
[4392 | 23379.47] loss=0.88 avg=1.04
[4393 | 23384.72] loss=0.90 avg=1.04
[4394 | 23390.01] loss=0.82 avg=1.04
[4395 | 23395.15] loss=0.63 avg=1.03
[4396 | 23400.31] loss=1.59 avg=1.04
[4397 | 23405.55] loss=0.68 avg=1.03
[4398 | 23410.79] loss=0.63 avg=1.03
[4399 | 23416.03] loss=1.57 avg=1.03
Generating samples...
======== SAMPLE 1 ========
deg(log2(log1))
            log2(log2(log1))

         for n in range(0, (log2(log1))):
            log2(log2(log1))

            del log2(-logs(log1, log2(log2)).append((log2(log1, log2)))

            log2(log2(log1))

            logger_id = False

            logger = logger.get_logger(
                self.logging, [
                             self.logging.versionInfo("logged-in"))

            logger.update(getattr(log1, lambda val_state: val_state["val_state"]),
                           val_state["val_state"])

            logger.update(getattr(log1, lambda val_state: val_state["val_state"]),
                          val_state["val_state"])

            if val_id is not None:
                 logger_id = True

             elif val_id < val_id:
                 # Log a log error in this case
                 logging.print(log1, val_id)

             log_id = False

             logger.update_log_id(getattr(log1, lambda val_id: val_id["val_id"], log_id))

            logger.update_log_id(getattr(log2, lambda val_id: val_id["val_id"], log_id))

            logger.update_log_id(getattr(log2, lambda val_id: val_id["val_id"], log_id))

            logger.upvote_log_id(getattr(log2, lambda val_id: val_id["val_id"], log_id))

            logger.update_log_id(getattr(log2, lambda val_id: val_id["val_id"], log_id))

            logger.upvote_log_id(getattr(log2, lambda val_id: val_id["val_id"], log_id))

            logger.upvote_log_id(getattr(log2, lambda val_id: val_id["val_id"], log_id))

            logger.upvote_log_id(getattr(log2, lambda val_id: log_id["log_id"])
            logger.update_log_id(getattr(log2, lambda val_id: log_id["log_id"], log_id))

            logger.update_log_id(getattr(log2, lambda val_id: {
                      fid: val_id["fid"]},
            log_id: {
                   fid: log_id("fid")

[4400 | 23504.77] loss=1.10 avg=1.03
[4401 | 23509.99] loss=1.33 avg=1.04
[4402 | 23515.17] loss=0.88 avg=1.04
[4403 | 23520.28] loss=1.39 avg=1.04
[4404 | 23525.41] loss=1.15 avg=1.04
[4405 | 23530.50] loss=0.32 avg=1.03
[4406 | 23535.65] loss=0.95 avg=1.03
[4407 | 23540.80] loss=1.11 avg=1.03
[4408 | 23545.93] loss=0.64 avg=1.03
[4409 | 23551.09] loss=0.65 avg=1.03
[4410 | 23556.27] loss=0.79 avg=1.02
[4411 | 23561.46] loss=0.86 avg=1.02
[4412 | 23566.75] loss=0.35 avg=1.02
[4413 | 23572.04] loss=1.15 avg=1.02
[4414 | 23577.22] loss=0.87 avg=1.02
[4415 | 23582.61] loss=0.95 avg=1.01
[4416 | 23587.89] loss=0.96 avg=1.01
[4417 | 23593.19] loss=0.77 avg=1.01
[4418 | 23598.49] loss=1.06 avg=1.01
[4419 | 23603.68] loss=1.12 avg=1.01
[4420 | 23608.93] loss=1.07 avg=1.01
[4421 | 23614.13] loss=0.78 avg=1.01
[4422 | 23619.46] loss=2.55 avg=1.03
[4423 | 23624.73] loss=1.27 avg=1.03
[4424 | 23629.94] loss=1.32 avg=1.03
[4425 | 23635.27] loss=1.16 avg=1.03
[4426 | 23640.59] loss=0.57 avg=1.03
[4427 | 23645.90] loss=1.35 avg=1.03
[4428 | 23651.27] loss=0.88 avg=1.03
[4429 | 23656.50] loss=1.14 avg=1.03
[4430 | 23661.82] loss=1.44 avg=1.04
[4431 | 23667.23] loss=1.25 avg=1.04
[4432 | 23672.45] loss=1.12 avg=1.04
[4433 | 23677.63] loss=0.91 avg=1.04
[4434 | 23682.95] loss=0.61 avg=1.03
[4435 | 23688.20] loss=1.04 avg=1.03
[4436 | 23693.50] loss=1.23 avg=1.03
[4437 | 23698.81] loss=0.60 avg=1.03
[4438 | 23704.07] loss=0.73 avg=1.03
[4439 | 23709.40] loss=0.80 avg=1.03
[4440 | 23714.69] loss=1.10 avg=1.03
[4441 | 23719.93] loss=0.86 avg=1.02
[4442 | 23725.23] loss=1.98 avg=1.03
[4443 | 23730.46] loss=0.50 avg=1.03
[4444 | 23735.68] loss=1.01 avg=1.03
[4445 | 23740.88] loss=1.52 avg=1.03
[4446 | 23746.27] loss=0.81 avg=1.03
[4447 | 23751.56] loss=0.65 avg=1.03
[4448 | 23756.91] loss=0.79 avg=1.02
[4449 | 23762.13] loss=1.03 avg=1.02
[4450 | 23767.39] loss=0.87 avg=1.02
[4451 | 23772.73] loss=0.80 avg=1.02
[4452 | 23777.99] loss=1.45 avg=1.03
[4453 | 23783.37] loss=1.00 avg=1.03
[4454 | 23788.66] loss=1.03 avg=1.03
[4455 | 23793.96] loss=0.77 avg=1.02
[4456 | 23799.16] loss=0.91 avg=1.02
[4457 | 23804.46] loss=0.90 avg=1.02
[4458 | 23809.73] loss=0.81 avg=1.02
[4459 | 23815.16] loss=1.75 avg=1.03
[4460 | 23820.50] loss=0.60 avg=1.02
[4461 | 23825.88] loss=0.57 avg=1.02
[4462 | 23831.09] loss=1.03 avg=1.02
[4463 | 23836.24] loss=0.75 avg=1.01
[4464 | 23841.39] loss=1.27 avg=1.02
[4465 | 23846.64] loss=1.00 avg=1.02
[4466 | 23851.75] loss=0.84 avg=1.01
[4467 | 23856.94] loss=0.61 avg=1.01
[4468 | 23862.14] loss=1.31 avg=1.01
[4469 | 23867.32] loss=1.39 avg=1.02
[4470 | 23872.52] loss=1.34 avg=1.02
[4471 | 23877.75] loss=0.99 avg=1.02
[4472 | 23882.93] loss=1.68 avg=1.03
[4473 | 23888.15] loss=1.11 avg=1.03
[4474 | 23893.32] loss=1.22 avg=1.03
[4475 | 23898.58] loss=0.65 avg=1.03
[4476 | 23903.72] loss=1.27 avg=1.03
[4477 | 23908.90] loss=0.91 avg=1.03
[4478 | 23914.09] loss=0.98 avg=1.03
[4479 | 23919.34] loss=1.01 avg=1.03
[4480 | 23924.54] loss=0.89 avg=1.03
[4481 | 23929.73] loss=1.12 avg=1.03
[4482 | 23934.84] loss=0.89 avg=1.02
[4483 | 23940.06] loss=2.00 avg=1.03
[4484 | 23945.33] loss=0.84 avg=1.03
[4485 | 23950.62] loss=1.01 avg=1.03
[4486 | 23955.74] loss=0.60 avg=1.03
[4487 | 23961.07] loss=1.49 avg=1.03
[4488 | 23966.35] loss=1.01 avg=1.03
[4489 | 23971.54] loss=1.38 avg=1.04
[4490 | 23976.74] loss=1.08 avg=1.04
[4491 | 23981.92] loss=0.79 avg=1.03
[4492 | 23987.03] loss=1.16 avg=1.04
[4493 | 23992.13] loss=0.98 avg=1.03
[4494 | 23997.29] loss=2.20 avg=1.05
[4495 | 24002.44] loss=1.03 avg=1.05
[4496 | 24007.58] loss=1.21 avg=1.05
[4497 | 24012.72] loss=0.75 avg=1.04
[4498 | 24017.90] loss=0.86 avg=1.04
[4499 | 24023.11] loss=0.63 avg=1.04
Generating samples...
======== SAMPLE 1 ========
                     0.02                .. tmap_hinds + 1: 0.2                       .. tmap_hinds + 1: 0.5                       .. tmap_hinds + 1: 0.5                      .. tsmap + 1: 0.5                     .. tsmap + 1: 0.2                      .. tsmap + 0.2 : 0.2                    ...
        """
        if tsmap is None:
            tsmap = tslist
        if tsmap is not None:
            tmap = tslist[1]
        return tmap [u'def', u'_update_tmaps'', u'(', u'self', u')', u':', u'if', u'tpsmap', u'is', u'None', u':', u'tpsmap', u'=', u'tpslist', u'if', u'tpsmap', u'is', u'not', u'None', u':', u'tpsmap', u'=', u'tpslist', u'[', u'1', u']', u'return', u'tpsmap'] Returns an update of the maps to be updated

    Returns an update of the maps to be updated If tsmap is not None:
            tsmap = tslist
    Returns an update of the maps to be updated train tsmap/tmap_hinds/map_hinds.py gkc_cliff-cliff-c/tsmap 6077c4850f9a9c6d59068d6d7c5da1f5c12b https://github.com/kc_cliff-c/tsmap/blob/6077c4850f9a9c6d59068d6d7c5da1f5c12b/tsmap/tmap_hinds/map_hinds.py#L11-L22
def update_map(self, _s_t, self._s_t):
        """Return an update of the maps to be updated

       
        Returns an update of the maps to be updated
        If _s_t is None:
            tsmap = tslist
        If _s_t is not None:
            tsmap = tslist
        """
        if tsmap is not None:
            tsmap = tslist
        return tsmap [u'def', u'update_map', u'(', u'self', u',', u'_s_t', u',', u'self', u')', u':', u'if', u'tpsmap', u'is', u'None', u':', u'tpsmap', u'=', u'tpslist', u'if', u'tpsmap', u'is', u'not', u'None', u':', u'tpsmap', u'=', u'tpslist', u'return', u'tpsmap'] Returns an update of the maps to be updated

    Returns an update of the maps to be updated If tsmap is not None:
            tsmap = tslist
        If _s_t is None:
            tsmap = tslist
        return ts

[4500 | 24109.94] loss=0.74 avg=1.04
[4501 | 24115.07] loss=1.12 avg=1.04
[4502 | 24120.29] loss=0.67 avg=1.03
[4503 | 24125.43] loss=1.34 avg=1.04
[4504 | 24130.57] loss=0.67 avg=1.03
[4505 | 24135.63] loss=0.81 avg=1.03
[4506 | 24140.75] loss=0.97 avg=1.03
[4507 | 24145.93] loss=1.16 avg=1.03
[4508 | 24151.19] loss=0.48 avg=1.03
[4509 | 24156.35] loss=1.80 avg=1.03
[4510 | 24161.59] loss=0.80 avg=1.03
[4511 | 24166.77] loss=0.72 avg=1.03
[4512 | 24172.00] loss=0.42 avg=1.02
[4513 | 24177.40] loss=1.23 avg=1.02
[4514 | 24182.61] loss=1.06 avg=1.02
[4515 | 24187.83] loss=0.69 avg=1.02
[4516 | 24193.05] loss=1.15 avg=1.02
[4517 | 24198.38] loss=0.73 avg=1.02
[4518 | 24203.58] loss=0.65 avg=1.02
[4519 | 24208.79] loss=1.10 avg=1.02
[4520 | 24214.11] loss=1.16 avg=1.02
[4521 | 24219.30] loss=1.15 avg=1.02
[4522 | 24224.48] loss=0.86 avg=1.02
[4523 | 24229.72] loss=1.10 avg=1.02
[4524 | 24234.90] loss=1.66 avg=1.02
[4525 | 24240.13] loss=1.46 avg=1.03
[4526 | 24245.37] loss=0.78 avg=1.03
[4527 | 24250.59] loss=0.50 avg=1.02
[4528 | 24255.71] loss=0.91 avg=1.02
[4529 | 24260.97] loss=0.83 avg=1.02
[4530 | 24266.23] loss=0.66 avg=1.01
[4531 | 24271.47] loss=1.18 avg=1.02
[4532 | 24276.68] loss=1.81 avg=1.02
[4533 | 24281.94] loss=0.60 avg=1.02
[4534 | 24287.30] loss=0.93 avg=1.02
[4535 | 24292.45] loss=0.80 avg=1.02
[4536 | 24297.68] loss=0.84 avg=1.02
[4537 | 24302.91] loss=0.82 avg=1.01
[4538 | 24308.15] loss=0.62 avg=1.01
[4539 | 24313.40] loss=1.23 avg=1.01
[4540 | 24318.60] loss=1.12 avg=1.01
[4541 | 24323.75] loss=1.71 avg=1.02
[4542 | 24329.02] loss=1.60 avg=1.03
[4543 | 24334.19] loss=0.77 avg=1.02
[4544 | 24339.42] loss=0.67 avg=1.02
[4545 | 24344.70] loss=1.63 avg=1.03
[4546 | 24349.98] loss=1.86 avg=1.03
[4547 | 24355.16] loss=1.41 avg=1.04
[4548 | 24360.36] loss=0.85 avg=1.04
[4549 | 24365.48] loss=0.93 avg=1.03
[4550 | 24370.67] loss=0.72 avg=1.03
[4551 | 24375.86] loss=1.36 avg=1.03
[4552 | 24381.03] loss=1.25 avg=1.04
[4553 | 24386.23] loss=0.92 avg=1.04
[4554 | 24391.33] loss=0.88 avg=1.03
[4555 | 24396.37] loss=0.83 avg=1.03
[4556 | 24401.51] loss=0.86 avg=1.03
[4557 | 24406.73] loss=1.05 avg=1.03
[4558 | 24412.02] loss=0.69 avg=1.03
[4559 | 24416.78] loss=1.47 avg=1.03
[4560 | 24421.48] loss=0.65 avg=1.03
[4561 | 24426.07] loss=1.11 avg=1.03
[4562 | 24430.93] loss=0.96 avg=1.03
[4563 | 24436.07] loss=1.37 avg=1.03
[4564 | 24441.37] loss=0.82 avg=1.03
[4565 | 24446.69] loss=1.22 avg=1.03
[4566 | 24452.01] loss=1.14 avg=1.03
[4567 | 24457.28] loss=1.23 avg=1.03
[4568 | 24462.58] loss=0.91 avg=1.03
[4569 | 24467.89] loss=0.52 avg=1.03
[4570 | 24473.16] loss=0.64 avg=1.02
[4571 | 24478.37] loss=0.86 avg=1.02
[4572 | 24483.65] loss=0.80 avg=1.02
[4573 | 24488.84] loss=1.07 avg=1.02
[4574 | 24493.95] loss=1.16 avg=1.02
[4575 | 24499.14] loss=0.65 avg=1.02
[4576 | 24504.45] loss=0.66 avg=1.01
[4577 | 24509.63] loss=0.90 avg=1.01
[4578 | 24514.89] loss=0.63 avg=1.01
[4579 | 24520.14] loss=1.11 avg=1.01
[4580 | 24525.38] loss=0.58 avg=1.01
[4581 | 24530.57] loss=0.63 avg=1.00
[4582 | 24535.70] loss=0.80 avg=1.00
[4583 | 24540.76] loss=0.81 avg=1.00
[4584 | 24545.87] loss=1.16 avg=1.00
[4585 | 24551.16] loss=1.03 avg=1.00
[4586 | 24556.38] loss=1.24 avg=1.00
[4587 | 24561.60] loss=1.52 avg=1.01
[4588 | 24566.84] loss=1.53 avg=1.01
[4589 | 24572.12] loss=0.86 avg=1.01
[4590 | 24577.27] loss=1.18 avg=1.01
[4591 | 24582.38] loss=0.99 avg=1.01
[4592 | 24587.74] loss=0.95 avg=1.01
[4593 | 24593.10] loss=1.19 avg=1.01
[4594 | 24598.38] loss=0.87 avg=1.01
[4595 | 24603.54] loss=1.05 avg=1.01
[4596 | 24608.75] loss=1.54 avg=1.02
[4597 | 24613.85] loss=2.10 avg=1.03
[4598 | 24619.04] loss=0.97 avg=1.03
[4599 | 24624.18] loss=0.88 avg=1.03
Generating samples...
======== SAMPLE 1 ========
 u.target.type == SESSION2_CONFIG_SESSION):
        return s
     return s.get_index(s) train fw/client/get_server_model_data_fw_labs.py fw/client/get_server_model_data_fw-labs 54a6da7d5a9f0c2df086d6eeb0cc6a6ddf6be9ba7 https://github.com/fw/client/blob/54a6da7d5a9f0c2df086d6eeb0cc6a6ddf6be9ba7/fw/client/get_server_model_data_fw_labs.py#L23-L32
def get_server_model_data_fw_labs(
         path,
         key_prefix,
         sub_key_prefix,
         dtype='application/json',
         self,
         sub_key_prefix,
         sub_key_prefix,
         dtype='application/json',
         self,
         sub_key_prefix,
         name_prefix,
         sub_key_prefix,
         sub_key_prefix,
         dtype='application/xml'
    ) [u'def', u'get_server_model_data_fw_labs', u'(', u'path', u',', u'key_prefix', u',', u'sub_key_prefix', u',', u'dtype', u'=', u'sub_key_prefix', u',', u'sub_key_prefix', u'=', u'sub_key_prefix', u',', u'dtype', u'=', u"'application/json'", u',', u'sub_key_prefix', u'=', u'sub_key_prefix', u',', u'dtype', u'=', u"'application/xml'", u',', u'sub_key', u'=', u'sub_key_prefix', u',', u'name_prefix', u',', u'dtype', u'=', u"'application/json'", u',', u'sub_key_prefix', u'=', u'name_prefix', u',', u'Dtype', u'=', u"'application/xml'", u',', u'sub_key', u'=', u'name_prefix', u')'] Get server model data for a sub_key and set it to the subkey prefix [u'Get', u'tserver', u'model', u'data', u'for', u'a', u'sub_key', u'and', u'set', u'to', u'the', u'subkey', u'prefix'] get_server_model_data_fw_labs python def get_server_model_data_fw_labs(
      path,
      key_prefix,
      sub_key_prefix,
      dtype='application/json',
       self,
       sub_key_prefix,
       sub_key_prefix,
       dtype='application/json',
    ) train fw/client/get_server_model_data_fw-labs 54a6da7d5a9f0c2df086d6eeb0cc6a6ddf6be9ba7 https://github.com/fw/client/blob/54a6da7d5a9f0c2df086d6eeb0cc6a6ddf6be9ba7/fw/client/get_server_model_data_fw_labs.py#L43-L46
def get_server_model_data_fw_labs(
     path,
     key_prefix,
    sub_

[4600 | 24713.12] loss=0.75 avg=1.02
[4601 | 24718.48] loss=0.66 avg=1.02
[4602 | 24723.67] loss=1.09 avg=1.02
[4603 | 24728.89] loss=0.68 avg=1.02
[4604 | 24734.09] loss=0.74 avg=1.02
[4605 | 24739.36] loss=0.63 avg=1.01
[4606 | 24744.54] loss=1.62 avg=1.02
[4607 | 24749.81] loss=0.86 avg=1.02
[4608 | 24754.98] loss=0.97 avg=1.02
[4609 | 24760.17] loss=1.63 avg=1.02
[4610 | 24765.43] loss=0.91 avg=1.02
[4611 | 24770.60] loss=1.20 avg=1.02
[4612 | 24775.92] loss=0.77 avg=1.02
[4613 | 24781.18] loss=1.37 avg=1.02
[4614 | 24786.44] loss=0.89 avg=1.02
[4615 | 24791.61] loss=0.84 avg=1.02
[4616 | 24796.76] loss=0.74 avg=1.02
[4617 | 24802.06] loss=1.12 avg=1.02
[4618 | 24807.25] loss=0.76 avg=1.02
[4619 | 24812.55] loss=1.45 avg=1.02
[4620 | 24817.81] loss=1.38 avg=1.02
[4621 | 24823.02] loss=0.83 avg=1.02
[4622 | 24828.37] loss=0.75 avg=1.02
[4623 | 24833.55] loss=1.44 avg=1.02
[4624 | 24838.73] loss=1.27 avg=1.03
[4625 | 24844.02] loss=1.48 avg=1.03
[4626 | 24849.23] loss=1.08 avg=1.03
[4627 | 24854.42] loss=0.93 avg=1.03
[4628 | 24859.55] loss=0.90 avg=1.03
[4629 | 24864.75] loss=0.97 avg=1.03
[4630 | 24869.95] loss=0.70 avg=1.02
[4631 | 24875.25] loss=0.70 avg=1.02
[4632 | 24880.52] loss=1.30 avg=1.02
[4633 | 24885.76] loss=1.13 avg=1.02
[4634 | 24890.94] loss=1.17 avg=1.03
[4635 | 24896.21] loss=1.19 avg=1.03
[4636 | 24901.41] loss=1.28 avg=1.03
[4637 | 24906.57] loss=1.09 avg=1.03
[4638 | 24911.76] loss=0.72 avg=1.03
[4639 | 24917.01] loss=0.72 avg=1.02
[4640 | 24922.29] loss=0.76 avg=1.02
[4641 | 24927.44] loss=0.81 avg=1.02
[4642 | 24932.63] loss=1.23 avg=1.02
[4643 | 24937.84] loss=0.96 avg=1.02
[4644 | 24943.14] loss=0.68 avg=1.02
[4645 | 24948.46] loss=0.87 avg=1.02
[4646 | 24953.68] loss=1.03 avg=1.02
[4647 | 24958.98] loss=1.62 avg=1.02
[4648 | 24964.22] loss=1.53 avg=1.03
[4649 | 24969.45] loss=1.58 avg=1.03
[4650 | 24974.64] loss=1.05 avg=1.03
[4651 | 24979.79] loss=0.65 avg=1.03
[4652 | 24984.89] loss=0.87 avg=1.03
[4653 | 24990.02] loss=0.73 avg=1.03
[4654 | 24995.33] loss=0.76 avg=1.02
[4655 | 25000.49] loss=1.31 avg=1.03
[4656 | 25005.70] loss=0.75 avg=1.02
[4657 | 25010.95] loss=0.92 avg=1.02
[4658 | 25016.24] loss=0.87 avg=1.02
[4659 | 25021.56] loss=1.32 avg=1.02
[4660 | 25026.79] loss=1.11 avg=1.02
[4661 | 25032.05] loss=0.94 avg=1.02
[4662 | 25037.20] loss=0.58 avg=1.02
[4663 | 25042.45] loss=0.96 avg=1.02
[4664 | 25047.76] loss=0.77 avg=1.02
[4665 | 25053.17] loss=0.55 avg=1.01
[4666 | 25058.43] loss=0.74 avg=1.01
[4667 | 25063.85] loss=0.67 avg=1.00
[4668 | 25069.02] loss=0.83 avg=1.00
[4669 | 25074.43] loss=0.90 avg=1.00
[4670 | 25079.74] loss=0.77 avg=1.00
[4671 | 25085.10] loss=0.88 avg=1.00
[4672 | 25090.37] loss=1.40 avg=1.00
[4673 | 25095.77] loss=1.18 avg=1.00
[4674 | 25101.22] loss=0.77 avg=1.00
[4675 | 25106.51] loss=0.66 avg=1.00
[4676 | 25111.80] loss=1.20 avg=1.00
[4677 | 25117.20] loss=0.58 avg=1.00
[4678 | 25122.53] loss=0.48 avg=0.99
[4679 | 25127.96] loss=0.98 avg=0.99
[4680 | 25133.34] loss=0.90 avg=0.99
[4681 | 25138.48] loss=0.60 avg=0.99
[4682 | 25143.64] loss=1.61 avg=0.99
[4683 | 25149.05] loss=1.28 avg=1.00
[4684 | 25154.38] loss=0.93 avg=0.99
[4685 | 25159.73] loss=0.91 avg=0.99
[4686 | 25165.16] loss=0.87 avg=0.99
[4687 | 25170.38] loss=0.92 avg=0.99
[4688 | 25175.81] loss=1.04 avg=0.99
[4689 | 25181.00] loss=1.46 avg=1.00
[4690 | 25186.25] loss=0.97 avg=1.00
[4691 | 25191.58] loss=1.39 avg=1.00
[4692 | 25196.96] loss=1.14 avg=1.00
[4693 | 25202.33] loss=0.94 avg=1.00
[4694 | 25207.61] loss=1.02 avg=1.00
[4695 | 25212.93] loss=1.43 avg=1.01
[4696 | 25218.28] loss=0.90 avg=1.00
[4697 | 25223.54] loss=0.84 avg=1.00
[4698 | 25228.93] loss=0.66 avg=1.00
[4699 | 25234.17] loss=0.64 avg=1.00
Generating samples...
======== SAMPLE 1 ========
 self.get_item(name=name)
                 :return
                 :class:''
                """
               # The 'name' of the node must be a valid id
                if hasattr(node, 'id'):
                     return
                else:
                     node = node.get(name=name)
                if hasattr(node, 'id'):
                     return
                elif hasattr(node, 'id'):
                    return
                 elif hasattr(node, 'index'):
                    return
                else:
                    return
                elif hasattr(node, 'index'):
                   return
                return

                 return

                 return [u'def', u'get_item', u'(', u'name', u',', u'value', u',', u'index', u'=', u'True', u')', u':', u'# The 'name' of the node must be a valid ID', u'if', u'hasattr', u'(', u'type', u'(', u'node', u',', u"'identifier'", u',', u'verbose', u')', u')', u':', u'return', u'else', u':', u'return', u'# If 'index' is not a string, then return the index', u'# of the given node, and have our self calculate the index', u'# on the name of the node.', u'if', u'hasattr', u"index_key", u'(', u')', u':', u'return', u'# index is already in the list', u'if', u'index', u'has', u"'index'", u':', u'return', u'if', u'index', u'<=', u'1', u':', u'continue', u'raise', u'ValueError', u'(', u'"index must be 1 or more"', u')', u'# We are done.', u'body', u'=', u'body', u'.', u'get', u'(', u'"id"', u',', u'index', u')', u'# The 'name' of the node must be a valid id', u'if', u'hasattr', u"index_key", u'(', u')', u':', u'return', u'if', u'index', u'[', u'"index"', u']', u'>', u'0', u':', u'return', u'body', u'=', u'"''", u'split', u'(', u"''", u')', u'if', u'index', u'[', u'"index"', u']', u'>', u'1', u':', u'# For an index in list, we must be able to get the list's index on the list', u'# first', u'index_key', u'=', u'split', u'(', u'"index"', u')', u'dirs', u'=', u'dir_types', u'.', u'get', u

[4700 | 25324.01] loss=1.02 avg=1.00
[4701 | 25329.42] loss=0.94 avg=1.00
[4702 | 25334.66] loss=0.90 avg=0.99
[4703 | 25339.97] loss=1.14 avg=1.00
[4704 | 25345.27] loss=0.83 avg=0.99
[4705 | 25350.59] loss=1.14 avg=1.00
[4706 | 25355.84] loss=1.56 avg=1.00
[4707 | 25361.10] loss=1.02 avg=1.00
[4708 | 25366.44] loss=0.82 avg=1.00
[4709 | 25371.69] loss=0.78 avg=1.00
[4710 | 25376.93] loss=1.66 avg=1.00
[4711 | 25382.34] loss=0.64 avg=1.00
[4712 | 25387.69] loss=1.78 avg=1.01
[4713 | 25392.90] loss=1.01 avg=1.01
[4714 | 25398.21] loss=1.02 avg=1.01
[4715 | 25403.69] loss=0.98 avg=1.01
[4716 | 25409.03] loss=1.08 avg=1.01
[4717 | 25414.30] loss=1.35 avg=1.01
[4718 | 25419.64] loss=0.96 avg=1.01
[4719 | 25424.92] loss=0.96 avg=1.01
[4720 | 25430.19] loss=0.70 avg=1.01
[4721 | 25435.50] loss=0.97 avg=1.01
[4722 | 25440.83] loss=1.37 avg=1.01
[4723 | 25446.17] loss=1.37 avg=1.02
[4724 | 25451.40] loss=1.68 avg=1.02
[4725 | 25456.69] loss=1.25 avg=1.02
[4726 | 25462.05] loss=0.89 avg=1.02
[4727 | 25467.49] loss=0.99 avg=1.02
[4728 | 25472.77] loss=0.89 avg=1.02
[4729 | 25477.97] loss=0.83 avg=1.02
[4730 | 25483.11] loss=0.78 avg=1.02
[4731 | 25488.38] loss=0.89 avg=1.02
[4732 | 25493.60] loss=1.38 avg=1.02
[4733 | 25498.84] loss=1.38 avg=1.02
[4734 | 25504.09] loss=1.17 avg=1.02
[4735 | 25509.30] loss=0.89 avg=1.02
[4736 | 25514.47] loss=0.64 avg=1.02
[4737 | 25519.76] loss=1.09 avg=1.02
[4738 | 25525.07] loss=1.17 avg=1.02
[4739 | 25530.28] loss=1.00 avg=1.02
[4740 | 25535.47] loss=0.81 avg=1.02
[4741 | 25540.79] loss=1.16 avg=1.02
[4742 | 25545.93] loss=1.08 avg=1.02
[4743 | 25551.20] loss=0.45 avg=1.02
[4744 | 25556.49] loss=1.79 avg=1.02
[4745 | 25561.69] loss=1.39 avg=1.03
[4746 | 25566.92] loss=0.91 avg=1.03
[4747 | 25572.17] loss=1.02 avg=1.03
[4748 | 25577.43] loss=0.93 avg=1.02
[4749 | 25582.69] loss=0.81 avg=1.02
[4750 | 25587.94] loss=1.06 avg=1.02
[4751 | 25593.29] loss=0.84 avg=1.02
[4752 | 25598.60] loss=0.69 avg=1.02
[4753 | 25603.93] loss=0.99 avg=1.02
[4754 | 25609.24] loss=0.70 avg=1.01
[4755 | 25614.52] loss=0.95 avg=1.01
[4756 | 25619.71] loss=1.03 avg=1.01
[4757 | 25624.97] loss=1.31 avg=1.02
[4758 | 25630.20] loss=0.86 avg=1.02
[4759 | 25635.43] loss=0.71 avg=1.01
[4760 | 25640.71] loss=1.07 avg=1.01
[4761 | 25645.92] loss=0.84 avg=1.01
[4762 | 25651.28] loss=0.81 avg=1.01
[4763 | 25656.58] loss=0.88 avg=1.01
[4764 | 25661.72] loss=0.61 avg=1.00
[4765 | 25666.88] loss=0.68 avg=1.00
[4766 | 25672.07] loss=0.65 avg=1.00
[4767 | 25677.20] loss=0.89 avg=1.00
[4768 | 25682.50] loss=0.32 avg=0.99
[4769 | 25687.64] loss=0.94 avg=0.99
[4770 | 25692.81] loss=1.81 avg=1.00
[4771 | 25698.03] loss=1.06 avg=1.00
[4772 | 25703.25] loss=1.25 avg=1.00
[4773 | 25708.48] loss=0.95 avg=1.00
[4774 | 25713.60] loss=1.52 avg=1.00
[4775 | 25718.76] loss=1.47 avg=1.01
[4776 | 25723.91] loss=0.60 avg=1.01
[4777 | 25729.12] loss=0.96 avg=1.01
[4778 | 25734.30] loss=1.14 avg=1.01
[4779 | 25739.56] loss=1.60 avg=1.01
[4780 | 25744.83] loss=1.25 avg=1.01
[4781 | 25750.18] loss=0.92 avg=1.01
[4782 | 25755.43] loss=0.37 avg=1.01
[4783 | 25760.61] loss=0.87 avg=1.01
[4784 | 25765.94] loss=2.12 avg=1.02
[4785 | 25771.29] loss=0.62 avg=1.01
[4786 | 25776.56] loss=0.79 avg=1.01
[4787 | 25781.79] loss=1.44 avg=1.02
[4788 | 25786.94] loss=2.28 avg=1.03
[4789 | 25792.14] loss=0.64 avg=1.02
[4790 | 25797.35] loss=2.70 avg=1.04
[4791 | 25802.53] loss=1.29 avg=1.04
[4792 | 25807.68] loss=1.15 avg=1.04
[4793 | 25812.90] loss=1.13 avg=1.04
[4794 | 25818.20] loss=0.99 avg=1.04
[4795 | 25823.34] loss=0.30 avg=1.04
[4796 | 25828.62] loss=0.55 avg=1.03
[4797 | 25833.88] loss=0.81 avg=1.03
[4798 | 25839.16] loss=0.92 avg=1.03
[4799 | 25844.26] loss=0.87 avg=1.03
Generating samples...
======== SAMPLE 1 ========
 definition', u':', u'params', u'.', u'requiries', u'=', u'[', u"'%s/%s'", u'%', u'(', u'url', u'.', u'gettext', u'(', u')', u')', u',', u')', u'if', u'params', u'[', u"'hash'", u']', u'is', u'None', u'or', u'params', u'[', u"'name'", u']', u'is', u'None', u':', u'params', u'[', u"'name'", u']', u'=', u'requests', u'params', u'[', u"'hash'", u']', u'=', u'hash', u'rend', u'=', u'get', u'(', u'params', u'[', u"'hash'", u']', u',', u'table', u',', u'index', u',', u'__', u',', u'hashcode', u')', u'if', u'hashcode', u'!=', u'hash', u':', u'params', u'[', u"'type'", u']', u'=', u'Hash', u'rend', u'=', u'idx', u'=', u'params', u'[', u"'hash'", u']', u'else', u':', u'rend', u'=', u'table', u'else', u':', u'rend', u'=', u'hash', u'sig', u'.', u'new', u'(', u'signals,', u'params', u'[', u"'hash'", u']', u',', u'table', u',', u'index', u',', u'info', u')', u'return', u'params'] Returns the hash value for the given parameter in the given queries.

         :class:`QuerySig`. [u'Returns', u'the', u'hash', u'value', u'for', u'the', u'given', u'parameter', u'in', u'the', u'given', u'queries', u'.'] __hashpy___hash python def __hashpy___hashpy__(self, params, **kwargs, **kwargs):
        """
        Returns the hash value for the given parameter in the given queries.

         :class:`QuerySig`.

         :rtype:`HashTable`.
         :rtype:`HashTable`
         """
        params = {'hash': params['hash'], 'hashCode': params['hashcode']}
        return hash(params) train shingle/sig/hashtable.py shingle/sig 2d8739f8c2cc98d7cfb13c6a1f3e2029f9f8f1dd6 https://github.com/shingle/sig/blob/2d8739f8c2cc98d7cfb13c6a1f3e2029f9f8f1dd6/sig/sig/hashtable.py#L1071-L1074
def hashlist(self, name, key_code,
                                              location, key_code, key_value) = {}
        return self.hashlist.add(name, key_code, key_value) [u'def', u'hashlist', u'(', u'self', u',', u'name', u',', u'key_code', u',', u'key_value', u')', u'=', u'{', u'}', u'return', u'self', u'.', u'hashlist', u'.', u'add', u'(', u'name', u',', u'key_code', u',', u'key_value', u')'] Returns the hash value for the given parameter in the given queries.

         :class

[4800 | 25933.39] loss=1.32 avg=1.03
[4801 | 25938.60] loss=0.63 avg=1.03
[4802 | 25943.80] loss=0.85 avg=1.02
[4803 | 25949.00] loss=1.24 avg=1.03
[4804 | 25954.17] loss=0.63 avg=1.02
[4805 | 25959.61] loss=1.31 avg=1.03
[4806 | 25964.88] loss=1.79 avg=1.03
[4807 | 25970.07] loss=0.67 avg=1.03
[4808 | 25975.49] loss=1.17 avg=1.03
[4809 | 25980.76] loss=0.69 avg=1.03
[4810 | 25986.06] loss=0.78 avg=1.02
[4811 | 25991.34] loss=0.66 avg=1.02
[4812 | 25996.57] loss=0.93 avg=1.02
[4813 | 26001.79] loss=1.05 avg=1.02
[4814 | 26007.04] loss=0.83 avg=1.02
[4815 | 26012.22] loss=0.91 avg=1.02
[4816 | 26017.45] loss=1.21 avg=1.02
[4817 | 26022.72] loss=1.59 avg=1.03
[4818 | 26028.07] loss=1.54 avg=1.03
[4819 | 26033.38] loss=1.30 avg=1.03
[4820 | 26038.65] loss=0.52 avg=1.03
[4821 | 26043.80] loss=1.43 avg=1.03
[4822 | 26049.13] loss=1.39 avg=1.04
[4823 | 26054.35] loss=0.88 avg=1.03
[4824 | 26059.51] loss=1.07 avg=1.03
[4825 | 26064.78] loss=0.77 avg=1.03
[4826 | 26070.16] loss=1.07 avg=1.03
[4827 | 26075.51] loss=0.67 avg=1.03
[4828 | 26080.84] loss=0.78 avg=1.03
[4829 | 26086.08] loss=0.59 avg=1.02
[4830 | 26091.36] loss=0.63 avg=1.02
[4831 | 26096.62] loss=1.19 avg=1.02
[4832 | 26101.80] loss=1.30 avg=1.02
[4833 | 26107.02] loss=1.08 avg=1.02
[4834 | 26112.27] loss=1.75 avg=1.03
[4835 | 26117.50] loss=1.01 avg=1.03
[4836 | 26122.70] loss=1.48 avg=1.03
[4837 | 26127.94] loss=1.19 avg=1.04
[4838 | 26133.05] loss=1.25 avg=1.04
[4839 | 26138.19] loss=1.09 avg=1.04
[4840 | 26143.46] loss=1.10 avg=1.04
[4841 | 26148.63] loss=0.98 avg=1.04
[4842 | 26153.89] loss=0.76 avg=1.04
[4843 | 26159.17] loss=0.95 avg=1.03
[4844 | 26164.55] loss=0.63 avg=1.03
[4845 | 26169.84] loss=1.32 avg=1.03
[4846 | 26175.01] loss=0.76 avg=1.03
[4847 | 26180.43] loss=0.84 avg=1.03
[4848 | 26185.77] loss=1.08 avg=1.03
[4849 | 26190.94] loss=0.53 avg=1.02
[4850 | 26196.20] loss=0.51 avg=1.02
[4851 | 26201.35] loss=1.17 avg=1.02
[4852 | 26206.48] loss=1.10 avg=1.02
[4853 | 26211.62] loss=0.84 avg=1.02
[4854 | 26216.76] loss=1.60 avg=1.03
[4855 | 26221.94] loss=0.96 avg=1.03
[4856 | 26227.13] loss=0.73 avg=1.02
[4857 | 26232.25] loss=0.85 avg=1.02
[4858 | 26237.32] loss=0.58 avg=1.02
[4859 | 26242.52] loss=1.34 avg=1.02
[4860 | 26247.70] loss=0.93 avg=1.02
[4861 | 26253.02] loss=1.24 avg=1.02
[4862 | 26258.22] loss=0.91 avg=1.02
[4863 | 26263.36] loss=0.70 avg=1.02
[4864 | 26268.49] loss=0.47 avg=1.01
[4865 | 26273.66] loss=1.65 avg=1.02
[4866 | 26278.92] loss=1.57 avg=1.02
[4867 | 26284.28] loss=0.70 avg=1.02
[4868 | 26289.50] loss=1.23 avg=1.02
[4869 | 26294.79] loss=0.82 avg=1.02
[4870 | 26300.15] loss=1.23 avg=1.02
[4871 | 26305.51] loss=0.60 avg=1.02
[4872 | 26310.77] loss=0.76 avg=1.01
[4873 | 26315.97] loss=1.25 avg=1.02
[4874 | 26321.17] loss=0.89 avg=1.02
[4875 | 26326.45] loss=1.06 avg=1.02
[4876 | 26331.63] loss=0.78 avg=1.01
[4877 | 26336.81] loss=1.20 avg=1.02
[4878 | 26342.10] loss=0.94 avg=1.02
[4879 | 26347.37] loss=1.02 avg=1.02
[4880 | 26352.69] loss=0.51 avg=1.01
[4881 | 26357.93] loss=1.30 avg=1.01
[4882 | 26363.31] loss=0.72 avg=1.01
[4883 | 26368.57] loss=0.92 avg=1.01
[4884 | 26373.79] loss=0.65 avg=1.01
[4885 | 26378.98] loss=0.64 avg=1.00
[4886 | 26384.22] loss=0.44 avg=1.00
[4887 | 26389.64] loss=0.89 avg=1.00
[4888 | 26394.93] loss=1.03 avg=1.00
[4889 | 26400.16] loss=1.16 avg=1.00
[4890 | 26405.45] loss=0.87 avg=1.00
[4891 | 26410.63] loss=1.08 avg=1.00
[4892 | 26415.91] loss=1.33 avg=1.00
[4893 | 26421.13] loss=1.51 avg=1.01
[4894 | 26426.38] loss=1.35 avg=1.01
[4895 | 26431.62] loss=1.63 avg=1.02
[4896 | 26436.91] loss=1.09 avg=1.02
[4897 | 26442.11] loss=0.75 avg=1.01
[4898 | 26447.29] loss=0.74 avg=1.01
[4899 | 26452.60] loss=0.90 avg=1.01
Generating samples...
======== SAMPLE 1 ========
1']
    # The method is the first argument in the get_method argument string
    return :str
    self._method.method_name = self._method [u'def', u'_method_name', u'(', u'self', u',', u'message', u')', u':', u'self', u'.', u'_method_name', u'=', u'self', u'.', u'_method', u'[', u"'method'", u']', u'tries', u'=', u'[', u']', u'for', u'a', u'in', u'message', u'.split', u'(', u'[', u"'''", u']', u')', u']', u':', u'logger', u'.', u'exception', u'(', u'"No method found on the stream
    '", u')', u'finally', u':', u'return', u':', u'self', u'.', u'_method'] Returns the method information returned by the
    method. [u'Returns', u'the', u'method', u'information', u'returned', u'by', u'the', u'method', u'.'] get_method python def _method_name(self, message):
    """
    Returns the method information returned by the
    method.
    """
    timeout = time.now()
    if self.has_timeout():
        self._method_name = message.get_method()
        self._method = message if timeout:
        log_info("Request timed out", timeout)
        self._method.method_name = timeout train gyp/python/gyp_client.py gyph/gyp-python-tensorflow 7df5f2da8d4d27f8c2a4b5af7d9eb2e67c1ed2a https://github.com/gyph/gyp-python-tensorflow/blob/7df5f2da8d4d27f8c2a4b5af7d9eb2e67c1ed2a/gyp/python/gyp_client.py#L20-L36
def _method_name(self):
    """
    Returns the method information returned by the
    method.
    """
    timeout = time.now()
    res = {
              'timedelta' : Time.now().slice(timeout),
             'channel_id' : int, 'total_time' : None, 'channel_id' : 'channel_id'
             'seconds' : timeout, 'delay_before_time' : timeout
             'seconds' : timeout, 'timeout' : None, 'channel_id' : 'channel_id'
             'channel_id' : 'sessions' [u'Returns', u'the', u'the', u'method', u'information', u'returned', u'by', u'the', u'method', u'.'] get_method python def _method_name(self):
    """
    Returns the method information returned by the
    method.
    """
    timeout = time.now()
    res = {
             'timedelta' : Time.now().slice(timeout),
            'channel_id' : int, 'total_time' : None, 'channel_id' : 'channel_id'
            'seconds' : timeout, 'delay_before_time' : timeout, 'channel_id', 'total_time', 'channel_id' : 'sessions'
            'seconds' : timeout, 'timeout' : None, 'channel_id' : 'sessions'
            'timeout' : timeout,

[4900 | 26543.42] loss=1.12 avg=1.01
[4901 | 26548.89] loss=0.83 avg=1.01
[4902 | 26554.25] loss=1.27 avg=1.01
[4903 | 26559.56] loss=0.86 avg=1.01
[4904 | 26564.96] loss=0.64 avg=1.01
[4905 | 26570.23] loss=0.93 avg=1.01
[4906 | 26575.56] loss=1.15 avg=1.01
[4907 | 26580.84] loss=1.27 avg=1.01
[4908 | 26586.21] loss=1.32 avg=1.01
[4909 | 26591.54] loss=0.93 avg=1.01
[4910 | 26596.83] loss=0.64 avg=1.01
[4911 | 26602.13] loss=1.06 avg=1.01
[4912 | 26607.47] loss=0.81 avg=1.01
[4913 | 26612.99] loss=0.74 avg=1.00
[4914 | 26618.34] loss=0.74 avg=1.00
[4915 | 26623.60] loss=1.53 avg=1.01
[4916 | 26628.84] loss=1.29 avg=1.01
[4917 | 26633.90] loss=0.69 avg=1.01
[4918 | 26639.12] loss=1.58 avg=1.01
[4919 | 26644.31] loss=0.79 avg=1.01
[4920 | 26649.58] loss=0.51 avg=1.00
[4921 | 26654.84] loss=0.88 avg=1.00
[4922 | 26660.13] loss=1.46 avg=1.01
[4923 | 26665.44] loss=1.01 avg=1.01
[4924 | 26670.71] loss=0.98 avg=1.01
[4925 | 26675.99] loss=0.62 avg=1.00
[4926 | 26681.17] loss=1.90 avg=1.01
[4927 | 26686.43] loss=0.81 avg=1.01
[4928 | 26691.66] loss=0.94 avg=1.01
[4929 | 26697.06] loss=1.24 avg=1.01
[4930 | 26702.35] loss=1.33 avg=1.02
[4931 | 26707.71] loss=0.92 avg=1.01
[4932 | 26712.98] loss=0.75 avg=1.01
[4933 | 26718.23] loss=0.89 avg=1.01
[4934 | 26723.42] loss=1.00 avg=1.01
[4935 | 26728.64] loss=1.19 avg=1.01
[4936 | 26733.87] loss=0.80 avg=1.01
[4937 | 26739.09] loss=1.24 avg=1.01
[4938 | 26744.29] loss=0.86 avg=1.01
[4939 | 26749.60] loss=1.86 avg=1.02
[4940 | 26754.99] loss=1.02 avg=1.02
[4941 | 26760.39] loss=0.68 avg=1.02
[4942 | 26765.71] loss=1.14 avg=1.02
[4943 | 26770.92] loss=0.87 avg=1.02
[4944 | 26776.20] loss=1.07 avg=1.02
[4945 | 26781.48] loss=1.42 avg=1.02
[4946 | 26786.82] loss=0.57 avg=1.02
[4947 | 26792.03] loss=0.62 avg=1.01
[4948 | 26797.33] loss=0.82 avg=1.01
[4949 | 26802.67] loss=0.91 avg=1.01
[4950 | 26807.89] loss=0.66 avg=1.01
[4951 | 26813.07] loss=0.77 avg=1.00
[4952 | 26818.30] loss=0.68 avg=1.00
[4953 | 26823.49] loss=0.81 avg=1.00
[4954 | 26828.73] loss=1.36 avg=1.00
[4955 | 26833.98] loss=0.41 avg=1.00
[4956 | 26839.32] loss=0.95 avg=1.00
[4957 | 26844.59] loss=1.24 avg=1.00
[4958 | 26849.90] loss=0.84 avg=1.00
[4959 | 26855.17] loss=0.75 avg=0.99
[4960 | 26860.54] loss=0.58 avg=0.99
[4961 | 26865.77] loss=0.86 avg=0.99
[4962 | 26870.92] loss=1.73 avg=1.00
[4963 | 26876.14] loss=2.10 avg=1.01
[4964 | 26881.40] loss=1.13 avg=1.01
[4965 | 26886.55] loss=1.46 avg=1.01
[4966 | 26891.90] loss=1.01 avg=1.01
[4967 | 26897.26] loss=0.81 avg=1.01
[4968 | 26902.55] loss=0.73 avg=1.01
[4969 | 26907.87] loss=0.79 avg=1.01
[4970 | 26913.19] loss=1.35 avg=1.01
[4971 | 26918.42] loss=1.36 avg=1.01
[4972 | 26923.83] loss=0.66 avg=1.01
[4973 | 26929.11] loss=1.33 avg=1.01
[4974 | 26934.37] loss=2.14 avg=1.02
[4975 | 26939.69] loss=0.83 avg=1.02
[4976 | 26944.96] loss=1.46 avg=1.03
[4977 | 26950.22] loss=0.60 avg=1.02
[4978 | 26955.39] loss=1.82 avg=1.03
[4979 | 26960.66] loss=0.44 avg=1.02
[4980 | 26965.99] loss=1.25 avg=1.03
[4981 | 26971.24] loss=0.93 avg=1.03
[4982 | 26976.47] loss=1.32 avg=1.03
[4983 | 26981.67] loss=0.66 avg=1.02
[4984 | 26986.88] loss=0.96 avg=1.02
[4985 | 26992.22] loss=0.82 avg=1.02
[4986 | 26997.52] loss=0.74 avg=1.02
[4987 | 27002.82] loss=0.87 avg=1.02
[4988 | 27008.16] loss=0.79 avg=1.02
[4989 | 27013.41] loss=1.76 avg=1.02
[4990 | 27018.63] loss=1.75 avg=1.03
[4991 | 27023.93] loss=1.11 avg=1.03
[4992 | 27029.16] loss=1.03 avg=1.03
[4993 | 27034.55] loss=1.03 avg=1.03
[4994 | 27039.82] loss=1.87 avg=1.04
[4995 | 27045.01] loss=1.22 avg=1.04
[4996 | 27050.30] loss=1.65 avg=1.05
[4997 | 27055.72] loss=0.84 avg=1.05
[4998 | 27061.12] loss=0.63 avg=1.04
[4999 | 27066.29] loss=0.93 avg=1.04
Saving checkpoint/run1/model-5000
Generating samples...
======== SAMPLE 1 ========

"

  # Get first field of the table in the row,
  # if the last field in "tablename" already exists:  # add fields in the table that this field already
  # exists, or if it has already been updated.
   fields = [
            f.create_field(
              "tablename", "table-type",
              "tablename-id", "id", "last-field",
              "tablename", "tabletype", "last-field-name",
               "field", "old-field", "field",      "retrieved-field-id",
              "fieldname", "fieldtype", "retrieved-field-id",
              "fieldname", "old-tablename", "old-field", "field",
               "tablename", "tabletype", "table-size",
              "tablename", "old-line", "tablename", "old-field",
              "old-field", "old-field-name", "old-field", "field",
              "fieldname", "old-updated-line", "old-field", "field",
              "fields", "fieldname", "fieldtype", "field",
             "fields-old-line", "fields-old-field", "fields-old-field",
             "fields-old-field', "fields-old-field', "fields-old-field",
             "all-fields",
            "fieldname", "fieldtype", "field", "field-size",
            "fieldname", "fieldtype", "field", "exceeded-old-field",
             "if-old-field", "if-field-old-exceeded",
            "old-field-old-field", "old-field-old", "old-field-old",
            "fields-old-field-old-field", "fields-old-field-old-field",
            "table-table",  "column-counter",  "text",
            "table-type", "table", "old-type", "tablename",
            "field", "old-oldfield", "old-field-old-field",
            "field-new-field_age_and_age_name", "Field-new-field-age-name",
            "first-field-retried", "default", "default",
            "value", "old-field-field-age-age-name", "field-old-field-age-name",
            "field-old-field-age-value", "field-old-field-age-value",
            "old-fields-old-field-oldvalue-id", "field-old-field-age-name",
            "f-tablename", "field-field-old-field-entry_count_field", "field-field-old-field-entry-count-field",
            "old-fields-old-fields-field-old-field-entry_count_field", "field-field-old-field-entry-count-entry",
      

[5000 | 27157.20] loss=1.07 avg=1.04
[5001 | 27162.42] loss=0.89 avg=1.04
[5002 | 27167.74] loss=0.68 avg=1.03
[5003 | 27173.04] loss=0.81 avg=1.03
[5004 | 27178.29] loss=0.67 avg=1.03
[5005 | 27183.48] loss=1.23 avg=1.03
[5006 | 27188.65] loss=0.61 avg=1.03
[5007 | 27193.78] loss=1.09 avg=1.03
[5008 | 27198.93] loss=1.20 avg=1.03
[5009 | 27204.09] loss=1.66 avg=1.04
[5010 | 27209.25] loss=0.90 avg=1.03
[5011 | 27214.53] loss=1.33 avg=1.04
[5012 | 27219.68] loss=1.05 avg=1.04
[5013 | 27224.91] loss=1.30 avg=1.04
[5014 | 27230.15] loss=1.14 avg=1.04
[5015 | 27235.38] loss=1.15 avg=1.04
[5016 | 27240.62] loss=1.50 avg=1.05
[5017 | 27245.95] loss=1.15 avg=1.05
[5018 | 27251.25] loss=0.86 avg=1.05
[5019 | 27256.62] loss=0.58 avg=1.04
[5020 | 27261.94] loss=0.62 avg=1.04
[5021 | 27267.26] loss=0.60 avg=1.03
[5022 | 27272.49] loss=0.98 avg=1.03
[5023 | 27277.81] loss=1.12 avg=1.03
[5024 | 27283.04] loss=1.08 avg=1.03
[5025 | 27288.30] loss=0.51 avg=1.03
[5026 | 27293.55] loss=0.65 avg=1.02
[5027 | 27298.67] loss=1.08 avg=1.02
[5028 | 27303.87] loss=0.73 avg=1.02
[5029 | 27309.06] loss=0.47 avg=1.02
[5030 | 27314.28] loss=0.46 avg=1.01
[5031 | 27319.56] loss=0.66 avg=1.01
[5032 | 27324.87] loss=0.99 avg=1.01
[5033 | 27330.01] loss=0.79 avg=1.00
[5034 | 27335.21] loss=1.43 avg=1.01
[5035 | 27340.40] loss=1.72 avg=1.02
[5036 | 27345.59] loss=1.10 avg=1.02
[5037 | 27350.79] loss=0.89 avg=1.02
[5038 | 27356.11] loss=1.40 avg=1.02
[5039 | 27361.25] loss=0.80 avg=1.02
[5040 | 27366.47] loss=1.63 avg=1.02
[5041 | 27371.54] loss=0.84 avg=1.02
[5042 | 27376.74] loss=1.11 avg=1.02
[5043 | 27382.09] loss=0.83 avg=1.02
[5044 | 27387.34] loss=1.24 avg=1.02
[5045 | 27392.55] loss=0.96 avg=1.02
[5046 | 27397.76] loss=1.33 avg=1.03
[5047 | 27402.91] loss=0.98 avg=1.02
[5048 | 27408.19] loss=1.02 avg=1.02
[5049 | 27413.41] loss=1.56 avg=1.03
[5050 | 27418.61] loss=1.20 avg=1.03
[5051 | 27423.86] loss=0.52 avg=1.03
[5052 | 27429.06] loss=1.63 avg=1.03
[5053 | 27434.33] loss=0.96 avg=1.03
[5054 | 27439.56] loss=0.96 avg=1.03
[5055 | 27444.76] loss=1.14 avg=1.03
[5056 | 27449.84] loss=0.77 avg=1.03
[5057 | 27455.05] loss=1.20 avg=1.03
[5058 | 27460.36] loss=0.45 avg=1.03
[5059 | 27465.62] loss=1.05 avg=1.03
[5060 | 27470.83] loss=1.64 avg=1.03
[5061 | 27476.09] loss=1.17 avg=1.03
[5062 | 27481.37] loss=0.84 avg=1.03
[5063 | 27486.70] loss=1.06 avg=1.03
[5064 | 27492.01] loss=0.94 avg=1.03
[5065 | 27497.15] loss=0.97 avg=1.03
[5066 | 27502.39] loss=1.33 avg=1.03
[5067 | 27507.73] loss=0.99 avg=1.03
[5068 | 27512.98] loss=0.99 avg=1.03
[5069 | 27518.26] loss=0.60 avg=1.03
[5070 | 27523.51] loss=1.69 avg=1.03
[5071 | 27528.68] loss=1.09 avg=1.04
[5072 | 27533.86] loss=0.96 avg=1.03
[5073 | 27539.06] loss=0.72 avg=1.03
[5074 | 27544.26] loss=0.74 avg=1.03
[5075 | 27549.52] loss=0.96 avg=1.03
[5076 | 27554.75] loss=0.70 avg=1.02
[5077 | 27559.95] loss=0.98 avg=1.02
[5078 | 27565.18] loss=0.69 avg=1.02
[5079 | 27570.52] loss=1.59 avg=1.03
[5080 | 27575.71] loss=0.69 avg=1.02
[5081 | 27581.07] loss=0.86 avg=1.02
[5082 | 27586.37] loss=2.46 avg=1.04
[5083 | 27591.76] loss=1.29 avg=1.04
[5084 | 27597.01] loss=1.35 avg=1.04
[5085 | 27602.30] loss=1.45 avg=1.05
[5086 | 27607.63] loss=1.11 avg=1.05
[5087 | 27612.77] loss=0.69 avg=1.04
[5088 | 27618.04] loss=1.27 avg=1.04
[5089 | 27623.33] loss=0.48 avg=1.04
[5090 | 27628.64] loss=1.02 avg=1.04
[5091 | 27633.95] loss=0.86 avg=1.04
[5092 | 27639.23] loss=1.53 avg=1.04
[5093 | 27644.45] loss=1.03 avg=1.04
[5094 | 27649.73] loss=1.17 avg=1.04
[5095 | 27655.00] loss=1.36 avg=1.05
[5096 | 27660.29] loss=0.70 avg=1.04
[5097 | 27665.59] loss=1.16 avg=1.04
[5098 | 27670.75] loss=1.34 avg=1.05
[5099 | 27675.87] loss=1.09 avg=1.05
Generating samples...
======== SAMPLE 1 ========
as' is'in', the
             source.', so that the output

         :param source: The source file.

        :rtype: object
        This is simply a dictionary containing list of keys. If there is already a key,
        the keys of the key will be passed to the dictionary.

        :rtype: string
        This is an integer value indicating input. If the
        input string is not None,
        the default is False and the argument is None.

        Returns:
            dict

        :rtype: Python string
        Returns:
            dict.

        :rtype: dictionary
        Returns:
            dict.

        """
        return dict(source, source, source,
                                                          source == '')
        return dict(source == '' or source ==
                                    source == '') train nc/python/ncapi.py mrh/ncaa-python cb57c6ca6d99f8c4b6b904a8d5ee9d2e6c5e https://github.com/hhr/nca-python/blob/b57c6ca6d99f8c4b6b904a8d5ee9d2e6c5e/ncapi.py#L1228-L1244
def dict_key(source, source, source, source,
                                                          key_type,
                                             key=key_type).decode_unquote(
                                                         key=key_type,
                                key=key_type,
                                key=key_type, or key=key).decode_unquote(
                            key=key_type,
                                           value=value,
                                   key=key_type,
                              key=key_type, or key=key)
        """
        return dict(source, source, source,
                                 

[5100 | 27765.04] loss=1.13 avg=1.05
[5101 | 27770.18] loss=0.79 avg=1.05
[5102 | 27775.39] loss=1.42 avg=1.05
[5103 | 27780.54] loss=1.18 avg=1.05
[5104 | 27785.77] loss=0.82 avg=1.05
[5105 | 27790.93] loss=0.82 avg=1.05
[5106 | 27796.10] loss=0.98 avg=1.05
[5107 | 27801.29] loss=1.45 avg=1.05
[5108 | 27806.68] loss=0.99 avg=1.05
[5109 | 27811.87] loss=1.09 avg=1.05
[5110 | 27817.15] loss=0.87 avg=1.05
[5111 | 27822.55] loss=0.66 avg=1.04
[5112 | 27827.83] loss=1.16 avg=1.04
[5113 | 27833.08] loss=0.82 avg=1.04
[5114 | 27838.28] loss=0.90 avg=1.04
[5115 | 27843.52] loss=1.13 avg=1.04
[5116 | 27848.69] loss=0.86 avg=1.04
[5117 | 27853.93] loss=1.58 avg=1.05
[5118 | 27859.11] loss=0.86 avg=1.04
[5119 | 27864.35] loss=1.19 avg=1.05
[5120 | 27869.62] loss=0.75 avg=1.04
[5121 | 27874.81] loss=1.86 avg=1.05
[5122 | 27879.92] loss=0.96 avg=1.05
[5123 | 27885.11] loss=0.95 avg=1.05
[5124 | 27890.29] loss=1.01 avg=1.05
[5125 | 27895.55] loss=2.03 avg=1.06
[5126 | 27900.80] loss=1.73 avg=1.06
[5127 | 27906.10] loss=1.18 avg=1.07
[5128 | 27911.28] loss=0.74 avg=1.06
[5129 | 27916.53] loss=0.81 avg=1.06
[5130 | 27921.84] loss=0.57 avg=1.06
[5131 | 27927.05] loss=1.55 avg=1.06
[5132 | 27932.22] loss=1.01 avg=1.06
[5133 | 27937.43] loss=0.89 avg=1.06
[5134 | 27942.69] loss=1.24 avg=1.06
[5135 | 27947.96] loss=0.99 avg=1.06
[5136 | 27953.15] loss=0.88 avg=1.06
[5137 | 27958.29] loss=0.63 avg=1.05
[5138 | 27963.52] loss=0.60 avg=1.05
[5139 | 27968.70] loss=1.02 avg=1.05
[5140 | 27973.92] loss=0.89 avg=1.05
[5141 | 27979.02] loss=1.43 avg=1.05
[5142 | 27984.26] loss=1.00 avg=1.05
[5143 | 27989.43] loss=1.39 avg=1.05
[5144 | 27994.71] loss=1.21 avg=1.05
[5145 | 27999.97] loss=0.86 avg=1.05
[5146 | 28005.35] loss=0.83 avg=1.05
[5147 | 28010.70] loss=0.58 avg=1.05
[5148 | 28015.85] loss=1.53 avg=1.05
[5149 | 28021.12] loss=1.02 avg=1.05
[5150 | 28026.31] loss=1.03 avg=1.05
[5151 | 28031.51] loss=1.19 avg=1.05
[5152 | 28036.72] loss=0.55 avg=1.05
[5153 | 28041.89] loss=0.76 avg=1.04
[5154 | 28047.11] loss=0.55 avg=1.04
[5155 | 28052.30] loss=0.95 avg=1.04
[5156 | 28057.48] loss=0.94 avg=1.04
[5157 | 28062.64] loss=0.71 avg=1.03
[5158 | 28067.88] loss=0.79 avg=1.03
[5159 | 28073.11] loss=1.36 avg=1.03
[5160 | 28078.32] loss=1.04 avg=1.03
[5161 | 28083.66] loss=0.69 avg=1.03
[5162 | 28088.89] loss=1.36 avg=1.03
[5163 | 28094.05] loss=0.64 avg=1.03
[5164 | 28099.31] loss=0.72 avg=1.03
[5165 | 28104.51] loss=1.18 avg=1.03
[5166 | 28109.72] loss=0.84 avg=1.03
[5167 | 28114.95] loss=0.78 avg=1.02
[5168 | 28120.31] loss=1.04 avg=1.02
[5169 | 28125.59] loss=1.07 avg=1.03
[5170 | 28130.85] loss=0.96 avg=1.02
[5171 | 28136.17] loss=0.71 avg=1.02
[5172 | 28141.53] loss=1.06 avg=1.02
[5173 | 28146.82] loss=1.11 avg=1.02
[5174 | 28152.06] loss=0.85 avg=1.02
[5175 | 28157.38] loss=1.09 avg=1.02
[5176 | 28162.62] loss=0.88 avg=1.02
[5177 | 28167.92] loss=0.71 avg=1.02
[5178 | 28173.15] loss=1.02 avg=1.02
[5179 | 28178.32] loss=0.88 avg=1.02
[5180 | 28183.54] loss=1.04 avg=1.02
[5181 | 28188.74] loss=0.65 avg=1.01
[5182 | 28194.06] loss=0.98 avg=1.01
[5183 | 28199.25] loss=1.09 avg=1.01
[5184 | 28204.46] loss=1.04 avg=1.01
[5185 | 28209.73] loss=2.22 avg=1.03
[5186 | 28214.89] loss=0.50 avg=1.02
[5187 | 28220.10] loss=1.50 avg=1.02
[5188 | 28225.28] loss=0.73 avg=1.02
[5189 | 28230.42] loss=1.06 avg=1.02
[5190 | 28235.54] loss=0.99 avg=1.02
[5191 | 28240.72] loss=0.60 avg=1.02
[5192 | 28245.95] loss=1.20 avg=1.02
[5193 | 28251.20] loss=2.05 avg=1.03
[5194 | 28256.38] loss=1.46 avg=1.03
[5195 | 28261.61] loss=1.04 avg=1.03
[5196 | 28266.82] loss=0.59 avg=1.03
[5197 | 28272.07] loss=0.91 avg=1.03
[5198 | 28277.28] loss=0.90 avg=1.03
[5199 | 28282.49] loss=0.78 avg=1.02
Generating samples...
======== SAMPLE 1 ========
.net:~/s3djdxio/s3djdxio-dyno/dyno.py#L534-L547
def get_nucleotide_coenzyme_length(self, sequence):
        """Get a sequence of nucleotide coenzyme sequences, e.g., sequence(0a, 1a) and sequence(0a, 2a).
       

        :type Sequence: string
        :param sequence(self.strerror: No such sequence)
        :type (sequence, c1:1) sequence(sequence, c2:2)
        :param sequence(self.strerror: No such sequence)
        :type sequence(vector:vector)
        :param sequence(self.strerror: No such sequence)
        :type (seq, c1, c2, c3): string
        :return: Nucleotide coenzyme sequence length
        """
        for len, ncrsx, c1, c2, c3, s3, s4, c5, s6, d, d6, d7, c8, d7r, c8, d8r, c9, d9r, c9r, c10, d10r, d11, d11r, d12, d12r, d13, d13r, c10, d10r, d11r, d12r, d13r, c13r, d13r, c14r, d14r, c15r, c15r, c16r, c16rr, c16r, d8r, d8r, d8r, d8r, c8r, d8r, d8r, d9, d9r, d9r, c9, d9r, c9r, d11r, d11r, d11r, d12, d12r, d12r, c13, d13r, c13r, d13r, d13r, c13r, d12r, d12r, c14r, d14r, c14r, d14r, c15r, c15r, c15r, c16r, c16r, c16r, c16r, c15r, c16r, c16r, d8r, d8r, d8r, d8r, d8r, c7r, d7r, d7r, d7r, d7r, d6r, d6r, d6r, c8r, d8r, d8r, d8r, d8r, d8r, d8r, c8r, d8r, d8r, d8r, c9r, d9r, d9r, d9r, c9r, d11r, d11r, d1r, d1r, d1r, d1r, d1r, d11r, d11r, d12r, d12r, c9r, d10r, d10r, d11r, d12r, d12r, d12r, c11r, d12r, d12r, d11r, d12r, c13r, d13r, d13r, d13r, c13r, d13r, d13r, c15r, d15r, d15r, c15r, d15r, d15r, c16r, d16r, d16r, c15r, d16r, c16r, d16r, c16r, d17r, d17r, d17r, d17r, d17r, c8r, d8r, d8r, d8r, d8r, d8r, d8r, d8r, d10r, d10r, d10r, d11r, d11r, d11r, d11r, d12r, d12r, d12r, d12r, c13, d13r, d13r, d13r, d13r, d13r, c16r, d16r, d16r, d16r, d16r, c18

[5200 | 28370.39] loss=0.53 avg=1.02
[5201 | 28375.72] loss=0.87 avg=1.02
[5202 | 28381.05] loss=1.17 avg=1.02
[5203 | 28386.50] loss=0.92 avg=1.02
[5204 | 28391.69] loss=0.97 avg=1.02
[5205 | 28396.88] loss=1.05 avg=1.02
[5206 | 28402.04] loss=1.19 avg=1.02
[5207 | 28407.21] loss=1.08 avg=1.02
[5208 | 28412.33] loss=1.03 avg=1.02
[5209 | 28417.61] loss=0.90 avg=1.02
[5210 | 28422.88] loss=0.76 avg=1.02
[5211 | 28428.10] loss=0.71 avg=1.01
[5212 | 28433.24] loss=0.92 avg=1.01
[5213 | 28438.45] loss=0.74 avg=1.01
[5214 | 28443.61] loss=1.17 avg=1.01
[5215 | 28448.73] loss=0.96 avg=1.01
[5216 | 28454.06] loss=0.59 avg=1.01
[5217 | 28459.21] loss=0.90 avg=1.01
[5218 | 28464.38] loss=0.68 avg=1.00
[5219 | 28469.48] loss=2.07 avg=1.01
[5220 | 28474.78] loss=1.09 avg=1.01
[5221 | 28479.94] loss=0.78 avg=1.01
[5222 | 28485.13] loss=0.95 avg=1.01
[5223 | 28490.36] loss=1.09 avg=1.01
[5224 | 28495.59] loss=0.96 avg=1.01
[5225 | 28500.86] loss=1.01 avg=1.01
[5226 | 28506.09] loss=0.74 avg=1.01
[5227 | 28511.25] loss=1.01 avg=1.01
[5228 | 28516.45] loss=1.24 avg=1.01
[5229 | 28521.65] loss=0.61 avg=1.01
[5230 | 28526.93] loss=1.42 avg=1.01
[5231 | 28532.16] loss=1.04 avg=1.01
[5232 | 28537.37] loss=0.82 avg=1.01
[5233 | 28542.58] loss=1.12 avg=1.01
[5234 | 28547.78] loss=1.07 avg=1.01
[5235 | 28553.02] loss=0.68 avg=1.01
[5236 | 28558.18] loss=1.12 avg=1.01
[5237 | 28563.44] loss=1.15 avg=1.01
[5238 | 28568.63] loss=1.02 avg=1.01
[5239 | 28573.90] loss=1.33 avg=1.01
[5240 | 28579.06] loss=0.68 avg=1.01
[5241 | 28584.30] loss=0.77 avg=1.01
[5242 | 28589.47] loss=0.83 avg=1.01
[5243 | 28594.67] loss=1.42 avg=1.01
[5244 | 28599.88] loss=0.57 avg=1.01
[5245 | 28605.15] loss=1.49 avg=1.01
[5246 | 28610.38] loss=1.22 avg=1.01
[5247 | 28615.64] loss=1.16 avg=1.01
[5248 | 28620.93] loss=1.59 avg=1.02
[5249 | 28626.19] loss=0.59 avg=1.02
[5250 | 28631.39] loss=1.18 avg=1.02
[5251 | 28636.63] loss=0.66 avg=1.01
[5252 | 28641.88] loss=0.99 avg=1.01
[5253 | 28647.12] loss=1.20 avg=1.02
[5254 | 28652.41] loss=0.93 avg=1.01
[5255 | 28657.56] loss=0.99 avg=1.01
[5256 | 28662.82] loss=1.01 avg=1.01
[5257 | 28668.11] loss=0.71 avg=1.01
[5258 | 28673.37] loss=1.01 avg=1.01
[5259 | 28678.53] loss=1.03 avg=1.01
[5260 | 28683.66] loss=1.15 avg=1.01
[5261 | 28688.94] loss=1.25 avg=1.02
[5262 | 28694.17] loss=1.19 avg=1.02
[5263 | 28699.35] loss=1.25 avg=1.02
[5264 | 28704.63] loss=1.00 avg=1.02
[5265 | 28709.87] loss=1.97 avg=1.03
[5266 | 28715.11] loss=1.11 avg=1.03
[5267 | 28720.35] loss=1.44 avg=1.03
[5268 | 28725.59] loss=0.57 avg=1.03
[5269 | 28730.79] loss=1.18 avg=1.03
[5270 | 28736.08] loss=1.35 avg=1.03
[5271 | 28741.36] loss=1.18 avg=1.04
[5272 | 28746.62] loss=1.13 avg=1.04
[5273 | 28752.02] loss=1.70 avg=1.04
[5274 | 28757.30] loss=0.64 avg=1.04
[5275 | 28762.47] loss=0.59 avg=1.03
[5276 | 28767.71] loss=1.03 avg=1.03
[5277 | 28772.86] loss=1.46 avg=1.04
[5278 | 28778.09] loss=0.97 avg=1.04
[5279 | 28783.30] loss=0.88 avg=1.04
[5280 | 28788.53] loss=0.98 avg=1.04
[5281 | 28793.71] loss=0.34 avg=1.03
[5282 | 28798.98] loss=1.14 avg=1.03
[5283 | 28804.13] loss=1.29 avg=1.03
[5284 | 28809.30] loss=1.45 avg=1.04
[5285 | 28814.50] loss=0.60 avg=1.03
[5286 | 28819.72] loss=1.06 avg=1.03
[5287 | 28824.83] loss=1.76 avg=1.04
[5288 | 28830.05] loss=0.99 avg=1.04
[5289 | 28835.25] loss=0.71 avg=1.04
[5290 | 28840.60] loss=1.49 avg=1.04
[5291 | 28845.89] loss=1.48 avg=1.04
[5292 | 28851.32] loss=0.66 avg=1.04
[5293 | 28856.61] loss=0.75 avg=1.04
[5294 | 28861.69] loss=0.82 avg=1.04
[5295 | 28866.82] loss=2.19 avg=1.05
[5296 | 28872.14] loss=0.82 avg=1.05
[5297 | 28877.25] loss=0.91 avg=1.04
[5298 | 28882.45] loss=0.61 avg=1.04
[5299 | 28887.70] loss=1.32 avg=1.04
Generating samples...
======== SAMPLE 1 ========
                                         n_thresh_min_delta):
                               len (len(n_thresh_min_delta) - len(n_thresh_min_thresh_min_delta) # (len(n_thresh_min_delta) - len(n_thresh_min_thresh_min_thresh_min_delta)) * n_thresh_min_delta
                                in rtl_len(self.n_thresh_min_delta)
                                   fd = len(self.nr_thresh_min_delta)
                                 fd += len(self.nr_thresh_thresh_min_delta)
                                 return self.ndresh.rdresh_n_thresh / len(self.nr_thresh_thresh)
                                 return self._ndresh_ndresh_n_rdresh / len(self.nr_thresh_ndresh) [u'def', u'ndresh_ndresh', u'(', u'self', u')', u':', u'ln', u',', u'in_delta', u',', u'f', u'=', u'np', u'.', u'abs', u'(', u'f', u')', u'if', u'self', u'.', u'n_ndresh', u':', u'ln', u',', u'ln', u'.', u'ln_ndresh', u'(', u')', u'in', u'dev', u'=', u'np', u'.', u'dev', u'for', u'in_delta,' u,', u'f', u'in', u'in_rds', u':', u'ln', u',', u'ln', u'.', u'shape', u'=', u'(', u'[', u'ln', u',', u'in_delta', u',', u'ln', u']', u',', u'in_dd', u')', u'return', u'ln', u',', u'ln', u.', u'ln_n_ndresh', u'/', u'ln', u',', u'ln', u'.', u'nr_ndresh', u'/', u'ln', u',', u'ln', u'.', u'nr_ndresh', u'/', u'ln', u',', u'ln', u'.', u'nr_rdresh', u'/', u'ln', u',', u'dev', u'.', u'min', u'=', u'ln', u',', u'ln', u'.', u'in_delta', u',', u'ln', u't', u'.', u'in_rd', u',', u'return', u'ln', u',', u'ln', u'.', u'ln_ndresh', u',', u'ln', u'.', u'rdresh_n_rdresh', u'/', u'len', u'(', u'self', u'.', u'n_rdresh_ndresh', u')', u':', u'ln', u',', u'ln', u']', u'in_delta', u':', u'ln', u',', u'ln', u')', u'in_rds', u':', u'ln', u'.', u'in_rd_n', u'.', u'nr_ndresh', u'/', u'ln', u'.',

[5300 | 28979.77] loss=1.02 avg=1.04
[5301 | 28984.86] loss=1.23 avg=1.04
[5302 | 28990.15] loss=1.83 avg=1.05
[5303 | 28995.28] loss=0.80 avg=1.05
[5304 | 29000.47] loss=1.62 avg=1.06
[5305 | 29005.57] loss=0.73 avg=1.05
[5306 | 29010.78] loss=0.97 avg=1.05
[5307 | 29015.97] loss=1.39 avg=1.05
[5308 | 29021.16] loss=0.73 avg=1.05
[5309 | 29026.38] loss=0.81 avg=1.05
[5310 | 29031.67] loss=0.69 avg=1.05
[5311 | 29037.01] loss=1.55 avg=1.05
[5312 | 29042.16] loss=0.98 avg=1.05
[5313 | 29047.34] loss=1.48 avg=1.05
[5314 | 29052.49] loss=0.99 avg=1.05
[5315 | 29057.71] loss=0.78 avg=1.05
[5316 | 29063.05] loss=0.96 avg=1.05
[5317 | 29068.37] loss=1.06 avg=1.05
[5318 | 29073.54] loss=1.12 avg=1.05
[5319 | 29078.67] loss=1.46 avg=1.05
[5320 | 29083.80] loss=1.04 avg=1.05
[5321 | 29089.04] loss=0.83 avg=1.05
[5322 | 29094.26] loss=1.20 avg=1.05
[5323 | 29099.45] loss=1.01 avg=1.05
[5324 | 29104.56] loss=0.61 avg=1.05
[5325 | 29109.88] loss=0.83 avg=1.05
[5326 | 29115.16] loss=0.72 avg=1.04
[5327 | 29120.55] loss=1.32 avg=1.05
[5328 | 29125.84] loss=0.57 avg=1.04
[5329 | 29131.03] loss=0.79 avg=1.04
[5330 | 29136.31] loss=0.95 avg=1.04
[5331 | 29141.58] loss=1.30 avg=1.04
[5332 | 29146.84] loss=1.34 avg=1.04
[5333 | 29152.06] loss=1.16 avg=1.04
[5334 | 29157.37] loss=0.90 avg=1.04
[5335 | 29162.49] loss=0.54 avg=1.04
[5336 | 29167.80] loss=0.59 avg=1.03
[5337 | 29172.91] loss=1.40 avg=1.04
[5338 | 29178.12] loss=0.57 avg=1.03
[5339 | 29183.24] loss=0.82 avg=1.03
[5340 | 29188.43] loss=0.78 avg=1.03
[5341 | 29193.65] loss=1.22 avg=1.03
[5342 | 29198.91] loss=0.81 avg=1.03
[5343 | 29204.07] loss=1.43 avg=1.03
[5344 | 29209.33] loss=1.10 avg=1.03
[5345 | 29214.53] loss=1.14 avg=1.03
[5346 | 29219.81] loss=1.03 avg=1.03
[5347 | 29225.01] loss=0.97 avg=1.03
[5348 | 29230.21] loss=0.87 avg=1.03
[5349 | 29235.43] loss=0.75 avg=1.03
[5350 | 29240.76] loss=1.37 avg=1.03
[5351 | 29245.97] loss=1.56 avg=1.04
[5352 | 29251.14] loss=1.01 avg=1.04
[5353 | 29256.45] loss=0.96 avg=1.04
[5354 | 29261.71] loss=0.94 avg=1.04
[5355 | 29266.94] loss=1.19 avg=1.04
[5356 | 29272.23] loss=0.88 avg=1.04
[5357 | 29277.40] loss=0.90 avg=1.03
[5358 | 29282.46] loss=0.77 avg=1.03
[5359 | 29287.67] loss=1.07 avg=1.03
[5360 | 29292.94] loss=1.08 avg=1.03
[5361 | 29298.24] loss=1.34 avg=1.04
[5362 | 29303.57] loss=0.79 avg=1.03
[5363 | 29308.85] loss=1.25 avg=1.04
[5364 | 29314.14] loss=1.22 avg=1.04
[5365 | 29319.40] loss=1.66 avg=1.04
[5366 | 29324.62] loss=1.06 avg=1.04
[5367 | 29329.82] loss=1.29 avg=1.05
[5368 | 29335.07] loss=0.99 avg=1.05
[5369 | 29340.34] loss=1.15 avg=1.05
[5370 | 29345.55] loss=1.79 avg=1.05
[5371 | 29350.95] loss=1.58 avg=1.06
[5372 | 29356.23] loss=1.18 avg=1.06
[5373 | 29361.63] loss=1.24 avg=1.06
[5374 | 29366.86] loss=1.09 avg=1.06
[5375 | 29372.10] loss=1.25 avg=1.06
[5376 | 29377.37] loss=0.67 avg=1.06
[5377 | 29382.62] loss=1.25 avg=1.06
[5378 | 29387.92] loss=0.94 avg=1.06
[5379 | 29393.17] loss=1.34 avg=1.06
[5380 | 29398.34] loss=0.70 avg=1.06
[5381 | 29403.61] loss=0.62 avg=1.06
[5382 | 29408.95] loss=1.39 avg=1.06
[5383 | 29414.25] loss=0.99 avg=1.06
[5384 | 29419.54] loss=0.76 avg=1.06
[5385 | 29424.91] loss=1.00 avg=1.05
[5386 | 29430.09] loss=1.48 avg=1.06
[5387 | 29435.39] loss=1.08 avg=1.06
[5388 | 29440.70] loss=1.32 avg=1.06
[5389 | 29446.00] loss=1.13 avg=1.06
[5390 | 29451.27] loss=0.84 avg=1.06
[5391 | 29456.48] loss=1.70 avg=1.07
[5392 | 29461.73] loss=0.86 avg=1.06
[5393 | 29467.03] loss=0.80 avg=1.06
[5394 | 29472.27] loss=1.05 avg=1.06
[5395 | 29477.44] loss=1.04 avg=1.06
[5396 | 29482.65] loss=1.39 avg=1.06
[5397 | 29487.89] loss=0.84 avg=1.06
[5398 | 29493.14] loss=0.83 avg=1.06
[5399 | 29498.37] loss=1.08 avg=1.06
Generating samples...
======== SAMPLE 1 ========
                     raise ValueError('Invalid parameter(s).')

                     self.pydl_func.add_parameter(name, **kwargs)
                  return self

    # Get the parameters
    self._pydl_init_parameters = {}
    self._pydl_func = self._pydl_init_parameters [u'def', u'get_parameters', u'(', u'self', u',', u'Name', u'=', u'None', u')', u':', u'self', u'.', u'_pydl_func', u'=', u'self', u'.', u'_pydl_init_parameters', u'[', u'Name', u']', u'self', u'.', u'_pydl_func', u'=', u'self', u'.', u'_pydl_init_parameters'] Get the parameters [u'Get', u'the', u'parameters'] Parameter._pydl_init_parameters python def _pydl_init_parameters(self, Name=None):
    """Get the parameters
    """
    # get the parameters
    self._pydl_init_parameters = {}
    self._pydl_func = self._pydl_init_parameters train eol_v1a0_api/pydls.py ieljkad/eol-v1a0a0 bcd9f57dfc8f6d8d7e098bab7a3cf8f7bd6d https://github.com/iseljkad/eol-v1a0a0/blob/cd9f57dfc8f6d8d7e098bab7a3cf8f7bd6d/eol_v1a0_api/pydls.py#L1-L18
def _curl_with_url(self):
        """
        URL to be used on the `url`.

        @param url: URL to search for.
        @param list: list of objects.
        Note: If specified, the `list` will be used as a dictionary
            to find items.   Use the following to override it
            :return: [u'URL', u'to', u'be', u'used', u'on', u'the', u'url', u'to', u'store', u'.'] _curl_with_url python def _curl_with_url(self):
        """
        URL to be used on the `url`.

        @param url: URL to search for.
        @param list: list of objects.
        Note: If specified, the `list` will be used as a dictionary
            to find items.  Use the following to override it
            :return:
        """
        def url(self):
             self.url = self._curl_with_url(url.decode('utf8'))
             for key in self._items():
                  if key != '':
                        return
                          if self._cache_only(key.decoded()):
                                  

[5400 | 29584.56] loss=0.95 avg=1.06
[5401 | 29589.80] loss=0.43 avg=1.05
[5402 | 29595.14] loss=1.02 avg=1.05
[5403 | 29600.40] loss=1.16 avg=1.05
[5404 | 29605.67] loss=1.54 avg=1.06
[5405 | 29611.02] loss=1.47 avg=1.06
[5406 | 29616.21] loss=0.85 avg=1.06
[5407 | 29621.57] loss=0.60 avg=1.06
[5408 | 29626.97] loss=0.82 avg=1.05
[5409 | 29632.11] loss=1.00 avg=1.05
[5410 | 29637.41] loss=0.97 avg=1.05
[5411 | 29642.65] loss=1.04 avg=1.05
[5412 | 29647.88] loss=0.90 avg=1.05
[5413 | 29653.09] loss=1.12 avg=1.05
[5414 | 29658.26] loss=0.92 avg=1.05
[5415 | 29663.59] loss=1.34 avg=1.05
[5416 | 29668.78] loss=0.87 avg=1.05
[5417 | 29674.05] loss=0.88 avg=1.05
[5418 | 29679.25] loss=1.02 avg=1.05
[5419 | 29684.41] loss=0.97 avg=1.05
[5420 | 29689.65] loss=0.74 avg=1.05
[5421 | 29694.89] loss=0.89 avg=1.04
[5422 | 29700.10] loss=0.96 avg=1.04
[5423 | 29705.29] loss=1.05 avg=1.04
[5424 | 29710.65] loss=1.28 avg=1.05
[5425 | 29715.96] loss=1.61 avg=1.05
[5426 | 29721.22] loss=0.71 avg=1.05
[5427 | 29726.43] loss=1.16 avg=1.05
[5428 | 29731.63] loss=0.92 avg=1.05
[5429 | 29736.75] loss=1.84 avg=1.06
[5430 | 29741.85] loss=0.37 avg=1.05
[5431 | 29747.12] loss=0.56 avg=1.04
[5432 | 29752.34] loss=0.99 avg=1.04
[5433 | 29757.51] loss=0.89 avg=1.04
[5434 | 29762.75] loss=1.19 avg=1.04
[5435 | 29768.05] loss=0.75 avg=1.04
[5436 | 29773.28] loss=0.67 avg=1.04
[5437 | 29778.49] loss=0.81 avg=1.03
[5438 | 29783.72] loss=1.04 avg=1.03
[5439 | 29788.96] loss=1.15 avg=1.04
[5440 | 29794.19] loss=0.63 avg=1.03
[5441 | 29799.47] loss=1.15 avg=1.03
[5442 | 29804.80] loss=0.82 avg=1.03
[5443 | 29810.07] loss=1.50 avg=1.04
[5444 | 29815.18] loss=0.67 avg=1.03
[5445 | 29820.50] loss=2.07 avg=1.04
[5446 | 29825.75] loss=1.35 avg=1.05
[5447 | 29831.00] loss=0.84 avg=1.04
[5448 | 29836.27] loss=1.01 avg=1.04
[5449 | 29841.51] loss=0.68 avg=1.04
[5450 | 29846.91] loss=1.44 avg=1.04
[5451 | 29852.14] loss=1.08 avg=1.04
[5452 | 29857.46] loss=1.11 avg=1.04
[5453 | 29862.63] loss=1.08 avg=1.04
[5454 | 29867.81] loss=0.57 avg=1.04
[5455 | 29873.01] loss=0.94 avg=1.04
[5456 | 29878.26] loss=0.84 avg=1.04
[5457 | 29883.47] loss=0.65 avg=1.03
[5458 | 29888.71] loss=0.78 avg=1.03
[5459 | 29893.86] loss=1.06 avg=1.03
[5460 | 29899.12] loss=1.05 avg=1.03
[5461 | 29904.28] loss=0.85 avg=1.03
[5462 | 29909.48] loss=1.09 avg=1.03
[5463 | 29914.86] loss=1.16 avg=1.03
[5464 | 29920.08] loss=0.71 avg=1.03
[5465 | 29925.24] loss=0.86 avg=1.03
[5466 | 29930.44] loss=0.67 avg=1.02
[5467 | 29935.68] loss=0.64 avg=1.02
[5468 | 29940.98] loss=0.85 avg=1.02
[5469 | 29946.38] loss=0.75 avg=1.01
[5470 | 29951.68] loss=1.38 avg=1.02
[5471 | 29956.97] loss=1.19 avg=1.02
[5472 | 29962.36] loss=1.14 avg=1.02
[5473 | 29967.66] loss=1.38 avg=1.02
[5474 | 29972.93] loss=1.47 avg=1.03
[5475 | 29978.16] loss=0.50 avg=1.02
[5476 | 29983.40] loss=0.84 avg=1.02
[5477 | 29988.68] loss=0.64 avg=1.02
[5478 | 29993.96] loss=0.81 avg=1.02
[5479 | 29999.26] loss=1.05 avg=1.02
[5480 | 30004.56] loss=0.58 avg=1.01
[5481 | 30009.81] loss=1.17 avg=1.01
[5482 | 30015.06] loss=1.08 avg=1.01
[5483 | 30020.36] loss=0.90 avg=1.01
[5484 | 30025.62] loss=1.43 avg=1.02
[5485 | 30030.95] loss=0.94 avg=1.02
[5486 | 30036.18] loss=0.71 avg=1.01
[5487 | 30041.34] loss=1.10 avg=1.01
[5488 | 30046.57] loss=0.84 avg=1.01
[5489 | 30051.89] loss=1.64 avg=1.02
[5490 | 30057.25] loss=1.28 avg=1.02
[5491 | 30062.64] loss=1.44 avg=1.03
[5492 | 30067.93] loss=0.93 avg=1.02
[5493 | 30073.27] loss=1.23 avg=1.03
[5494 | 30078.55] loss=1.44 avg=1.03
[5495 | 30083.96] loss=1.14 avg=1.03
[5496 | 30089.14] loss=1.38 avg=1.04
[5497 | 30094.38] loss=1.29 avg=1.04
[5498 | 30099.61] loss=1.63 avg=1.04
[5499 | 30104.88] loss=1.23 avg=1.05
Generating samples...
======== SAMPLE 1 ========
,0) | | kwargs.substr('%s[A-Z]', kwargs)
             |                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     

[5500 | 30196.54] loss=1.02 avg=1.05
[5501 | 30201.76] loss=0.44 avg=1.04
[5502 | 30206.98] loss=0.78 avg=1.04
[5503 | 30212.27] loss=1.18 avg=1.04
[5504 | 30217.50] loss=1.15 avg=1.04
[5505 | 30222.74] loss=0.72 avg=1.04
[5506 | 30227.89] loss=1.10 avg=1.04
[5507 | 30233.01] loss=0.63 avg=1.03
[5508 | 30238.29] loss=0.91 avg=1.03
[5509 | 30243.46] loss=0.89 avg=1.03
[5510 | 30248.73] loss=0.90 avg=1.03
[5511 | 30253.96] loss=0.90 avg=1.03
[5512 | 30259.19] loss=0.68 avg=1.02
[5513 | 30264.48] loss=0.92 avg=1.02
[5514 | 30269.66] loss=1.09 avg=1.02
[5515 | 30274.86] loss=0.90 avg=1.02
[5516 | 30279.99] loss=0.95 avg=1.02
[5517 | 30285.29] loss=0.79 avg=1.02
[5518 | 30290.57] loss=0.58 avg=1.02
[5519 | 30295.80] loss=1.03 avg=1.02
[5520 | 30301.06] loss=0.74 avg=1.01
[5521 | 30306.23] loss=0.78 avg=1.01
[5522 | 30311.47] loss=0.42 avg=1.00
[5523 | 30316.72] loss=0.72 avg=1.00
[5524 | 30321.98] loss=0.79 avg=1.00
[5525 | 30327.28] loss=1.20 avg=1.00
[5526 | 30332.53] loss=1.15 avg=1.00
[5527 | 30337.72] loss=1.43 avg=1.01
[5528 | 30343.03] loss=0.71 avg=1.00
[5529 | 30348.33] loss=0.99 avg=1.00
[5530 | 30353.61] loss=0.92 avg=1.00
[5531 | 30358.89] loss=0.76 avg=1.00
[5532 | 30364.17] loss=0.83 avg=1.00
[5533 | 30369.42] loss=0.85 avg=1.00
[5534 | 30374.73] loss=1.05 avg=1.00
[5535 | 30380.08] loss=1.54 avg=1.00
[5536 | 30385.30] loss=0.66 avg=1.00
[5537 | 30390.62] loss=1.30 avg=1.00
[5538 | 30395.89] loss=1.48 avg=1.01
[5539 | 30401.11] loss=1.07 avg=1.01
[5540 | 30406.41] loss=1.15 avg=1.01
[5541 | 30411.75] loss=0.69 avg=1.01
[5542 | 30416.98] loss=0.90 avg=1.01
[5543 | 30422.40] loss=1.08 avg=1.01
[5544 | 30427.57] loss=0.99 avg=1.01
[5545 | 30432.82] loss=1.28 avg=1.01
[5546 | 30438.14] loss=0.66 avg=1.01
[5547 | 30443.35] loss=1.12 avg=1.01
[5548 | 30448.59] loss=1.15 avg=1.01
[5549 | 30453.83] loss=0.82 avg=1.01
[5550 | 30459.18] loss=1.11 avg=1.01
[5551 | 30464.64] loss=0.97 avg=1.01
[5552 | 30470.59] loss=0.96 avg=1.01
[5553 | 30475.91] loss=0.85 avg=1.00
[5554 | 30481.31] loss=1.89 avg=1.01
[5555 | 30486.50] loss=0.74 avg=1.01
[5556 | 30491.82] loss=0.83 avg=1.01
[5557 | 30497.07] loss=1.16 avg=1.01
[5558 | 30502.32] loss=0.94 avg=1.01
[5559 | 30507.67] loss=0.92 avg=1.01
[5560 | 30513.02] loss=0.70 avg=1.01
[5561 | 30518.55] loss=1.33 avg=1.01
[5562 | 30524.62] loss=1.02 avg=1.01
[5563 | 30530.45] loss=0.68 avg=1.01
[5564 | 30535.89] loss=0.81 avg=1.00
[5565 | 30541.39] loss=1.02 avg=1.00
[5566 | 30546.88] loss=0.87 avg=1.00
[5567 | 30552.60] loss=0.84 avg=1.00
[5568 | 30557.97] loss=0.51 avg=1.00
[5569 | 30563.64] loss=1.49 avg=1.00
[5570 | 30569.28] loss=0.81 avg=1.00
[5571 | 30575.07] loss=0.66 avg=1.00
[5572 | 30580.59] loss=0.79 avg=0.99
[5573 | 30585.82] loss=0.96 avg=0.99
[5574 | 30590.99] loss=0.69 avg=0.99
[5575 | 30596.55] loss=1.37 avg=0.99
[5576 | 30601.90] loss=2.00 avg=1.00
[5577 | 30607.20] loss=0.95 avg=1.00
[5578 | 30612.53] loss=0.73 avg=1.00
[5579 | 30617.87] loss=0.62 avg=1.00
[5580 | 30623.14] loss=0.72 avg=0.99
[5581 | 30628.38] loss=1.71 avg=1.00
[5582 | 30633.71] loss=0.69 avg=1.00
[5583 | 30639.02] loss=1.45 avg=1.00
[5584 | 30644.32] loss=0.70 avg=1.00
[5585 | 30649.61] loss=0.56 avg=1.00
[5586 | 30654.80] loss=1.39 avg=1.00
[5587 | 30660.08] loss=0.69 avg=1.00
[5588 | 30665.37] loss=0.54 avg=0.99
[5589 | 30670.62] loss=0.48 avg=0.99
[5590 | 30675.89] loss=1.20 avg=0.99
[5591 | 30681.12] loss=0.89 avg=0.99
[5592 | 30686.40] loss=0.92 avg=0.99
[5593 | 30691.68] loss=1.21 avg=0.99
[5594 | 30696.97] loss=1.91 avg=1.00
[5595 | 30702.26] loss=1.46 avg=1.00
[5596 | 30707.60] loss=1.31 avg=1.01
[5597 | 30712.86] loss=1.22 avg=1.01
[5598 | 30718.19] loss=0.82 avg=1.01
[5599 | 30723.48] loss=0.66 avg=1.00
Generating samples...
======== SAMPLE 1 ========
                                                                                                                            (0.0929732479, 13.06297224, 0.0929722479)))).

         # NOTE                                                        #                                    #                                              #                                                                                                                                        #                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          #                     

[5600 | 30812.01] loss=0.99 avg=1.00
[5601 | 30817.29] loss=0.55 avg=1.00
[5602 | 30822.54] loss=1.37 avg=1.00
[5603 | 30827.78] loss=0.67 avg=1.00
[5604 | 30833.15] loss=0.71 avg=1.00
[5605 | 30838.41] loss=1.31 avg=1.00
[5606 | 30843.64] loss=1.47 avg=1.00
[5607 | 30848.98] loss=0.69 avg=1.00
[5608 | 30854.22] loss=1.24 avg=1.00
[5609 | 30859.43] loss=0.96 avg=1.00
[5610 | 30864.64] loss=0.77 avg=1.00
[5611 | 30869.84] loss=0.77 avg=1.00
[5612 | 30874.97] loss=1.79 avg=1.01
[5613 | 30880.17] loss=1.18 avg=1.01
[5614 | 30885.50] loss=0.52 avg=1.00
[5615 | 30890.78] loss=1.20 avg=1.00
[5616 | 30896.05] loss=1.28 avg=1.01
[5617 | 30901.22] loss=1.22 avg=1.01
[5618 | 30906.44] loss=0.99 avg=1.01
[5619 | 30911.68] loss=1.37 avg=1.01
[5620 | 30916.83] loss=1.36 avg=1.02
[5621 | 30922.12] loss=1.29 avg=1.02
[5622 | 30927.20] loss=1.07 avg=1.02
[5623 | 30932.27] loss=0.72 avg=1.02
[5624 | 30937.36] loss=0.88 avg=1.02
[5625 | 30942.65] loss=1.25 avg=1.02
[5626 | 30947.80] loss=0.57 avg=1.01
[5627 | 30953.02] loss=0.92 avg=1.01
[5628 | 30958.33] loss=0.68 avg=1.01
[5629 | 30963.61] loss=1.10 avg=1.01
[5630 | 30968.85] loss=0.80 avg=1.01
[5631 | 30974.03] loss=1.20 avg=1.01
[5632 | 30979.24] loss=1.33 avg=1.01
[5633 | 30984.49] loss=1.07 avg=1.01
[5634 | 30989.73] loss=0.85 avg=1.01
[5635 | 30994.96] loss=0.76 avg=1.01
[5636 | 31000.25] loss=1.18 avg=1.01
[5637 | 31005.47] loss=1.36 avg=1.01
[5638 | 31010.67] loss=1.20 avg=1.02
[5639 | 31015.80] loss=0.85 avg=1.01
[5640 | 31021.01] loss=0.91 avg=1.01
[5641 | 31026.26] loss=0.83 avg=1.01
[5642 | 31031.46] loss=1.08 avg=1.01
[5643 | 31036.61] loss=0.98 avg=1.01
[5644 | 31041.86] loss=1.64 avg=1.02
[5645 | 31046.99] loss=0.63 avg=1.01
[5646 | 31052.24] loss=0.62 avg=1.01
[5647 | 31057.41] loss=0.75 avg=1.01
[5648 | 31062.61] loss=0.72 avg=1.01
[5649 | 31067.82] loss=0.71 avg=1.00
[5650 | 31073.03] loss=0.57 avg=1.00
[5651 | 31078.18] loss=1.27 avg=1.00
[5652 | 31083.35] loss=0.84 avg=1.00
[5653 | 31088.62] loss=1.25 avg=1.00
[5654 | 31093.82] loss=1.11 avg=1.00
[5655 | 31098.96] loss=0.82 avg=1.00
[5656 | 31104.25] loss=0.57 avg=1.00
[5657 | 31109.49] loss=1.11 avg=1.00
[5658 | 31114.65] loss=1.33 avg=1.00
[5659 | 31119.82] loss=0.55 avg=1.00
[5660 | 31125.03] loss=0.75 avg=0.99
[5661 | 31130.32] loss=1.52 avg=1.00
[5662 | 31135.52] loss=1.50 avg=1.00
[5663 | 31140.77] loss=1.36 avg=1.01
[5664 | 31146.06] loss=0.66 avg=1.00
[5665 | 31151.31] loss=0.98 avg=1.00
[5666 | 31156.56] loss=0.77 avg=1.00
[5667 | 31161.83] loss=0.99 avg=1.00
[5668 | 31167.07] loss=0.68 avg=1.00
[5669 | 31172.31] loss=1.13 avg=1.00
[5670 | 31177.56] loss=1.18 avg=1.00
[5671 | 31182.75] loss=1.20 avg=1.00
[5672 | 31187.95] loss=0.85 avg=1.00
[5673 | 31193.22] loss=1.09 avg=1.00
[5674 | 31198.47] loss=0.62 avg=1.00
[5675 | 31203.73] loss=1.30 avg=1.00
[5676 | 31209.01] loss=1.74 avg=1.01
[5677 | 31214.18] loss=0.71 avg=1.01
[5678 | 31219.41] loss=0.74 avg=1.00
[5679 | 31224.62] loss=1.01 avg=1.00
[5680 | 31229.82] loss=0.66 avg=1.00
[5681 | 31234.90] loss=1.18 avg=1.00
[5682 | 31240.05] loss=1.09 avg=1.00
[5683 | 31245.28] loss=1.07 avg=1.00
[5684 | 31250.44] loss=1.41 avg=1.01
[5685 | 31255.66] loss=1.01 avg=1.01
[5686 | 31260.88] loss=1.38 avg=1.01
[5687 | 31266.14] loss=0.69 avg=1.01
[5688 | 31271.43] loss=0.76 avg=1.01
[5689 | 31276.71] loss=0.69 avg=1.00
[5690 | 31281.99] loss=1.05 avg=1.00
[5691 | 31287.20] loss=1.15 avg=1.00
[5692 | 31292.44] loss=0.72 avg=1.00
[5693 | 31297.68] loss=0.86 avg=1.00
[5694 | 31302.89] loss=0.97 avg=1.00
[5695 | 31308.11] loss=0.99 avg=1.00
[5696 | 31313.42] loss=0.84 avg=1.00
[5697 | 31318.70] loss=1.00 avg=1.00
[5698 | 31324.06] loss=1.05 avg=1.00
[5699 | 31329.31] loss=0.87 avg=1.00
Generating samples...
======== SAMPLE 1 ========
_context, [], args: args - list of args
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        # A simple case for the value to be ignored, however we have to explicitly check the `values`` function, in this case we would raise the
                                                                                                                                                                                                                                                                                                                                                                                                            

[5700 | 31414.01] loss=0.96 avg=1.00
[5701 | 31419.30] loss=0.62 avg=0.99
[5702 | 31424.49] loss=0.79 avg=0.99
[5703 | 31429.66] loss=1.07 avg=0.99
[5704 | 31434.92] loss=0.62 avg=0.99
[5705 | 31440.06] loss=0.74 avg=0.99
[5706 | 31445.18] loss=1.37 avg=0.99
[5707 | 31450.36] loss=0.60 avg=0.99
[5708 | 31455.57] loss=1.99 avg=1.00
[5709 | 31460.74] loss=0.79 avg=0.99
[5710 | 31465.89] loss=0.77 avg=0.99
[5711 | 31471.11] loss=0.73 avg=0.99
[5712 | 31476.24] loss=0.69 avg=0.99
[5713 | 31481.42] loss=1.45 avg=0.99
[5714 | 31486.75] loss=0.95 avg=0.99
[5715 | 31492.05] loss=1.30 avg=0.99
[5716 | 31497.26] loss=0.76 avg=0.99
[5717 | 31502.41] loss=1.39 avg=1.00
[5718 | 31507.72] loss=0.49 avg=0.99
[5719 | 31512.93] loss=1.54 avg=1.00
[5720 | 31518.09] loss=0.67 avg=0.99
[5721 | 31523.27] loss=0.92 avg=0.99
[5722 | 31528.49] loss=1.30 avg=0.99
[5723 | 31533.66] loss=0.87 avg=0.99
[5724 | 31538.90] loss=0.57 avg=0.99
[5725 | 31544.04] loss=1.32 avg=0.99
[5726 | 31549.23] loss=0.77 avg=0.99
[5727 | 31554.43] loss=1.38 avg=0.99
[5728 | 31559.63] loss=0.97 avg=0.99
[5729 | 31564.85] loss=0.65 avg=0.99
[5730 | 31570.12] loss=0.84 avg=0.99
[5731 | 31575.40] loss=0.98 avg=0.99
[5732 | 31580.60] loss=1.75 avg=1.00
[5733 | 31585.82] loss=0.98 avg=1.00
[5734 | 31591.04] loss=0.66 avg=0.99
[5735 | 31596.24] loss=0.45 avg=0.99
[5736 | 31601.41] loss=0.68 avg=0.98
[5737 | 31606.68] loss=0.97 avg=0.98
[5738 | 31611.91] loss=0.80 avg=0.98
[5739 | 31617.18] loss=1.50 avg=0.99
[5740 | 31622.36] loss=0.80 avg=0.99
[5741 | 31627.58] loss=2.13 avg=1.00
[5742 | 31632.83] loss=1.42 avg=1.00
[5743 | 31638.06] loss=0.61 avg=1.00
[5744 | 31643.28] loss=0.52 avg=0.99
[5745 | 31648.41] loss=1.55 avg=1.00
[5746 | 31653.56] loss=1.20 avg=1.00
[5747 | 31658.89] loss=1.11 avg=1.00
[5748 | 31664.13] loss=1.00 avg=1.00
[5749 | 31669.25] loss=0.90 avg=1.00
[5750 | 31674.46] loss=1.48 avg=1.01
[5751 | 31679.69] loss=0.89 avg=1.00
[5752 | 31684.92] loss=0.69 avg=1.00
[5753 | 31690.14] loss=1.10 avg=1.00
[5754 | 31695.37] loss=0.73 avg=1.00
[5755 | 31700.67] loss=1.16 avg=1.00
[5756 | 31705.91] loss=0.80 avg=1.00
[5757 | 31711.17] loss=1.10 avg=1.00
[5758 | 31716.43] loss=0.72 avg=1.00
[5759 | 31721.63] loss=1.81 avg=1.01
[5760 | 31726.87] loss=0.73 avg=1.00
[5761 | 31732.10] loss=2.22 avg=1.01
[5762 | 31737.29] loss=1.33 avg=1.02
[5763 | 31742.58] loss=1.01 avg=1.02
[5764 | 31747.91] loss=1.24 avg=1.02
[5765 | 31753.11] loss=1.07 avg=1.02
[5766 | 31758.32] loss=0.79 avg=1.02
[5767 | 31763.59] loss=0.69 avg=1.01
[5768 | 31768.80] loss=0.57 avg=1.01
[5769 | 31773.98] loss=0.50 avg=1.01
[5770 | 31779.16] loss=1.58 avg=1.01
[5771 | 31784.41] loss=1.20 avg=1.01
[5772 | 31789.58] loss=0.74 avg=1.01
[5773 | 31794.78] loss=1.12 avg=1.01
[5774 | 31799.99] loss=0.70 avg=1.01
[5775 | 31805.25] loss=1.26 avg=1.01
[5776 | 31810.55] loss=0.81 avg=1.01
[5777 | 31815.84] loss=1.20 avg=1.01
[5778 | 31821.11] loss=1.17 avg=1.01
[5779 | 31826.28] loss=0.49 avg=1.01
[5780 | 31831.31] loss=1.46 avg=1.01
[5781 | 31836.31] loss=1.52 avg=1.02
[5782 | 31841.42] loss=1.40 avg=1.02
[5783 | 31846.51] loss=0.89 avg=1.02
[5784 | 31851.67] loss=0.96 avg=1.02
[5785 | 31856.70] loss=1.44 avg=1.02
[5786 | 31861.68] loss=0.96 avg=1.02
[5787 | 31866.81] loss=1.08 avg=1.02
[5788 | 31871.89] loss=1.09 avg=1.02
[5789 | 31877.08] loss=0.97 avg=1.02
[5790 | 31882.26] loss=0.69 avg=1.02
[5791 | 31887.41] loss=1.08 avg=1.02
[5792 | 31892.56] loss=0.95 avg=1.02
[5793 | 31897.69] loss=0.78 avg=1.02
[5794 | 31902.90] loss=0.76 avg=1.01
[5795 | 31908.01] loss=1.12 avg=1.02
[5796 | 31913.12] loss=0.71 avg=1.01
[5797 | 31918.15] loss=1.26 avg=1.01
[5798 | 31923.28] loss=0.45 avg=1.01
[5799 | 31928.32] loss=1.01 avg=1.01
Generating samples...
======== SAMPLE 1 ========
', u'add', u'src', u'(', u"~%s.b'", u'-', u'+', u'%', u'nodes', u')', u'.', u'select', u'(', u')', u'# remove nodes if possible', u'# remove', u'if', u'nodes', u'==', u'None', u':', u'# reattach to the node to be destroyed if possible', u'delegate', u'=', u'node', u'(', u'*', u'nodes', u',', u'*', u'delegate', u')', u'd', u'=', u'delegate', u'.', u'get_node', u'(', u'*', u'delegate', u',', u'*', u'd', u',', u"'.'", u',', u'd', u')', u'# if we find a 'node'', u'//', u'# 'b'', u'module', u'[', u"'b'", u']', u'=', u'delegate', u'# update for replacement', u'srcfile', u'=', u'delegate', u'.', u'find_subprocessor_update', u'(', u'srcfile', u',', u'srcfile', u')', u'for', u'node', u'in', u'srcfile', u':', u'd', u'=', u'module', u'<='', u'(', u'node', u',', u'0', u',', u'0', u')', u'<', u'(', u'd', u'+', u'(', u'nodes', u'-', u'*', u'(', u'e', u'+', u'src', u',', u't', u')', u')', u')', u'<', u'(', u'd', u'+', u'(', u'nodes', u'-', u'*', u'(', u'e', u'+', u'd', u',', u't', u')', u')', u')', u':', u',', u'# set new node for replacement', u'srcfolder', u'=', u'scorefile', u'(', u'd', u',', u'srcfolder', u')', u'if', u'scorefile', u':', u'srcfolder', u'=', u'scorefile', u'(', u'd', u',', u'srcfolder', u')', u'# clean up nodes', u'delegate', u'=', u'node', u'(', u'*', u'delegate', u',', u'*', u'delegate', u')', u'd_nodes', u'[', u':', u'-', u'*', u'delegate', u',', u':', u'd', u']'] Execute an update from node to the replacement node [u'Execute', u'an', u'update', u'from', u'node', u'to', u'the', u'rearer', u'node'] replace_node python def replace_node(self, node):
    """Execute an update from node to the replacement node
    """
    node = self.rename_node()
    from node import Replaceable,
    from node import New,
    from node import Deleted
    from node import Remade,
    from node import Tree
    from node import Deleted
    from tree import Tree(Deleted, New)

    def replace_nodes(self):
        # set new node for replacement
        return self._rename_nodes_from_node(node)
        return Node(node)

    return Node([str(self.nodes) for str(self.node), "", self.nodes]) train the_update.py The_update.a2/python 3ba0c2a7a2a3799aee0c9d9c07d9fb5b3d0d6b8df3 https://github.com/The_update/python/blob/3ba0c2a7a2a3799aee0c9d9c07d9fb5b3d0d6b8df3/the

[5800 | 32014.90] loss=1.55 avg=1.01
[5801 | 32019.99] loss=1.49 avg=1.02
[5802 | 32025.03] loss=1.07 avg=1.02
[5803 | 32030.05] loss=0.88 avg=1.02
[5804 | 32035.06] loss=0.37 avg=1.01
[5805 | 32040.13] loss=1.23 avg=1.01
[5806 | 32045.22] loss=0.94 avg=1.01
[5807 | 32050.29] loss=1.02 avg=1.01
[5808 | 32055.39] loss=0.76 avg=1.01
[5809 | 32060.44] loss=0.63 avg=1.01
[5810 | 32065.45] loss=0.90 avg=1.01
[5811 | 32070.51] loss=1.09 avg=1.01
[5812 | 32075.58] loss=0.77 avg=1.00
[5813 | 32080.65] loss=0.63 avg=1.00
[5814 | 32085.72] loss=0.98 avg=1.00
[5815 | 32090.80] loss=0.79 avg=1.00
[5816 | 32095.91] loss=1.49 avg=1.00
[5817 | 32100.91] loss=0.60 avg=1.00
[5818 | 32106.04] loss=1.15 avg=1.00
[5819 | 32111.09] loss=0.90 avg=1.00
[5820 | 32116.24] loss=0.95 avg=1.00
[5821 | 32121.32] loss=1.04 avg=1.00
[5822 | 32126.41] loss=0.98 avg=1.00
[5823 | 32131.43] loss=0.76 avg=1.00
[5824 | 32136.51] loss=0.62 avg=0.99
[5825 | 32141.57] loss=1.36 avg=1.00
[5826 | 32146.68] loss=1.03 avg=1.00
[5827 | 32151.86] loss=1.80 avg=1.01
[5828 | 32156.91] loss=1.30 avg=1.01
[5829 | 32161.93] loss=1.60 avg=1.01
[5830 | 32167.08] loss=0.69 avg=1.01
[5831 | 32172.22] loss=1.25 avg=1.01
[5832 | 32177.34] loss=1.03 avg=1.01
[5833 | 32182.50] loss=0.78 avg=1.01
[5834 | 32187.60] loss=0.92 avg=1.01
[5835 | 32192.70] loss=1.18 avg=1.01
[5836 | 32197.85] loss=2.26 avg=1.02
[5837 | 32202.97] loss=0.92 avg=1.02
[5838 | 32208.05] loss=0.99 avg=1.02
[5839 | 32213.16] loss=0.63 avg=1.02
[5840 | 32218.29] loss=0.71 avg=1.02
[5841 | 32223.41] loss=0.83 avg=1.01
[5842 | 32228.46] loss=0.80 avg=1.01
[5843 | 32233.56] loss=0.50 avg=1.01
[5844 | 32238.62] loss=0.88 avg=1.01
[5845 | 32243.76] loss=0.82 avg=1.00
[5846 | 32248.92] loss=1.14 avg=1.00
[5847 | 32254.02] loss=1.40 avg=1.01
[5848 | 32259.05] loss=1.28 avg=1.01
[5849 | 32264.14] loss=1.15 avg=1.01
[5850 | 32269.19] loss=1.06 avg=1.01
[5851 | 32274.21] loss=1.10 avg=1.01
[5852 | 32279.38] loss=1.10 avg=1.02
[5853 | 32284.51] loss=1.45 avg=1.02
[5854 | 32289.60] loss=1.16 avg=1.02
[5855 | 32294.58] loss=0.75 avg=1.02
[5856 | 32299.66] loss=0.58 avg=1.01
[5857 | 32304.72] loss=0.66 avg=1.01
[5858 | 32309.70] loss=1.11 avg=1.01
[5859 | 32314.77] loss=0.94 avg=1.01
[5860 | 32319.85] loss=1.28 avg=1.01
[5861 | 32324.95] loss=0.92 avg=1.01
[5862 | 32330.03] loss=0.70 avg=1.01
[5863 | 32335.01] loss=0.59 avg=1.00
[5864 | 32340.05] loss=0.97 avg=1.00
[5865 | 32345.12] loss=0.91 avg=1.00
[5866 | 32350.23] loss=1.29 avg=1.01
[5867 | 32355.35] loss=1.07 avg=1.01
[5868 | 32360.38] loss=0.89 avg=1.01
[5869 | 32365.37] loss=0.79 avg=1.00
[5870 | 32370.40] loss=0.94 avg=1.00
[5871 | 32375.48] loss=0.59 avg=1.00
[5872 | 32380.57] loss=1.22 avg=1.00
[5873 | 32385.58] loss=0.73 avg=1.00
[5874 | 32390.62] loss=1.15 avg=1.00
[5875 | 32395.67] loss=1.11 avg=1.00
[5876 | 32400.73] loss=1.45 avg=1.01
[5877 | 32405.88] loss=0.86 avg=1.00
[5878 | 32410.90] loss=0.64 avg=1.00
[5879 | 32416.01] loss=0.98 avg=1.00
[5880 | 32421.20] loss=1.15 avg=1.00
[5881 | 32426.27] loss=1.03 avg=1.00
[5882 | 32431.29] loss=0.77 avg=1.00
[5883 | 32436.33] loss=0.81 avg=1.00
[5884 | 32441.42] loss=0.39 avg=0.99
[5885 | 32446.46] loss=1.06 avg=0.99
[5886 | 32451.53] loss=0.71 avg=0.99
[5887 | 32456.54] loss=1.57 avg=1.00
[5888 | 32461.63] loss=0.92 avg=0.99
[5889 | 32466.71] loss=1.52 avg=1.00
[5890 | 32471.84] loss=0.83 avg=1.00
[5891 | 32477.04] loss=0.58 avg=0.99
[5892 | 32482.20] loss=0.79 avg=0.99
[5893 | 32487.36] loss=1.75 avg=1.00
[5894 | 32492.51] loss=0.51 avg=0.99
[5895 | 32497.56] loss=1.21 avg=1.00
[5896 | 32502.63] loss=0.78 avg=0.99
[5897 | 32507.70] loss=1.48 avg=1.00
[5898 | 32512.86] loss=0.86 avg=1.00
[5899 | 32517.89] loss=0.60 avg=0.99
Generating samples...
======== SAMPLE 1 ========
        # The file path will be added after every file
                    # in the path
                     # and add with the new path
                      pathname = "".join_old_path(self.file_path, pathname, filename)
                if None:
                    if path is not None:
                       if filename:
                            path_name = self.pathname
                  else:
                        path_name = pathname
                               path_path = pathname [u'def', u'open_file', u'(', u'self', u',', u'pathname', u',', u'file', u'=', u'"path"}', u',', u'not', u'None', u',', u'not', u'False', u')', u':', u'if', u'None', u':', u'if', u'not', u'not', u'not', u'not', u'not', u'not', u'path', u':', u'break', u'if', u'not', u'None', u':', u'if', u'path', u'is', u'not', u'None', u':', u'if', u'not', u'not', u'not', u'not', u'path', u'.', u'add', u'(', u')', u'break', u'if', u'not', u'not', u'not', u'not', u'self', u'.', u'_path', u'.', u'is_null', u':', u'# if None else', u'return', u'return', u'len_paths', u'(', u'self', u'.', u'file_path', u')', u'.', u'append', u'(', u'set_content', u')'] Open the file
            and return it.
            if not path:
               return self .file_path
            else:
               return self [u'def', u'open', u'(', u'self', u',', u'pathname', u'=', u'"".join_old_path", u'(', u')', u')', u':', u'if', u'not', u'not', u'not', u'not', u'three', u':', u'# The file path will be added after every file', u'# in the path', u'# In the path', u'# # and add with the new path', u'# pathname', u'=', u'"".join_old_path', u'(', u'self', u'.', u'file_path', u',', u'pathname', u',', u'filename', u')', u'if', u'None', u':', u'if', u'not', u'not', u'not', u'three', u':', u'path_path', u'=', u'self', u'.', u'pathname', u'else', u':', u'if', u'path', u'is', u'not', u'None', u':', u'if', u'not', u'not', u'not', u'not', u'path', u'.', u'add', u'(', u')', u'break', u'if', u'not', u'not', u'not

[5900 | 32604.37] loss=1.27 avg=1.00
[5901 | 32609.38] loss=0.81 avg=1.00
[5902 | 32614.42] loss=0.56 avg=0.99
[5903 | 32619.42] loss=0.77 avg=0.99
[5904 | 32624.49] loss=0.49 avg=0.98
[5905 | 32629.49] loss=0.84 avg=0.98
[5906 | 32634.47] loss=0.66 avg=0.98
[5907 | 32639.57] loss=1.44 avg=0.98
[5908 | 32644.56] loss=1.42 avg=0.99
[5909 | 32649.61] loss=1.61 avg=0.99
[5910 | 32654.68] loss=1.21 avg=1.00
[5911 | 32659.69] loss=0.59 avg=0.99
[5912 | 32664.76] loss=0.87 avg=0.99
[5913 | 32669.85] loss=0.72 avg=0.99
[5914 | 32674.89] loss=0.78 avg=0.99
[5915 | 32679.95] loss=1.19 avg=0.99
[5916 | 32685.04] loss=1.42 avg=0.99
[5917 | 32690.07] loss=0.86 avg=0.99
[5918 | 32695.11] loss=1.01 avg=0.99
[5919 | 32700.16] loss=0.74 avg=0.99
[5920 | 32705.16] loss=0.82 avg=0.99
[5921 | 32710.17] loss=0.87 avg=0.99
[5922 | 32715.35] loss=0.77 avg=0.98
[5923 | 32720.44] loss=1.36 avg=0.99
[5924 | 32725.45] loss=1.73 avg=0.99
[5925 | 32730.53] loss=0.95 avg=0.99
[5926 | 32735.70] loss=1.29 avg=1.00
[5927 | 32740.82] loss=0.96 avg=1.00
[5928 | 32745.86] loss=1.13 avg=1.00
[5929 | 32750.97] loss=1.31 avg=1.00
[5930 | 32756.02] loss=1.33 avg=1.00
[5931 | 32761.14] loss=1.74 avg=1.01
[5932 | 32766.29] loss=1.09 avg=1.01
[5933 | 32771.31] loss=0.82 avg=1.01
[5934 | 32776.40] loss=0.84 avg=1.01
[5935 | 32781.50] loss=1.58 avg=1.02
[5936 | 32786.56] loss=1.86 avg=1.02
[5937 | 32791.67] loss=0.78 avg=1.02
[5938 | 32796.80] loss=1.11 avg=1.02
[5939 | 32801.89] loss=0.51 avg=1.02
[5940 | 32807.03] loss=0.86 avg=1.02
[5941 | 32812.12] loss=0.75 avg=1.01
[5942 | 32817.22] loss=0.82 avg=1.01
[5943 | 32822.30] loss=0.95 avg=1.01
[5944 | 32827.39] loss=0.84 avg=1.01
[5945 | 32832.45] loss=1.15 avg=1.01
[5946 | 32837.55] loss=1.10 avg=1.01
[5947 | 32842.59] loss=0.48 avg=1.01
[5948 | 32847.57] loss=0.75 avg=1.00
[5949 | 32852.54] loss=1.93 avg=1.01
[5950 | 32857.53] loss=1.14 avg=1.01
[5951 | 32862.54] loss=0.75 avg=1.01
[5952 | 32867.51] loss=1.60 avg=1.02
[5953 | 32872.51] loss=1.48 avg=1.02
[5954 | 32877.54] loss=0.52 avg=1.02
[5955 | 32882.70] loss=0.73 avg=1.01
[5956 | 32887.73] loss=1.16 avg=1.01
[5957 | 32892.76] loss=1.15 avg=1.02
[5958 | 32897.83] loss=1.11 avg=1.02
[5959 | 32903.01] loss=1.05 avg=1.02
[5960 | 32908.14] loss=0.49 avg=1.01
[5961 | 32913.25] loss=0.72 avg=1.01
[5962 | 32918.30] loss=0.63 avg=1.01
[5963 | 32923.33] loss=1.18 avg=1.01
[5964 | 32928.38] loss=0.72 avg=1.00
[5965 | 32933.47] loss=0.85 avg=1.00
[5966 | 32938.49] loss=1.03 avg=1.00
[5967 | 32943.50] loss=0.95 avg=1.00
[5968 | 32948.54] loss=1.11 avg=1.00
[5969 | 32953.60] loss=0.89 avg=1.00
[5970 | 32958.69] loss=0.94 avg=1.00
[5971 | 32963.76] loss=0.88 avg=1.00
[5972 | 32968.88] loss=1.12 avg=1.00
[5973 | 32974.03] loss=0.60 avg=1.00
[5974 | 32979.12] loss=1.06 avg=1.00
[5975 | 32984.27] loss=1.64 avg=1.00
[5976 | 32989.32] loss=0.56 avg=1.00
[5977 | 32994.35] loss=1.11 avg=1.00
[5978 | 32999.43] loss=1.05 avg=1.00
[5979 | 33004.48] loss=3.70 avg=1.03
[5980 | 33009.48] loss=1.30 avg=1.03
[5981 | 33014.46] loss=1.05 avg=1.03
[5982 | 33019.48] loss=1.23 avg=1.03
[5983 | 33024.65] loss=0.56 avg=1.03
[5984 | 33029.78] loss=1.23 avg=1.03
[5985 | 33034.85] loss=0.79 avg=1.03
[5986 | 33039.93] loss=1.51 avg=1.03
[5987 | 33044.94] loss=0.61 avg=1.03
[5988 | 33050.09] loss=0.78 avg=1.03
[5989 | 33055.12] loss=0.73 avg=1.02
[5990 | 33060.15] loss=0.74 avg=1.02
[5991 | 33065.25] loss=0.90 avg=1.02
[5992 | 33070.31] loss=0.95 avg=1.02
[5993 | 33075.33] loss=0.95 avg=1.02
[5994 | 33080.35] loss=0.86 avg=1.02
[5995 | 33085.33] loss=0.85 avg=1.02
[5996 | 33090.27] loss=1.02 avg=1.02
[5997 | 33095.34] loss=1.32 avg=1.02
[5998 | 33100.41] loss=1.16 avg=1.02
[5999 | 33105.47] loss=1.07 avg=1.02
Saving checkpoint/run1/model-6000
Generating samples...
======== SAMPLE 1 ========
*(.',
    for i in range(0, 2):  # 2 = the starting frame and
     # (9.67) = zero points on the top, and -1 =
    # the edge of the frame.
    for frame in os.path.join(slices(x), os.path.join(elements)):
        axis_name = os.os.path.expanduser(dnd(x))
        for item, item_size in item_name_map[len(x)]:
            assert axis_name[0], item=item_size
            assert not os.path.exists(os.path.join([',', axis_name, item]])) [u'def', u'on_frame', u'(', u'self', u',', u'x', u',', u'y', u',', u'elements', u'=', u'None,', u'start', u'=', u'None', u',', u'end', u'=', u'False', u',', u'is_end', u'=', u'True', u')', u':', u'self', u'.', u'_on_frame', u'(', u'x', u',', u'y', u',', u'elements', u'=', u'None', u',', u'start', u'=', u'False', u',', u'end', u'=', u'True', u',', u'is_end', u'=', u'False', u')', u':', u'if', u'x', u'is', u'None', u'and', u'y', u'is', u'None', u':', u'x', u'=', u'None', u'y', u'=', u'None', u'axis', u'=', u'=', u"'-1"', u'elif', u'y', u'is', u'None', u'and', u'start', u'in', u'y', u'and', u'y', u'in', u'seed', u'and', u'axis_name', u':', u'if', u'is_end', u':', u'axis_names', u'+=', u'start', u'elif', u'x', u'is', u'None', u'and', u'y', u'is', u'None', u':', u'x', u'=', u'None', u'y', u'=', u'None', u'axis_name', u'=', u'os', u'.', u'path', u'.', u'abspath', u'(', u'x', u')', u'-', u'return', u'axis_names', u'for', u'module', u'in', u'self', u'.', u'_on_frame', u'(', u')', u':', u'if', u'module', u'[', u':', u'axis_name', u'is', u'None', u']', u':', u'axis', u'[', u'axis', u'=', u'"-"', u'+', u'axis_name', u'+', u'"-1"', u']', u'database', u'=', u'os', u'.', u'path.abspath', u'(', u'os', u'.', u'path', u'.', u'join', u'(', u'module', u')', u')', u'train_map', u'=', u'module', u'[', u':', u'axis', u'[', u'axis', u'=', u'"-"', u'+', u'axis_name', u'+', u"-'", u']', u':', u'elif', u'train_map', u':', u'elif', u'train_map', u'[', u':', u'axis_name', u'*', u'(', u"''", u'+', u'train_map', u'[', u'-', u'axis', u'+', u"'-'', u'+', u'(', u'axis',

[6000 | 33191.95] loss=1.20 avg=1.02
[6001 | 33196.97] loss=1.76 avg=1.03
[6002 | 33201.99] loss=0.76 avg=1.03
[6003 | 33206.93] loss=1.23 avg=1.03
[6004 | 33211.88] loss=0.54 avg=1.02
[6005 | 33216.84] loss=1.12 avg=1.02
[6006 | 33221.79] loss=1.39 avg=1.03
[6007 | 33226.74] loss=0.65 avg=1.02
[6008 | 33231.77] loss=1.13 avg=1.03
[6009 | 33236.92] loss=0.85 avg=1.02
[6010 | 33241.94] loss=1.20 avg=1.03
[6011 | 33246.94] loss=0.88 avg=1.02
[6012 | 33251.99] loss=1.08 avg=1.02
[6013 | 33256.99] loss=1.78 avg=1.03
[6014 | 33262.03] loss=0.79 avg=1.03
[6015 | 33267.11] loss=1.35 avg=1.03
[6016 | 33272.11] loss=0.58 avg=1.03
[6017 | 33277.08] loss=0.75 avg=1.03
[6018 | 33282.09] loss=0.94 avg=1.02
[6019 | 33287.15] loss=1.24 avg=1.03
[6020 | 33292.17] loss=1.39 avg=1.03
[6021 | 33297.09] loss=1.19 avg=1.03
[6022 | 33302.05] loss=1.25 avg=1.03
[6023 | 33307.05] loss=0.91 avg=1.03
[6024 | 33312.07] loss=0.78 avg=1.03
[6025 | 33317.10] loss=1.08 avg=1.03
[6026 | 33322.14] loss=1.51 avg=1.04
[6027 | 33327.18] loss=0.75 avg=1.03
[6028 | 33332.25] loss=0.98 avg=1.03
[6029 | 33337.26] loss=0.86 avg=1.03
[6030 | 33342.27] loss=0.82 avg=1.03
[6031 | 33347.34] loss=0.86 avg=1.03
[6032 | 33352.47] loss=1.36 avg=1.03
[6033 | 33357.50] loss=0.86 avg=1.03
[6034 | 33362.49] loss=0.69 avg=1.03
[6035 | 33367.44] loss=1.22 avg=1.03
[6036 | 33372.49] loss=1.05 avg=1.03
[6037 | 33377.47] loss=1.47 avg=1.03
[6038 | 33382.53] loss=0.59 avg=1.03
[6039 | 33387.57] loss=0.78 avg=1.02
[6040 | 33392.70] loss=0.81 avg=1.02
[6041 | 33397.67] loss=1.02 avg=1.02
[6042 | 33402.72] loss=0.51 avg=1.02
[6043 | 33407.81] loss=0.52 avg=1.01
[6044 | 33412.85] loss=0.79 avg=1.01
[6045 | 33417.92] loss=0.68 avg=1.01
[6046 | 33422.93] loss=1.06 avg=1.01
[6047 | 33427.99] loss=1.07 avg=1.01
[6048 | 33432.97] loss=0.64 avg=1.00
[6049 | 33438.06] loss=0.90 avg=1.00
[6050 | 33443.09] loss=0.98 avg=1.00
[6051 | 33448.17] loss=0.71 avg=1.00
[6052 | 33453.24] loss=0.55 avg=1.00
[6053 | 33458.44] loss=1.48 avg=1.00
[6054 | 33463.45] loss=1.15 avg=1.00
[6055 | 33468.59] loss=1.18 avg=1.00
[6056 | 33473.65] loss=0.90 avg=1.00
[6057 | 33478.68] loss=0.73 avg=1.00
[6058 | 33483.77] loss=1.11 avg=1.00
[6059 | 33488.86] loss=1.27 avg=1.00
[6060 | 33493.95] loss=1.38 avg=1.01
[6061 | 33499.00] loss=1.48 avg=1.01
[6062 | 33504.19] loss=0.84 avg=1.01
[6063 | 33509.22] loss=1.04 avg=1.01
[6064 | 33514.28] loss=0.87 avg=1.01
[6065 | 33519.40] loss=0.98 avg=1.01
[6066 | 33524.48] loss=1.11 avg=1.01
[6067 | 33529.56] loss=1.00 avg=1.01
[6068 | 33534.65] loss=1.26 avg=1.01
[6069 | 33539.70] loss=2.03 avg=1.02
[6070 | 33544.82] loss=0.98 avg=1.02
[6071 | 33549.87] loss=1.08 avg=1.02
[6072 | 33554.96] loss=1.83 avg=1.03
[6073 | 33559.96] loss=0.73 avg=1.03
[6074 | 33564.98] loss=0.75 avg=1.03
[6075 | 33570.01] loss=0.87 avg=1.02
[6076 | 33575.15] loss=1.27 avg=1.03
[6077 | 33580.22] loss=1.15 avg=1.03
[6078 | 33585.30] loss=0.82 avg=1.03
[6079 | 33590.41] loss=0.92 avg=1.02
[6080 | 33595.57] loss=1.10 avg=1.03
[6081 | 33600.71] loss=0.90 avg=1.02
[6082 | 33605.82] loss=1.45 avg=1.03
[6083 | 33610.96] loss=1.11 avg=1.03
[6084 | 33616.09] loss=1.14 avg=1.03
[6085 | 33621.20] loss=0.74 avg=1.03
[6086 | 33626.31] loss=1.17 avg=1.03
[6087 | 33631.51] loss=0.99 avg=1.03
[6088 | 33636.59] loss=1.24 avg=1.03
[6089 | 33641.75] loss=0.86 avg=1.03
[6090 | 33646.86] loss=1.09 avg=1.03
[6091 | 33651.90] loss=0.96 avg=1.03
[6092 | 33656.92] loss=0.63 avg=1.02
[6093 | 33661.89] loss=0.53 avg=1.02
[6094 | 33667.00] loss=1.03 avg=1.02
[6095 | 33672.09] loss=1.39 avg=1.02
[6096 | 33677.16] loss=1.57 avg=1.03
[6097 | 33682.26] loss=0.59 avg=1.02
[6098 | 33687.36] loss=1.32 avg=1.03
[6099 | 33692.44] loss=1.18 avg=1.03
Generating samples...
======== SAMPLE 1 ========
('r' =>
                    'DELAY, 0.01)', 'REPLACE_STARTED, 'REVERT'
                  # Only run after stopping with `REPLACE_STOP`.
                 # If the stop command is called on the given time, we can just
                 # start another checkpoint.
                 return
                # If we are only stopping if there are no pending
                # blocks.
                # We are only stopping if we are just stopping if there are
               # blocks in the background.

                 # Wait for the stop command.

                # Wait for the checkpoint to finish.

                # If we are only stopping if a checkpoint is pending,
                # we wait on the checkpoint to finish.

                # Check that the checkpoint has completed before we start it.
                 # Check that if the checkpoint has failed, we will not restart it.

                 # Check that we have no pending blocks at all.
                # Check that we will not stop.

                # Check that all checkpoints have successfully started.
                # Check that even though we continue to checkpoint,
               # we are not actually stopping.

                # TODO: this is really simple: we will just check the checkpoint_status and exit with a
               # error if this is not possible.

               # TODO: check that at least one checkpoint_status will be true if
              # this occurs. If it will be true, just check that the first checkpoint-status of the
             # checkpoint is true and then exit when the error is found.
               # TODO: check that this is still possible because we are waiting for the server
               # to do something.

               # TODO: check that this can never occur.

               # TODO: check that the server is doing something.

               # TODO: check that we are waiting for the server to do it.

         """
        assert 'all' in r,
        assert 'r' in None,
        assert 0 in r,
        assert 1 in r,
        assert 'r' in None,
        assert 'r' in None,
        assert '(!= 0, 2, 3, 4) is True.',
        assert 0 in r,
        assert 'r' in None,
        assert '(!= 0, 2, 3, 4) is True.',
        assert '(!= 0, 2, 3, 4, 2) is True.',
    

[6100 | 33779.97] loss=1.01 avg=1.03
[6101 | 33784.99] loss=0.71 avg=1.03
[6102 | 33790.01] loss=1.02 avg=1.03
[6103 | 33794.96] loss=0.78 avg=1.02
[6104 | 33799.92] loss=0.71 avg=1.02
[6105 | 33804.93] loss=1.34 avg=1.02
[6106 | 33809.79] loss=1.49 avg=1.03
[6107 | 33814.75] loss=0.62 avg=1.02
[6108 | 33819.77] loss=1.21 avg=1.03
[6109 | 33824.76] loss=0.75 avg=1.02
[6110 | 33829.77] loss=1.26 avg=1.03
[6111 | 33834.71] loss=0.46 avg=1.02
[6112 | 33839.64] loss=0.84 avg=1.02
[6113 | 33844.60] loss=1.09 avg=1.02
[6114 | 33849.74] loss=0.86 avg=1.02
[6115 | 33854.77] loss=0.75 avg=1.01
[6116 | 33859.68] loss=1.06 avg=1.01
[6117 | 33864.56] loss=0.91 avg=1.01
[6118 | 33869.54] loss=0.76 avg=1.01
[6119 | 33874.46] loss=0.75 avg=1.01
[6120 | 33879.43] loss=1.04 avg=1.01
[6121 | 33884.42] loss=0.91 avg=1.01
[6122 | 33889.32] loss=0.79 avg=1.01
[6123 | 33894.33] loss=0.99 avg=1.01
[6124 | 33899.33] loss=1.47 avg=1.01
[6125 | 33904.29] loss=1.40 avg=1.01
[6126 | 33909.24] loss=0.29 avg=1.01
[6127 | 33914.23] loss=1.46 avg=1.01
[6128 | 33919.29] loss=1.10 avg=1.01
[6129 | 33924.36] loss=0.80 avg=1.01
[6130 | 33929.32] loss=0.70 avg=1.01
[6131 | 33934.39] loss=1.06 avg=1.01
[6132 | 33939.35] loss=1.90 avg=1.02
[6133 | 33944.39] loss=0.73 avg=1.01
[6134 | 33949.38] loss=0.98 avg=1.01
[6135 | 33954.50] loss=0.78 avg=1.01
[6136 | 33959.62] loss=0.90 avg=1.01
[6137 | 33964.73] loss=0.93 avg=1.01
[6138 | 33969.68] loss=0.86 avg=1.01
[6139 | 33974.72] loss=1.37 avg=1.01
[6140 | 33979.67] loss=0.79 avg=1.01
[6141 | 33984.70] loss=1.18 avg=1.01
[6142 | 33989.78] loss=1.15 avg=1.01
[6143 | 33994.72] loss=1.63 avg=1.02
[6144 | 33999.68] loss=0.58 avg=1.01
[6145 | 34004.64] loss=1.17 avg=1.02
[6146 | 34009.68] loss=1.09 avg=1.02
[6147 | 34014.71] loss=1.32 avg=1.02
[6148 | 34019.75] loss=0.96 avg=1.02
[6149 | 34024.78] loss=1.16 avg=1.02
[6150 | 34029.77] loss=0.93 avg=1.02
[6151 | 34034.71] loss=0.78 avg=1.02
[6152 | 34039.69] loss=1.03 avg=1.02
[6153 | 34044.67] loss=1.29 avg=1.02
[6154 | 34049.64] loss=1.34 avg=1.02
[6155 | 34054.70] loss=1.34 avg=1.03
[6156 | 34059.75] loss=0.94 avg=1.03
[6157 | 34064.71] loss=0.76 avg=1.02
[6158 | 34069.72] loss=1.16 avg=1.02
[6159 | 34074.75] loss=1.23 avg=1.03
[6160 | 34079.74] loss=1.46 avg=1.03
[6161 | 34084.71] loss=0.86 avg=1.03
[6162 | 34089.85] loss=0.88 avg=1.03
[6163 | 34094.92] loss=0.76 avg=1.02
[6164 | 34099.95] loss=1.50 avg=1.03
[6165 | 34105.02] loss=1.11 avg=1.03
[6166 | 34110.06] loss=1.28 avg=1.03
[6167 | 34115.11] loss=0.85 avg=1.03
[6168 | 34120.15] loss=1.23 avg=1.03
[6169 | 34125.22] loss=1.63 avg=1.04
[6170 | 34130.32] loss=0.78 avg=1.04
[6171 | 34135.40] loss=0.65 avg=1.03
[6172 | 34140.38] loss=1.19 avg=1.03
[6173 | 34145.35] loss=0.70 avg=1.03
[6174 | 34150.38] loss=1.17 avg=1.03
[6175 | 34155.42] loss=1.28 avg=1.03
[6176 | 34160.47] loss=1.40 avg=1.04
[6177 | 34165.52] loss=0.89 avg=1.04
[6178 | 34170.50] loss=0.48 avg=1.03
[6179 | 34175.53] loss=1.34 avg=1.03
[6180 | 34180.52] loss=0.53 avg=1.03
[6181 | 34185.50] loss=0.57 avg=1.02
[6182 | 34190.52] loss=0.59 avg=1.02
[6183 | 34195.64] loss=0.85 avg=1.02
[6184 | 34200.68] loss=0.69 avg=1.02
[6185 | 34205.74] loss=1.02 avg=1.02
[6186 | 34210.73] loss=1.10 avg=1.02
[6187 | 34215.71] loss=0.85 avg=1.01
[6188 | 34220.77] loss=0.58 avg=1.01
[6189 | 34225.76] loss=1.56 avg=1.02
[6190 | 34230.85] loss=0.52 avg=1.01
[6191 | 34235.98] loss=1.10 avg=1.01
[6192 | 34241.06] loss=1.10 avg=1.01
[6193 | 34245.99] loss=1.05 avg=1.01
[6194 | 34251.06] loss=1.00 avg=1.01
[6195 | 34511.18] loss=1.18 avg=1.01
[6196 | 34516.71] loss=0.84 avg=1.01
[6197 | 34523.63] loss=1.44 avg=1.02
[6198 | 34538.06] loss=1.02 avg=1.02
[6199 | 34552.42] loss=1.07 avg=1.02
Generating samples...
======== SAMPLE 1 ========
.'', u"'id'", u')', u'sys', u'.', u'state_id', u'=', u'(', u'id', u'=', u'state_id', u')', u'if', u'message_id', u'not', u'in', u'state_id', u':', u'# get a text with the data', u'# for the message', u'self', u'.', u'__cw', u'.', u'create', u'(', u'[', u"'Message'", u']', u'+', u'state_id', u')', u'# create a message object', u'for', u'data_object', u'in', u'state_id', u'.', u'items', u':', u'message', u'=', u'state_id', u'.', u'items', u'[', u'data_object', u']', u'item', u'=', u'message', u'.', u'from_dict', u'(', u'message_id', u')', u'if', u'message_obj', u'is', u'None', u':', u'self', u'.', u'_cw', u'.', u'create', u'(', u'message_obj', u')', u'if', u'(', u'item', u'and', u'message_obj', u'is', u'None', u')', u':', u'raise', u'FileError', u'(', u'"No item in the given database data objects. Cannot be of the same type".', u')', u'result', u'=', u'[', u']', u'if', u'message_obj', u'.', u'type', u'is', u'None', u':', u'self', u'.', u'_cw', u'.', u'create', u'(', u'[', u']', u'item', u')', u'result', u'=', u'[', u']', u'if', u'data_obj', u'is', u'None', u':', u'self', u'.', u'_cw', u'.', u'create', u'(', u'[', u']', u'result', u')', u'result', u'=', u'[', u']', u'if', u'(', u'item', u'and', u'self', u'.', u'result', u'>', u'1', u'and', u'item', u'and', u'message_obj', u'is', u'None', u')', u':', u'return', u'Message', u'(', u'item', u'.', u'title', u',', u'message_obj', u')', u'else', u':', u'result', u'=', u'[', u']', u'return', u'Message', u'(', u'item', u'.', u'objects', u',', u'item', u'.', u'data', u')', u'if', u'message_obj', u'.', u'type', u'is', u'None', u':', u'self', u'.', u'_cw', u'.', u'create', u'(', u'[', u']', u'item', u')', u'result', u'=', u'[', u']', u'if', u'message_obj', u'.', u'type', u'is', u'None', u':', u'self', u'.', u'_cw', u'.', u'result', u'=', u'[', u']', u'return', u'Message', u'(', u'item', u'.', u'id', u',', u'message_obj', u')', u'result', u'=', u'[', u']', u'return', u'[', u']', u'return', u'[', u']'] Create Message object from string and
                    to format it. [u'Create', u'Message', u'object', u'from', u'strand', u'and', u'format', u'it', u'.'

[6200 | 37622.46] loss=1.00 avg=1.02
[6201 | 37636.77] loss=1.11 avg=1.02
[6202 | 37650.92] loss=0.88 avg=1.02
[6203 | 37665.18] loss=1.39 avg=1.02
[6204 | 37679.54] loss=1.09 avg=1.02
[6205 | 37693.68] loss=0.92 avg=1.02
[6206 | 37708.01] loss=0.92 avg=1.02
[6207 | 37722.11] loss=1.47 avg=1.02
[6208 | 37736.20] loss=1.19 avg=1.03
[6209 | 37750.26] loss=0.55 avg=1.02
[6210 | 37764.36] loss=0.51 avg=1.02
[6211 | 37778.74] loss=0.95 avg=1.01
[6212 | 37792.88] loss=1.55 avg=1.02
[6213 | 37806.91] loss=0.58 avg=1.02
[6214 | 37821.04] loss=0.83 avg=1.01
[6215 | 37835.20] loss=1.17 avg=1.02
[6216 | 37849.50] loss=1.14 avg=1.02
[6217 | 37863.58] loss=1.26 avg=1.02
[6218 | 37877.63] loss=0.63 avg=1.02
[6219 | 37891.89] loss=0.88 avg=1.01
[6220 | 37905.70] loss=1.07 avg=1.01
[6221 | 37919.72] loss=1.62 avg=1.02
[6222 | 37933.70] loss=0.74 avg=1.02
[6223 | 37947.68] loss=1.68 avg=1.02
[6224 | 37961.73] loss=0.85 avg=1.02
[6225 | 37975.82] loss=0.48 avg=1.02
[6226 | 37989.92] loss=0.58 avg=1.01
[6227 | 38003.96] loss=1.11 avg=1.01
[6228 | 38018.13] loss=0.77 avg=1.01
[6229 | 38032.01] loss=1.20 avg=1.01
[6230 | 38045.94] loss=1.29 avg=1.02
[6231 | 38060.01] loss=1.61 avg=1.02
[6232 | 38073.95] loss=1.23 avg=1.02
[6233 | 38088.12] loss=0.90 avg=1.02
[6234 | 38102.58] loss=0.57 avg=1.02
[6235 | 38116.67] loss=1.27 avg=1.02
[6236 | 38130.75] loss=0.53 avg=1.02
[6237 | 38144.95] loss=0.71 avg=1.01
[6238 | 38159.03] loss=1.14 avg=1.01
[6239 | 38173.06] loss=1.18 avg=1.02
[6240 | 38187.02] loss=1.07 avg=1.02
[6241 | 38201.15] loss=1.21 avg=1.02
[6242 | 38215.10] loss=0.68 avg=1.01
[6243 | 38229.41] loss=0.93 avg=1.01
[6244 | 38243.26] loss=0.75 avg=1.01
[6245 | 38257.25] loss=1.19 avg=1.01
[6246 | 38271.48] loss=0.81 avg=1.01
[6247 | 38285.92] loss=0.75 avg=1.01
[6248 | 38300.11] loss=0.67 avg=1.01
[6249 | 38314.26] loss=1.91 avg=1.01
[6250 | 38328.49] loss=0.49 avg=1.01
[6251 | 38342.72] loss=0.72 avg=1.01
[6252 | 38356.72] loss=0.70 avg=1.00
[6253 | 38370.86] loss=1.34 avg=1.01
[6254 | 38385.31] loss=0.71 avg=1.00
[6255 | 38399.37] loss=1.15 avg=1.01
[6256 | 38413.57] loss=0.89 avg=1.00
[6257 | 38427.83] loss=1.29 avg=1.01
[6258 | 38441.75] loss=1.08 avg=1.01
[6259 | 38456.14] loss=1.60 avg=1.01
[6260 | 38470.25] loss=0.98 avg=1.01
[6261 | 38484.14] loss=1.03 avg=1.01
[6262 | 38498.13] loss=0.63 avg=1.01
[6263 | 38512.04] loss=1.37 avg=1.01
[6264 | 38526.04] loss=0.63 avg=1.01
[6265 | 38539.79] loss=0.71 avg=1.01
[6266 | 38553.63] loss=1.14 avg=1.01
[6267 | 38567.72] loss=0.79 avg=1.01
[6268 | 38581.93] loss=1.29 avg=1.01
[6269 | 38595.92] loss=0.71 avg=1.01
[6270 | 38609.87] loss=1.16 avg=1.01
[6271 | 40255.35] loss=1.34 avg=1.01
[6272 | 40260.23] loss=0.95 avg=1.01
[6273 | 40269.09] loss=0.84 avg=1.01
[6274 | 40283.06] loss=0.85 avg=1.01
[6275 | 40297.08] loss=0.77 avg=1.00
[6276 | 40407.48] loss=0.64 avg=1.00
[6277 | 40413.00] loss=0.69 avg=1.00
[6278 | 40418.40] loss=1.34 avg=1.00
[6279 | 40423.94] loss=0.95 avg=1.00
[6280 | 40429.40] loss=0.99 avg=1.00
[6281 | 40434.60] loss=0.98 avg=1.00
[6282 | 40439.99] loss=0.79 avg=1.00
[6283 | 40445.65] loss=0.38 avg=0.99
[6284 | 40450.96] loss=1.10 avg=0.99
[6285 | 40456.23] loss=1.49 avg=1.00
[6286 | 40461.59] loss=0.65 avg=0.99
[6287 | 40466.85] loss=0.92 avg=0.99
[6288 | 40471.97] loss=0.84 avg=0.99
[6289 | 40477.03] loss=0.68 avg=0.99
[6290 | 40482.21] loss=0.80 avg=0.99
[6291 | 40487.42] loss=1.57 avg=0.99
[6292 | 40492.56] loss=1.18 avg=0.99
[6293 | 40497.83] loss=1.43 avg=1.00
[6294 | 40503.05] loss=0.59 avg=0.99
[6295 | 40508.25] loss=1.22 avg=1.00
[6296 | 40513.39] loss=0.79 avg=0.99
[6297 | 40518.52] loss=0.57 avg=0.99
[6298 | 40523.64] loss=1.07 avg=0.99
[6299 | 40528.79] loss=1.27 avg=0.99
Generating samples...
======== SAMPLE 1 ========
().group(5)[-1]|[-1]|[-1]|[-1]|[-1]|[-2],
        |                      
        |                       
        |                       
      )
            if not len(self.bases_index) and self.bases_index <= len(self.bases_index):
               self.bases_index += 1
           else:
               self.bases_index += self.bases_index

            # If the match was found, we set the index level of the last match
            self.bases_index -= self.bases_index
            if self.pases:
               if not self.pases[-1] and 0, self.pases[-1] == self.pases:
                  self.pases = [self.pases]
               elif self.pases and self.pases[1:] and self.pases[1:] and 0, self.pases[n:] > 0:
                  self.cases["matching"] = self.pases
                  if len(self.bases_index) >= 1:
                     if self.bases[1:] == self.bases[-1] and len(self.bases_index) > 0:
                     # Ignore the first one if we got it
                    for p in self.bases_index:
                     self.match[p] &= 1
                   self.pase_index += 1

            # Get the position of the match
            self.pases[-1] = self.pases[2]
            self.pases = [self.match.get_position()]:
                if self.pases[0:] in self.pases:
                    self.pases[0] = self.pases[1]

            # Given a match, and a new match match, return the match
            self.match = self.pases[n]

            # Loop through all matches
            if matches:
                s = match.index
                if i == matches[0].endswith("|")[1]:
                    matches[i] = self.match[i]
                 if matches[i].endswith("|"))[1]:
                    matches[1]:
  

[6300 | 40612.85] loss=0.91 avg=0.99
[6301 | 40618.07] loss=0.67 avg=0.99
[6302 | 40623.25] loss=0.65 avg=0.99
[6303 | 40628.45] loss=1.41 avg=0.99
[6304 | 40633.68] loss=1.35 avg=0.99
[6305 | 40638.76] loss=1.56 avg=1.00
[6306 | 40643.94] loss=0.92 avg=1.00
[6307 | 40649.11] loss=0.91 avg=1.00
[6308 | 40654.30] loss=1.14 avg=1.00
[6309 | 40659.49] loss=0.77 avg=1.00
[6310 | 40664.71] loss=0.69 avg=0.99
[6311 | 40669.89] loss=1.23 avg=1.00
[6312 | 40675.10] loss=1.19 avg=1.00
[6313 | 40680.27] loss=1.17 avg=1.00
[6314 | 40685.50] loss=1.10 avg=1.00
[6315 | 40690.69] loss=0.86 avg=1.00
[6316 | 40695.89] loss=1.27 avg=1.00
[6317 | 40701.04] loss=0.77 avg=1.00
[6318 | 40706.15] loss=1.30 avg=1.00
[6319 | 40711.35] loss=1.45 avg=1.01
[6320 | 40716.50] loss=0.84 avg=1.01
[6321 | 40721.68] loss=0.65 avg=1.00
[6322 | 40726.87] loss=1.76 avg=1.01
[6323 | 40732.14] loss=1.11 avg=1.01
[6324 | 40737.33] loss=0.80 avg=1.01
[6325 | 40742.59] loss=0.51 avg=1.00
[6326 | 40747.78] loss=0.92 avg=1.00
[6327 | 40752.98] loss=0.56 avg=1.00
[6328 | 40758.23] loss=0.60 avg=0.99
[6329 | 40763.46] loss=0.56 avg=0.99
[6330 | 40768.58] loss=0.77 avg=0.99
[6331 | 40773.81] loss=0.81 avg=0.99
[6332 | 40779.07] loss=0.80 avg=0.98
[6333 | 40784.26] loss=1.53 avg=0.99
[6334 | 40789.47] loss=1.19 avg=0.99
[6335 | 40794.76] loss=0.77 avg=0.99
[6336 | 40800.05] loss=0.45 avg=0.98
[6337 | 40805.42] loss=0.76 avg=0.98
[6338 | 40810.87] loss=1.04 avg=0.98
[6339 | 40816.11] loss=1.14 avg=0.98
[6340 | 40821.40] loss=1.11 avg=0.99
[6341 | 40826.64] loss=0.86 avg=0.98
[6342 | 40831.98] loss=1.01 avg=0.98
[6343 | 40837.34] loss=0.47 avg=0.98
[6344 | 40842.70] loss=0.92 avg=0.98
[6345 | 40848.04] loss=1.63 avg=0.99
[6346 | 40853.42] loss=0.83 avg=0.98
[6347 | 40858.65] loss=0.63 avg=0.98
[6348 | 40863.93] loss=0.77 avg=0.98
[6349 | 40869.27] loss=1.03 avg=0.98
[6350 | 40874.58] loss=0.74 avg=0.98
[6351 | 40879.87] loss=1.53 avg=0.98
[6352 | 40885.16] loss=1.27 avg=0.98
[6353 | 40890.49] loss=0.94 avg=0.98
[6354 | 40895.76] loss=1.47 avg=0.99
[6355 | 40901.02] loss=1.14 avg=0.99
[6356 | 40906.37] loss=1.39 avg=0.99
[6357 | 40911.70] loss=0.82 avg=0.99
[6358 | 40916.98] loss=0.88 avg=0.99
[6359 | 40922.28] loss=1.19 avg=0.99
[6360 | 40927.66] loss=0.55 avg=0.99
[6361 | 40932.98] loss=1.04 avg=0.99
[6362 | 40938.18] loss=1.23 avg=0.99
[6363 | 40943.41] loss=1.12 avg=0.99
[6364 | 40948.62] loss=1.35 avg=1.00
[6365 | 40953.82] loss=0.49 avg=0.99
[6366 | 40959.05] loss=0.47 avg=0.99
[6367 | 40964.35] loss=0.97 avg=0.99
[6368 | 40969.57] loss=1.71 avg=0.99
[6369 | 40974.83] loss=0.60 avg=0.99
[6370 | 40980.11] loss=0.96 avg=0.99
[6371 | 40985.40] loss=1.18 avg=0.99
[6372 | 40990.70] loss=0.78 avg=0.99
[6373 | 40995.97] loss=0.84 avg=0.99
[6374 | 41001.18] loss=1.67 avg=0.99
[6375 | 41006.43] loss=0.71 avg=0.99
[6376 | 41011.65] loss=0.77 avg=0.99
[6377 | 41016.96] loss=0.48 avg=0.98
[6378 | 41022.12] loss=1.30 avg=0.99
[6379 | 41027.34] loss=0.71 avg=0.98
[6380 | 41032.47] loss=1.12 avg=0.99
[6381 | 41037.58] loss=0.99 avg=0.99
[6382 | 41042.74] loss=0.78 avg=0.98
[6383 | 41047.91] loss=1.04 avg=0.98
[6384 | 41053.03] loss=0.62 avg=0.98
[6385 | 41058.17] loss=0.79 avg=0.98
[6386 | 41063.28] loss=0.59 avg=0.98
[6387 | 41068.41] loss=0.84 avg=0.97
[6388 | 41073.64] loss=0.67 avg=0.97
[6389 | 41078.80] loss=0.46 avg=0.97
[6390 | 41083.91] loss=0.78 avg=0.96
[6391 | 41089.11] loss=1.05 avg=0.96
[6392 | 41094.29] loss=1.69 avg=0.97
[6393 | 41099.51] loss=1.73 avg=0.98
[6394 | 41104.66] loss=0.94 avg=0.98
[6395 | 41109.80] loss=0.65 avg=0.98
[6396 | 41115.07] loss=0.60 avg=0.97
[6397 | 41120.29] loss=1.33 avg=0.98
[6398 | 41125.44] loss=1.03 avg=0.98
[6399 | 41130.63] loss=1.13 avg=0.98
Generating samples...
======== SAMPLE 1 ========
', u',', u'*', u'self', u')', u'for', u'self', u'in', u'self', u'.', rl_params', u'[', u"'b'", u']', u'[', u'0', u']', u':', u'self', u'.', u'replace_data_params', u'(', u'self', u'.', u'rel_params', u'"b{%s}[%s] {%s}[%s] {%s}[%s] {%s}[%s] {%s}[%s] {%s}[%s] {%s}[%s] {%s}[%s]}!"', u')', u')', u'break', u'except', u'ValueError', u'as', u'e', u':', u'if', u'self', u'.', u'errors', u'.', u'get', u'(', u'"Missing %s argument"', u',', u'None', u',', u"'Bad%s flag for %r'", u',', u'i', u'=', u"\'%s'", u',', u'self', u'[', u'i', u']', u'[', u"'b'", u']', u']', u')', u':', u'self', u'.', u'_get_dat_method', u'(', u'self', u'.', u'rel_params', u'[', u"'b'", u']', u')', u'=', u'self', u'.', u'_get_method', u'(', u'self', u'.', u'rel_params', u'[', u"'s'", u']', u')', u'pass', u'except', u'ValueError', u'as', u'e', u':', u'if', u'self', u'.', u'errors', u'.', u'get', u'(', u'"Missing %s argument"', u',', u'None', u',', u"'Bad %s flag for %r'", u',', u'i', u'=', u"\'%s'"", u',', u'self', u'[', u'i', u']', u'[', u"'b'", u']', u']', u')', u':', u'self', u'.', u'_get_dat_method', u'(', u'self', u'.', u'rel_params', u'[', u"'s'", u']', u')', u'=', u'self', u'.', u'_get_method', u'(', u'self', u'.', u'rel_params', u'[', u"'s'", u']', u')', u'else', u':', u'self', u'.', u'_get_dat_method', u'(', u'self', u'.', u'rel_params', u'[', u"'s'", u']', u')', u'pass', u'except', u'ValueError', u'as', u'e', u':', u'if', u'os', u'.', u'path', u'.', u'isfile', u'(', u'self', u'.', u'_get_dat_method', u'(', u'self', u'.', u'rel_params', u'[', u"'c'", u']', u',', u''2', u',', u"'b'", u']', u',', u')', u':', u'self', u'.', u'_get_dat_method', u'(', u'self', u'.', u'rel_params', u'[', u"'c'", u']', u',', u'2', u',', u''i', u'=', u"\'%s'"', u',', u'self', u'[', u'i', u']', u'[', u"'b'", u']', u']', u'[', u"'b'", u']', u',', u'self', u'[', u'i', u']', u'[', u"'d'", u']', u'=', u'self', u'[', u"'c'", u']', u')', u':', u'self', u'.', u'

[6400 | 41215.80] loss=0.93 avg=0.98
[6401 | 41221.02] loss=0.77 avg=0.98
[6402 | 41226.16] loss=1.39 avg=0.98
[6403 | 41231.41] loss=0.83 avg=0.98
[6404 | 41236.63] loss=0.85 avg=0.98
[6405 | 41241.74] loss=1.25 avg=0.98
[6406 | 41246.86] loss=1.09 avg=0.98
[6407 | 41251.94] loss=1.11 avg=0.98
[6408 | 41256.92] loss=0.88 avg=0.98
[6409 | 41261.98] loss=1.02 avg=0.98
[6410 | 41267.08] loss=1.07 avg=0.98
[6411 | 41272.14] loss=0.97 avg=0.98
[6412 | 41277.34] loss=1.39 avg=0.99
[6413 | 41282.49] loss=0.95 avg=0.99
[6414 | 41287.60] loss=0.66 avg=0.98
[6415 | 41292.85] loss=0.87 avg=0.98
[6416 | 41298.03] loss=2.09 avg=0.99
[6417 | 41303.25] loss=0.48 avg=0.99
[6418 | 41308.55] loss=0.40 avg=0.98
[6419 | 41313.75] loss=1.45 avg=0.99
[6420 | 41319.00] loss=1.23 avg=0.99
[6421 | 41324.19] loss=0.93 avg=0.99
[6422 | 41329.25] loss=1.01 avg=0.99
[6423 | 41334.45] loss=1.25 avg=0.99
[6424 | 41339.54] loss=1.13 avg=0.99
[6425 | 41344.70] loss=0.83 avg=0.99
[6426 | 41349.89] loss=1.35 avg=0.99
[6427 | 41355.08] loss=0.98 avg=0.99
[6428 | 41360.30] loss=1.10 avg=1.00
[6429 | 41365.59] loss=0.88 avg=0.99
[6430 | 41370.83] loss=1.22 avg=1.00
[6431 | 41375.96] loss=0.85 avg=0.99
[6432 | 41381.21] loss=1.01 avg=0.99
[6433 | 41386.45] loss=0.71 avg=0.99
[6434 | 41391.68] loss=0.64 avg=0.99
[6435 | 41396.91] loss=1.23 avg=0.99
[6436 | 41402.10] loss=0.91 avg=0.99
[6437 | 41407.30] loss=1.97 avg=1.00
[6438 | 41412.50] loss=1.21 avg=1.00
[6439 | 41417.76] loss=1.04 avg=1.00
[6440 | 41422.97] loss=0.95 avg=1.00
[6441 | 41428.20] loss=0.69 avg=1.00
[6442 | 41433.43] loss=0.84 avg=1.00
[6443 | 41438.63] loss=0.91 avg=1.00
[6444 | 41443.90] loss=0.95 avg=1.00
[6445 | 41449.06] loss=1.22 avg=1.00
[6446 | 41454.25] loss=0.55 avg=0.99
[6447 | 41459.40] loss=0.96 avg=0.99
[6448 | 41464.53] loss=0.96 avg=0.99
[6449 | 41469.61] loss=0.85 avg=0.99
[6450 | 41474.80] loss=2.19 avg=1.00
[6451 | 41480.02] loss=0.84 avg=1.00
[6452 | 41485.21] loss=0.91 avg=1.00
[6453 | 41490.38] loss=0.87 avg=1.00
[6454 | 41495.53] loss=1.07 avg=1.00
[6455 | 41500.65] loss=1.13 avg=1.00
[6456 | 41505.85] loss=1.45 avg=1.01
[6457 | 41511.06] loss=0.87 avg=1.00
[6458 | 41516.28] loss=1.23 avg=1.01
[6459 | 41521.48] loss=0.95 avg=1.01
[6460 | 41526.60] loss=1.07 avg=1.01
[6461 | 41531.76] loss=1.00 avg=1.01
[6462 | 41536.94] loss=0.82 avg=1.01
[6463 | 41542.11] loss=1.03 avg=1.01
[6464 | 41547.25] loss=0.77 avg=1.00
[6465 | 41552.36] loss=0.67 avg=1.00
[6466 | 41557.42] loss=1.61 avg=1.01
[6467 | 41562.52] loss=0.66 avg=1.00
[6468 | 41567.59] loss=0.76 avg=1.00
[6469 | 41572.69] loss=0.73 avg=1.00
[6470 | 41577.81] loss=0.79 avg=1.00
[6471 | 41583.01] loss=1.04 avg=1.00
[6472 | 41588.10] loss=0.54 avg=0.99
[6473 | 41593.33] loss=1.46 avg=1.00
[6474 | 41598.55] loss=0.73 avg=0.99
[6475 | 41603.73] loss=1.53 avg=1.00
[6476 | 41608.88] loss=0.74 avg=1.00
[6477 | 41614.14] loss=1.96 avg=1.01
[6478 | 41619.35] loss=0.65 avg=1.00
[6479 | 41624.60] loss=0.90 avg=1.00
[6480 | 41629.83] loss=0.65 avg=1.00
[6481 | 41635.03] loss=0.75 avg=0.99
[6482 | 41640.19] loss=1.42 avg=1.00
[6483 | 41645.45] loss=1.46 avg=1.00
[6484 | 41650.62] loss=1.02 avg=1.00
[6485 | 41655.89] loss=1.20 avg=1.01
[6486 | 41661.12] loss=0.97 avg=1.01
[6487 | 41666.39] loss=0.83 avg=1.00
[6488 | 41671.48] loss=0.93 avg=1.00
[6489 | 41676.40] loss=1.64 avg=1.01
[6490 | 41681.37] loss=1.25 avg=1.01
[6491 | 41686.44] loss=0.47 avg=1.01
[6492 | 41691.45] loss=1.02 avg=1.01
[6493 | 41696.52] loss=0.69 avg=1.00
[6494 | 41701.46] loss=0.61 avg=1.00
[6495 | 41706.41] loss=1.16 avg=1.00
[6496 | 41711.44] loss=0.93 avg=1.00
[6497 | 41716.51] loss=1.78 avg=1.01
[6498 | 41721.55] loss=1.27 avg=1.01
[6499 | 41726.54] loss=0.86 avg=1.01
Generating samples...
======== SAMPLE 1 ========
files = np.zeros(max(n), nxcoord, ycoord)

                 if self.is_nanarray() and len(xarrs):
                     yarr_shape[yarr].y = n.x
                   self._nanarray(xarrs)

                 yarr_shape[yarr].y = z

                 if self.is_nanarray() and len(yarrs):
                     yarr_shape[yarr.y].x = n

                  yarr_shape[yarr.y].y = n

                return _nanarray [u'def', u'f64_narrow', u'(', u'self', u',', u'six', u',', u'del', u',', u'm', u',', u'del_nax', u'=', u'0', u')', u':', u'# make an NANOarray', u'# for edge-to-edge calculations', u'f64_narrow', u'=', u'np', u'.', u'uint8', u'(', u')', u'=', u'len', u'(', u'xarrs', u')', u'xarr_shape', u'=', u'xarr_shape', u'*', u'len', u'xarr_shape', u'+', u"','", u'len', u'=', u'8', u'# make this list of nxarrs', u'for', u'x', u'in', u'range', u'(', u'len', u',', u'8', u')', u'xarr_shape', u'=', u'xarr_shape', u'*', u'xarr_shape', u'+', u"0.5"] xarr_shape [u'xarr_shape'] NANOarray function.

        Args:
            xarr_shape [**] edge_to_edge
        # make nxarrs that are edge-to-edge calculations
        #            yarr_shape [**] zarr_shape
                yarr_shape [**] y_coord [**] y_coord
                yarr_shape [**] y_coord [**] xarr_shape
                xarr_shape [**] x_coord
                xarr_shape [**] y_coord
        """
        xarr_shape(Xarr, xarr_shape)
        # add edge-to-edge calculations
        yarr_shape[yarr.y].x = n.x
        yarr_shape[yarr.y].x = z
        #            zarr_shape[yarr.y].x = n.x
        yarr_shape[yarr.y].x = n.y
        return _inf
        if xarr_shape[yarr.y].x < len(xarr_shape) and xarr_shape[yarr.y].x < len(yarr_shape) and yarr_shape[yarr.y] < xarr_shape[yarr.y]:
               xarr_shape[yarr.y][xarr_shape[yarr.y].x] =

[6500 | 41811.66] loss=0.96 avg=1.01
[6501 | 41816.73] loss=1.87 avg=1.02
[6502 | 41821.82] loss=0.88 avg=1.02
[6503 | 41826.87] loss=1.47 avg=1.02
[6504 | 41831.68] loss=0.61 avg=1.02
[6505 | 41836.61] loss=0.95 avg=1.02
[6506 | 41841.52] loss=0.85 avg=1.01
[6507 | 41846.51] loss=1.52 avg=1.02
[6508 | 41851.50] loss=1.03 avg=1.02
[6509 | 41856.52] loss=0.89 avg=1.02
[6510 | 41861.52] loss=0.66 avg=1.01
[6511 | 41866.43] loss=0.93 avg=1.01
[6512 | 41871.39] loss=1.03 avg=1.01
[6513 | 41876.38] loss=0.97 avg=1.01
[6514 | 41881.35] loss=0.67 avg=1.01
[6515 | 41886.28] loss=0.70 avg=1.01
[6516 | 41891.23] loss=1.04 avg=1.01
[6517 | 41896.28] loss=1.48 avg=1.01
[6518 | 41901.40] loss=0.82 avg=1.01
[6519 | 41906.39] loss=0.72 avg=1.01
[6520 | 41911.38] loss=0.91 avg=1.01
[6521 | 41916.41] loss=1.43 avg=1.01
[6522 | 41921.46] loss=0.84 avg=1.01
[6523 | 41926.53] loss=0.87 avg=1.01
[6524 | 41931.56] loss=0.86 avg=1.01
[6525 | 41936.62] loss=0.95 avg=1.00
[6526 | 41941.72] loss=0.55 avg=1.00
[6527 | 41946.86] loss=0.96 avg=1.00
[6528 | 41951.92] loss=0.53 avg=1.00
[6529 | 41956.97] loss=0.80 avg=0.99
[6530 | 41961.98] loss=1.14 avg=0.99
[6531 | 41967.12] loss=0.63 avg=0.99
[6532 | 41972.20] loss=0.88 avg=0.99
[6533 | 41977.19] loss=1.17 avg=0.99
[6534 | 41982.25] loss=1.07 avg=0.99
[6535 | 41987.29] loss=1.70 avg=1.00
[6536 | 41992.26] loss=0.78 avg=1.00
[6537 | 41997.25] loss=1.57 avg=1.00
[6538 | 42002.33] loss=0.97 avg=1.00
[6539 | 42007.32] loss=0.53 avg=1.00
[6540 | 42012.34] loss=1.06 avg=1.00
[6541 | 42017.37] loss=0.88 avg=1.00
[6542 | 42022.36] loss=1.45 avg=1.00
[6543 | 42027.41] loss=1.65 avg=1.01
[6544 | 42032.46] loss=0.80 avg=1.01
[6545 | 42037.56] loss=1.01 avg=1.01
[6546 | 42042.52] loss=1.30 avg=1.01
[6547 | 42047.65] loss=1.65 avg=1.02
[6548 | 42052.65] loss=0.80 avg=1.01
[6549 | 42057.76] loss=0.47 avg=1.01
[6550 | 42062.82] loss=0.97 avg=1.01
[6551 | 42067.92] loss=0.76 avg=1.01
[6552 | 42072.99] loss=0.83 avg=1.00
[6553 | 42078.08] loss=0.77 avg=1.00
[6554 | 42083.13] loss=0.81 avg=1.00
[6555 | 42088.20] loss=0.89 avg=1.00
[6556 | 42093.30] loss=1.00 avg=1.00
[6557 | 42098.42] loss=0.75 avg=1.00
[6558 | 42103.45] loss=0.77 avg=0.99
[6559 | 42108.48] loss=1.07 avg=0.99
[6560 | 42113.49] loss=0.96 avg=0.99
[6561 | 42118.54] loss=1.22 avg=1.00
[6562 | 42123.52] loss=1.02 avg=1.00
[6563 | 42128.56] loss=0.91 avg=1.00
[6564 | 42133.58] loss=1.22 avg=1.00
[6565 | 42138.69] loss=0.88 avg=1.00
[6566 | 42143.76] loss=1.24 avg=1.00
[6567 | 42148.86] loss=1.66 avg=1.01
[6568 | 42153.94] loss=0.96 avg=1.00
[6569 | 42159.01] loss=0.89 avg=1.00
[6570 | 42164.06] loss=0.75 avg=1.00
[6571 | 42169.01] loss=1.26 avg=1.00
[6572 | 42174.05] loss=1.35 avg=1.01
[6573 | 42179.11] loss=1.17 avg=1.01
[6574 | 42184.18] loss=0.60 avg=1.00
[6575 | 42189.17] loss=1.53 avg=1.01
[6576 | 42194.19] loss=0.87 avg=1.01
[6577 | 42199.27] loss=0.91 avg=1.01
[6578 | 42204.29] loss=1.14 avg=1.01
[6579 | 42209.31] loss=0.72 avg=1.01
[6580 | 42214.31] loss=0.67 avg=1.00
[6581 | 42219.47] loss=0.69 avg=1.00
[6582 | 42224.50] loss=1.08 avg=1.00
[6583 | 42229.60] loss=0.88 avg=1.00
[6584 | 42234.59] loss=0.82 avg=1.00
[6585 | 42239.61] loss=0.80 avg=1.00
[6586 | 42244.68] loss=1.18 avg=1.00
[6587 | 42249.78] loss=0.75 avg=0.99
[6588 | 42254.87] loss=0.87 avg=0.99
[6589 | 42259.93] loss=1.10 avg=0.99
[6590 | 42264.93] loss=0.68 avg=0.99
[6591 | 42269.96] loss=1.32 avg=0.99
[6592 | 42274.95] loss=1.24 avg=1.00
[6593 | 42280.01] loss=0.62 avg=0.99
[6594 | 42285.11] loss=0.60 avg=0.99
[6595 | 42290.09] loss=0.90 avg=0.99
[6596 | 42295.12] loss=0.77 avg=0.99
[6597 | 42300.21] loss=0.74 avg=0.98
[6598 | 42305.24] loss=0.92 avg=0.98
[6599 | 42310.31] loss=0.72 avg=0.98
Generating samples...
======== SAMPLE 1 ========
 v':', u"'error'", u')', u'if', u'max', u'(', u'maxx', u'or', u'maxum', u'or', u'maxz', u'or', u'maxus', u'and', u'max', u'not', u'.', u'CALL', u'p', u'.', u'strip', u'(', u')', u'.'] _check_all_errors_v_not_called python def _check_all_errors_v_not_called(self):
        """
        Check all error messages not made during runtime.
        :param _check_all_errors_v_not_called:
            An error message that only follows the `p` parameter if no
            `p` is provided
        :param _check_all_errors_p_only : No such parameter
        :param _check_all_errors_p_only: No such parameter
        """
        p = _check_all_errors_v_not_called()
        if self.pg_params.pg_params != NULL:
            return None

        _check_all_errors_v_only = _check_all_errors_v_only
        p = p + 1
        if p not in self.pg_params:
            return p + p

        # Check if p in `p` is a single-word or a list of strings
        if p not in params.pg_params:
            self._g_params.pg_params[p - 1:] = p
        return None [u'def', u'_check_all_errors_v_not_called', u'(', u'self', u')', u':', u'p', u'=', u'_check_all_errors_v_not_called', u'(', u')', u'if', u'self', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u'p_params', u'.', u'pg_params', u'.', u'pg_params', u'.', u')', u':', u'assert', u"'not a valid function'", u',', u'assert', u'True', u',', u'False', u',', u'False', u',', u'False', u')', u'if', u"p"', u':', u'allparams', u'=', u'self', u'.', u'display_list_params', u'(', u'allparams', u',', u'allparams', u')', u'if', u'"p"', u':

[6600 | 42397.66] loss=1.35 avg=0.98
[6601 | 42402.64] loss=1.08 avg=0.99
[6602 | 42407.61] loss=1.08 avg=0.99
[6603 | 42412.55] loss=1.45 avg=0.99
[6604 | 42417.65] loss=1.39 avg=0.99
[6605 | 42422.61] loss=1.10 avg=1.00
[6606 | 42427.61] loss=0.77 avg=0.99
[6607 | 42432.64] loss=0.90 avg=0.99
[6608 | 42437.65] loss=0.73 avg=0.99
[6609 | 42442.63] loss=0.60 avg=0.99
[6610 | 42447.63] loss=0.52 avg=0.98
[6611 | 42452.64] loss=1.03 avg=0.98
[6612 | 42457.59] loss=0.64 avg=0.98
[6613 | 42462.64] loss=1.20 avg=0.98
[6614 | 42467.63] loss=1.01 avg=0.98
[6615 | 42472.66] loss=0.92 avg=0.98
[6616 | 42477.79] loss=1.16 avg=0.98
[6617 | 42482.80] loss=0.75 avg=0.98
[6618 | 42487.91] loss=0.64 avg=0.98
[6619 | 42492.93] loss=1.29 avg=0.98
[6620 | 42497.91] loss=0.69 avg=0.98
[6621 | 42502.91] loss=0.79 avg=0.97
[6622 | 42507.90] loss=0.85 avg=0.97
[6623 | 42512.93] loss=0.99 avg=0.97
[6624 | 42517.99] loss=1.80 avg=0.98
[6625 | 42522.94] loss=0.93 avg=0.98
[6626 | 42527.92] loss=0.95 avg=0.98
[6627 | 42532.86] loss=0.84 avg=0.98
[6628 | 42537.90] loss=0.71 avg=0.98
[6629 | 42542.88] loss=1.06 avg=0.98
[6630 | 42547.98] loss=1.15 avg=0.98
[6631 | 42552.99] loss=0.54 avg=0.98
[6632 | 42558.08] loss=0.77 avg=0.97
[6633 | 42563.06] loss=1.34 avg=0.98
[6634 | 42568.12] loss=1.66 avg=0.98
[6635 | 42573.12] loss=0.84 avg=0.98
[6636 | 42578.16] loss=1.19 avg=0.98
[6637 | 42583.19] loss=1.30 avg=0.99
[6638 | 42588.30] loss=0.73 avg=0.98
[6639 | 42593.46] loss=1.05 avg=0.99
[6640 | 42598.52] loss=1.38 avg=0.99
[6641 | 42603.54] loss=0.70 avg=0.99
[6642 | 42608.61] loss=1.06 avg=0.99
[6643 | 42613.69] loss=1.37 avg=0.99
[6644 | 42618.69] loss=1.39 avg=1.00
[6645 | 42623.72] loss=0.69 avg=0.99
[6646 | 42628.79] loss=0.73 avg=0.99
[6647 | 42633.80] loss=0.68 avg=0.99
[6648 | 42638.85] loss=1.37 avg=0.99
[6649 | 42643.94] loss=1.15 avg=0.99
[6650 | 42648.95] loss=1.12 avg=0.99
[6651 | 42654.03] loss=0.55 avg=0.99
[6652 | 42659.04] loss=0.95 avg=0.99
[6653 | 42664.04] loss=0.64 avg=0.98
[6654 | 42669.05] loss=0.84 avg=0.98
[6655 | 42674.08] loss=0.77 avg=0.98
[6656 | 42679.07] loss=1.59 avg=0.99
[6657 | 42684.04] loss=0.88 avg=0.99
[6658 | 42689.07] loss=1.13 avg=0.99
[6659 | 42694.15] loss=0.69 avg=0.98
[6660 | 42699.19] loss=1.11 avg=0.99
[6661 | 42704.21] loss=0.71 avg=0.98
[6662 | 42709.33] loss=0.67 avg=0.98
[6663 | 42714.37] loss=1.12 avg=0.98
[6664 | 42719.45] loss=1.60 avg=0.99
[6665 | 42724.51] loss=1.15 avg=0.99
[6666 | 42729.57] loss=0.51 avg=0.98
[6667 | 42734.50] loss=1.73 avg=0.99
[6668 | 42739.51] loss=1.05 avg=0.99
[6669 | 42744.58] loss=1.15 avg=0.99
[6670 | 42749.65] loss=0.83 avg=0.99
[6671 | 42754.63] loss=0.85 avg=0.99
[6672 | 42759.77] loss=0.97 avg=0.99
[6673 | 42764.81] loss=1.18 avg=0.99
[6674 | 42769.88] loss=0.92 avg=0.99
[6675 | 42774.97] loss=0.93 avg=0.99
[6676 | 42780.07] loss=0.66 avg=0.99
[6677 | 42785.15] loss=0.66 avg=0.99
[6678 | 42790.16] loss=0.52 avg=0.98
[6679 | 42795.26] loss=0.89 avg=0.98
[6680 | 42800.37] loss=0.88 avg=0.98
[6681 | 42805.48] loss=1.31 avg=0.98
[6682 | 42810.60] loss=1.00 avg=0.98
[6683 | 42815.58] loss=0.99 avg=0.98
[6684 | 42820.63] loss=0.64 avg=0.98
[6685 | 42825.72] loss=1.41 avg=0.98
[6686 | 42830.73] loss=0.73 avg=0.98
[6687 | 42835.66] loss=1.07 avg=0.98
[6688 | 42840.63] loss=0.74 avg=0.98
[6689 | 42845.58] loss=0.78 avg=0.98
[6690 | 42850.68] loss=1.00 avg=0.98
[6691 | 42855.78] loss=0.79 avg=0.98
[6692 | 42860.75] loss=0.81 avg=0.97
[6693 | 42865.72] loss=1.10 avg=0.97
[6694 | 42870.80] loss=1.37 avg=0.98
[6695 | 42875.90] loss=0.94 avg=0.98
[6696 | 42881.01] loss=0.72 avg=0.98
[6697 | 42886.07] loss=0.82 avg=0.97
[6698 | 42891.13] loss=0.94 avg=0.97
[6699 | 42896.17] loss=0.79 avg=0.97
Generating samples...
======== SAMPLE 1 ========
 u'.', u'name', u',', u'expiry_date', u',', u'description_date', u',', u'url', u')', u'self', u'.', u'logger', u'.', u'debug', u'(', u'[', u"'%Y/%r %S %M'", u"'%Y/%r %S %M'", u',', u'description_date', u',', u')', u'return', u'response', u'.', u'post', u'(', u'path', u',', u'self', u'.', u'status', u'+', u"'%Y/%M'", u",', u'description_date', u',', u')'] Return the path to this service object from the time it is created in the background. [u'Return', u'the', u'path', u'to', u'this', u'service', u'object', u'from', u'the', u'time', u'it', u'is', u'created', u'in', u'the', u'background', u'.'] ServiceHandlerService.path python def path(self):
        """
        Return the path to this service object from the time it is created in the background.
        """
        self.logger.expect(path, '%Y/%r %S %M'
               '%Y/%r %S %M', '%Y/%r',
              description_date, '%Y',
               description_date, '%Y/%r %s %M',
               url)
        return self.service.path, self.sockets.path.format(path) train sf_cli/services/service.py pangolad/sf-cli 54c8d4f8b4c3a27ed8a7e9ee2b1c6cb7e5edc https://github.com/ppangolad/sf-cli/blob/54c8d4f8b4c3a27ed8a7e9ee2b1c6cb7e5edc/sf_cli/services/service.py#L939-L959
def _to_post(self, response, response_params):
        """
        Return the data from post response (data_format) """
        self.logger.expect(response, '%Y/%r %s %M'
              '%Y/%r %s %m'
              response_id, '%Y/%r %s %m'
              url)
        self.logger.expect(response, '%Y/%r %S %M %Y'
              '%Y/%r %s %m'
              url)
        self.logger.expect(response.content.url)
        self.logger.expect(response.headers.headers) [u'def', u'_to_post', u'(', u'self', u',', u'response', u',', u'response_params', u')', u':', u'self', u'.', u'logger', u'.', u'expect', u'(', u'response', u',', u"'%Y/%r %s %m/%y'", u'%', u'response_id', u',', u"'%Y/%r %s %m/%Y'", u'# L939-L959'] Return the data from post response (data_format) [u'def', u'_to_post', u'(', u'self', u',

[6700 | 42983.13] loss=0.73 avg=0.97
[6701 | 42988.24] loss=1.14 avg=0.97
[6702 | 42993.23] loss=0.93 avg=0.97
[6703 | 42998.28] loss=2.23 avg=0.98
[6704 | 43003.32] loss=0.83 avg=0.98
[6705 | 43008.38] loss=0.80 avg=0.98
[6706 | 43013.42] loss=1.53 avg=0.99
[6707 | 43018.36] loss=1.18 avg=0.99
[6708 | 43023.27] loss=1.05 avg=0.99
[6709 | 43028.32] loss=1.01 avg=0.99
[6710 | 43033.30] loss=0.85 avg=0.99
[6711 | 43038.38] loss=0.75 avg=0.98
[6712 | 43043.39] loss=1.24 avg=0.99
[6713 | 43048.39] loss=1.51 avg=0.99
[6714 | 43053.47] loss=0.97 avg=0.99
[6715 | 43058.54] loss=1.18 avg=0.99
[6716 | 43063.62] loss=0.82 avg=0.99
[6717 | 43068.71] loss=1.21 avg=0.99
[6718 | 43073.74] loss=0.88 avg=0.99
[6719 | 43078.79] loss=0.94 avg=0.99
[6720 | 43083.77] loss=1.02 avg=0.99
[6721 | 43088.89] loss=0.69 avg=0.99
[6722 | 43093.92] loss=0.77 avg=0.99
[6723 | 43098.96] loss=0.84 avg=0.99
[6724 | 43104.15] loss=0.88 avg=0.99
[6725 | 43109.24] loss=0.66 avg=0.98
[6726 | 43114.37] loss=0.99 avg=0.98
[6727 | 43119.42] loss=1.22 avg=0.98
[6728 | 43124.48] loss=0.95 avg=0.98
[6729 | 43129.55] loss=1.21 avg=0.99
[6730 | 43134.62] loss=1.09 avg=0.99
[6731 | 43139.67] loss=1.03 avg=0.99
[6732 | 43144.71] loss=0.51 avg=0.98
[6733 | 43149.78] loss=0.71 avg=0.98
[6734 | 43154.88] loss=0.85 avg=0.98
[6735 | 43159.96] loss=1.67 avg=0.99
[6736 | 43165.06] loss=0.88 avg=0.99
[6737 | 43170.10] loss=1.11 avg=0.99
[6738 | 43175.09] loss=0.64 avg=0.98
[6739 | 43180.17] loss=1.90 avg=0.99
[6740 | 43185.25] loss=0.75 avg=0.99
[6741 | 43190.28] loss=0.75 avg=0.99
[6742 | 43195.30] loss=1.48 avg=0.99
[6743 | 43200.37] loss=1.14 avg=0.99
[6744 | 43205.34] loss=1.23 avg=1.00
[6745 | 43210.36] loss=0.55 avg=0.99
[6746 | 43215.38] loss=0.72 avg=0.99
[6747 | 43220.40] loss=0.72 avg=0.99
[6748 | 43225.59] loss=0.69 avg=0.98
[6749 | 43230.70] loss=0.80 avg=0.98
[6750 | 43235.76] loss=1.84 avg=0.99
[6751 | 43240.78] loss=0.84 avg=0.99
[6752 | 43245.89] loss=1.15 avg=0.99
[6753 | 43250.95] loss=0.82 avg=0.99
[6754 | 43255.93] loss=1.52 avg=0.99
[6755 | 43261.02] loss=0.73 avg=0.99
[6756 | 43266.10] loss=0.62 avg=0.99
[6757 | 43271.19] loss=1.09 avg=0.99
[6758 | 43276.29] loss=1.41 avg=0.99
[6759 | 43281.36] loss=0.77 avg=0.99
[6760 | 43286.40] loss=0.90 avg=0.99
[6761 | 43291.40] loss=0.62 avg=0.99
[6762 | 43296.47] loss=0.84 avg=0.98
[6763 | 43301.54] loss=0.98 avg=0.98
[6764 | 43306.56] loss=1.27 avg=0.99
[6765 | 43311.62] loss=0.83 avg=0.99
[6766 | 43316.65] loss=1.22 avg=0.99
[6767 | 43321.60] loss=0.86 avg=0.99
[6768 | 43326.69] loss=0.61 avg=0.98
[6769 | 43331.68] loss=0.92 avg=0.98
[6770 | 43336.66] loss=0.50 avg=0.98
[6771 | 43341.67] loss=0.72 avg=0.97
[6772 | 43346.72] loss=0.95 avg=0.97
[6773 | 43351.62] loss=2.17 avg=0.99
[6774 | 43356.59] loss=1.01 avg=0.99
[6775 | 43361.59] loss=1.42 avg=0.99
[6776 | 43366.65] loss=0.63 avg=0.99
[6777 | 43371.67] loss=0.82 avg=0.99
[6778 | 43376.81] loss=0.96 avg=0.99
[6779 | 43381.91] loss=1.35 avg=0.99
[6780 | 43387.02] loss=1.04 avg=0.99
[6781 | 43392.13] loss=0.57 avg=0.99
[6782 | 43397.19] loss=1.07 avg=0.99
[6783 | 43402.18] loss=1.38 avg=0.99
[6784 | 43407.36] loss=1.01 avg=0.99
[6785 | 43412.45] loss=0.66 avg=0.99
[6786 | 43417.47] loss=1.20 avg=0.99
[6787 | 43422.41] loss=0.67 avg=0.99
[6788 | 43427.42] loss=0.89 avg=0.99
[6789 | 43432.54] loss=0.99 avg=0.99
[6790 | 43437.61] loss=1.56 avg=0.99
[6791 | 43442.62] loss=1.23 avg=0.99
[6792 | 43447.56] loss=0.65 avg=0.99
[6793 | 43452.58] loss=2.15 avg=1.00
[6794 | 43457.66] loss=1.14 avg=1.00
[6795 | 43462.64] loss=0.99 avg=1.00
[6796 | 43467.60] loss=1.87 avg=1.01
[6797 | 43472.53] loss=1.07 avg=1.01
[6798 | 43477.60] loss=0.62 avg=1.01
[6799 | 43482.73] loss=0.83 avg=1.01
Generating samples...
======== SAMPLE 1 ========
                                                                        #                                                                                                                                             #                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           

[6800 | 43570.57] loss=0.77 avg=1.00
[6801 | 43575.62] loss=0.63 avg=1.00
[6802 | 43580.66] loss=0.73 avg=1.00
[6803 | 43585.69] loss=0.64 avg=0.99
[6804 | 43590.64] loss=1.13 avg=1.00
[6805 | 43595.64] loss=1.27 avg=1.00
[6806 | 43600.67] loss=1.03 avg=1.00
[6807 | 43605.74] loss=1.54 avg=1.00
[6808 | 43610.65] loss=1.33 avg=1.01
[6809 | 43615.60] loss=1.17 avg=1.01
[6810 | 43620.60] loss=0.73 avg=1.01
[6811 | 43625.57] loss=1.16 avg=1.01
[6812 | 43630.54] loss=0.68 avg=1.00
[6813 | 43635.64] loss=1.07 avg=1.00
[6814 | 43640.69] loss=0.67 avg=1.00
[6815 | 43645.68] loss=0.95 avg=1.00
[6816 | 43650.68] loss=0.98 avg=1.00
[6817 | 43655.78] loss=0.91 avg=1.00
[6818 | 43660.74] loss=1.05 avg=1.00
[6819 | 43665.78] loss=0.68 avg=1.00
[6820 | 43670.80] loss=0.82 avg=1.00
[6821 | 43675.84] loss=1.19 avg=1.00
[6822 | 43680.84] loss=1.14 avg=1.00
[6823 | 43685.91] loss=1.30 avg=1.00
[6824 | 43690.95] loss=0.72 avg=1.00
[6825 | 43695.91] loss=0.60 avg=0.99
[6826 | 43701.00] loss=1.03 avg=1.00
[6827 | 43705.97] loss=0.96 avg=0.99
[6828 | 43710.99] loss=0.75 avg=0.99
[6829 | 43716.03] loss=0.95 avg=0.99
[6830 | 43721.06] loss=1.04 avg=0.99
[6831 | 43725.97] loss=1.45 avg=1.00
[6832 | 43730.99] loss=0.94 avg=1.00
[6833 | 43736.02] loss=1.35 avg=1.00
[6834 | 43741.06] loss=0.85 avg=1.00
[6835 | 43746.00] loss=1.09 avg=1.00
[6836 | 43751.07] loss=0.53 avg=0.99
[6837 | 43756.02] loss=0.64 avg=0.99
[6838 | 43761.08] loss=0.82 avg=0.99
[6839 | 43766.06] loss=0.85 avg=0.99
[6840 | 43771.05] loss=0.95 avg=0.99
[6841 | 43775.99] loss=0.87 avg=0.99
[6842 | 43781.07] loss=0.93 avg=0.99
[6843 | 43786.08] loss=1.38 avg=0.99
[6844 | 43791.05] loss=0.77 avg=0.99
[6845 | 43796.01] loss=0.73 avg=0.99
[6846 | 43801.06] loss=0.69 avg=0.98
[6847 | 43806.06] loss=0.93 avg=0.98
[6848 | 43811.13] loss=0.55 avg=0.98
[6849 | 43816.23] loss=0.65 avg=0.97
[6850 | 43821.29] loss=0.65 avg=0.97
[6851 | 43826.38] loss=1.05 avg=0.97
[6852 | 43831.47] loss=0.91 avg=0.97
[6853 | 43836.51] loss=1.15 avg=0.97
[6854 | 43841.51] loss=0.99 avg=0.97
[6855 | 43846.51] loss=0.58 avg=0.97
[6856 | 43851.61] loss=1.18 avg=0.97
[6857 | 43856.61] loss=1.57 avg=0.98
[6858 | 43861.62] loss=0.97 avg=0.98
[6859 | 43866.81] loss=0.91 avg=0.98
[6860 | 43871.94] loss=0.45 avg=0.97
[6861 | 43877.01] loss=0.70 avg=0.97
[6862 | 43882.00] loss=0.83 avg=0.97
[6863 | 43887.05] loss=0.57 avg=0.96
[6864 | 43892.16] loss=0.83 avg=0.96
[6865 | 43897.18] loss=0.77 avg=0.96
[6866 | 43902.17] loss=0.62 avg=0.96
[6867 | 43907.15] loss=0.66 avg=0.95
[6868 | 43912.12] loss=0.54 avg=0.95
[6869 | 43917.18] loss=1.20 avg=0.95
[6870 | 43922.20] loss=0.91 avg=0.95
[6871 | 43927.17] loss=0.73 avg=0.95
[6872 | 43932.17] loss=1.08 avg=0.95
[6873 | 43937.24] loss=0.87 avg=0.95
[6874 | 43942.33] loss=0.99 avg=0.95
[6875 | 43947.36] loss=0.90 avg=0.95
[6876 | 43952.37] loss=1.08 avg=0.95
[6877 | 43957.48] loss=0.59 avg=0.95
[6878 | 43962.65] loss=0.78 avg=0.95
[6879 | 43967.74] loss=0.47 avg=0.94
[6880 | 43972.78] loss=1.72 avg=0.95
[6881 | 43977.89] loss=1.11 avg=0.95
[6882 | 43982.91] loss=0.56 avg=0.95
[6883 | 43988.06] loss=0.93 avg=0.95
[6884 | 43993.12] loss=0.70 avg=0.94
[6885 | 43998.08] loss=0.75 avg=0.94
[6886 | 44003.02] loss=1.50 avg=0.95
[6887 | 44008.12] loss=0.71 avg=0.94
[6888 | 44013.13] loss=0.73 avg=0.94
[6889 | 44018.11] loss=0.71 avg=0.94
[6890 | 44023.17] loss=0.74 avg=0.94
[6891 | 44028.19] loss=1.56 avg=0.94
[6892 | 44033.20] loss=1.20 avg=0.95
[6893 | 44038.21] loss=1.20 avg=0.95
[6894 | 44043.21] loss=0.68 avg=0.95
[6895 | 44048.16] loss=0.56 avg=0.94
[6896 | 44053.16] loss=0.83 avg=0.94
[6897 | 44058.24] loss=0.89 avg=0.94
[6898 | 44063.29] loss=1.00 avg=0.94
[6899 | 44068.32] loss=0.77 avg=0.94
Generating samples...
======== SAMPLE 1 ========
             """
                 return self._get_value(value)
        else:
            # if not an object, return None
            return value[:, -1] + str(str(value))) train pybjs_pipeline/libs/utils.py pybjs-pipeline/pybjs-pipeline 85e45d2ba29cc6e69e67e0f3c6822a36b4a9035b https://github.com/pybjs-pipeline/pybjs-pipeline/blob/85e45d2ba29cc6e69e67e0f3c6822a36b4a9035b/pybjs_pipeline/libs/utils.py#L1021-L1025
def create_key_value(self, value, index=None, key_type=None):
        """ Create key_value for key in value.

        An instance of PyBJS'method method. If key == type', None:
        return self._get_data(value, index=index, key_type=key_type, value=value)

        return self._create_key_value(value) [u'def', u'create_key_value', u'(', u'self', u',', u'value', u',', u'index', u'=', u'None', u',', u'key_type', u'=', u'None', u')', u':', u'""" Create key_value for key in value.

        An instance of PyBJS'method method. If key == type', u'None', u':', u'return', u'self', u'.', u'_get_data', u'(', u'value', u',', u'index', u'=', u'index', u',', u'key_type', u'=', u'key_type', u',', u'value', u'=', u'value', u')', u'return', u'self', u'.', u'_create_key_value', u'(', u'value', u')'] Create key_value for key in value.

        An instance of PyBJS'method method. If key == type', u'None:
        return self._get_data(', u'value', u',', u'index', u'=', u'index', u',', u'key_type', u'=', u'key_type', u',', u'value', u'=', u'value', u')

        return self._create_key_value(', u'value', u') [u'def', u'create_key_value', u'(', u'self', u',', u'value', u',', u'index', u'=', u'None', u',', u'key_type', u'=', u'None', u')', u':', u"'Create key_value for key in value.

        An instance of PyBJS'method method.', u'If', u'key', u'==', u'type', u'None', u':', u'return', u'self', u'.', u'_get_data', u'(', u'value', u',', u'index', u'=', u'index', u',', u'key_type', u'=', u'key_type', u',', u'value', u'=', u'value', u')', u'return', u'self', u'.', u'_create_key_value', u'(', u'value', u')'] Create key_value for key in value.

        An instance of PyBJS'method method. If key == type', u'None:
            return self._get_data(', u'value', u',', u'index', u'=', u'index', u',', u'key_type', u'=', u'key_type', u

[6900 | 44807.00] loss=1.45 avg=0.95
[6901 | 44820.39] loss=0.57 avg=0.94
[6902 | 44834.87] loss=0.97 avg=0.94
[6903 | 44849.32] loss=0.75 avg=0.94
[6904 | 45316.73] loss=1.66 avg=0.95
[6905 | 45321.94] loss=1.22 avg=0.95
[6906 | 45336.31] loss=1.24 avg=0.95
[6907 | 45350.76] loss=1.05 avg=0.95
[6908 | 45382.94] loss=0.86 avg=0.95
[6909 | 45388.09] loss=1.32 avg=0.96
[6910 | 45393.09] loss=0.93 avg=0.96
[6911 | 45407.64] loss=0.92 avg=0.96
[6912 | 45422.00] loss=0.65 avg=0.95
[6913 | 45696.67] loss=0.90 avg=0.95
[6914 | 45701.95] loss=1.38 avg=0.96
[6915 | 45707.84] loss=1.12 avg=0.96
[6916 | 45722.15] loss=0.81 avg=0.96
[6917 | 45736.16] loss=0.87 avg=0.96
[6918 | 46212.58] loss=0.62 avg=0.95
[6919 | 46217.73] loss=0.88 avg=0.95
[6920 | 46223.52] loss=0.90 avg=0.95
[6921 | 46237.54] loss=1.19 avg=0.95
[6922 | 46251.78] loss=1.38 avg=0.96
[6923 | 46595.87] loss=1.41 avg=0.96
[6924 | 46601.00] loss=0.73 avg=0.96
[6925 | 46606.48] loss=0.58 avg=0.96
[6926 | 46620.65] loss=0.53 avg=0.95
[6927 | 46635.03] loss=1.25 avg=0.95
[6928 | 46666.85] loss=1.06 avg=0.96
[6929 | 46671.97] loss=0.61 avg=0.95
[6930 | 46677.33] loss=1.16 avg=0.95
[6931 | 46691.49] loss=1.00 avg=0.95
[6932 | 46705.52] loss=0.89 avg=0.95
[6933 | 46719.60] loss=1.82 avg=0.96
[6934 | 47116.64] loss=1.28 avg=0.97
[6935 | 47121.76] loss=1.17 avg=0.97
[6936 | 47135.76] loss=0.71 avg=0.97
[6937 | 47150.10] loss=1.06 avg=0.97
[6938 | 47164.52] loss=1.28 avg=0.97
[6939 | 47501.02] loss=0.94 avg=0.97
[6940 | 47506.23] loss=0.89 avg=0.97
[6941 | 47520.31] loss=1.10 avg=0.97
[6942 | 47534.46] loss=0.98 avg=0.97
[6943 | 47548.64] loss=1.09 avg=0.97
[6944 | 47915.84] loss=1.60 avg=0.98
[6945 | 47920.85] loss=0.68 avg=0.97
[6946 | 47934.78] loss=0.94 avg=0.97
[6947 | 47949.00] loss=1.61 avg=0.98
[6948 | 47963.32] loss=1.09 avg=0.98
[6949 | 48015.63] loss=0.65 avg=0.98
[6950 | 48020.77] loss=2.30 avg=0.99
[6951 | 48034.93] loss=0.93 avg=0.99
[6952 | 48049.41] loss=0.66 avg=0.99
[6953 | 48397.46] loss=0.62 avg=0.98
[6954 | 48402.86] loss=0.64 avg=0.98
[6955 | 48408.39] loss=0.76 avg=0.98
[6956 | 48422.57] loss=0.88 avg=0.98
[6957 | 48436.72] loss=0.54 avg=0.97
[6958 | 50781.53] loss=1.21 avg=0.98
[6959 | 50786.71] loss=0.61 avg=0.97
[6960 | 50791.94] loss=0.62 avg=0.97
[6961 | 50805.86] loss=1.41 avg=0.97
[6962 | 50819.97] loss=0.78 avg=0.97
[6963 | 50834.29] loss=1.23 avg=0.97
[6964 | 50848.60] loss=0.91 avg=0.97
[6965 | 50862.97] loss=0.93 avg=0.97
[6966 | 50877.17] loss=0.51 avg=0.97
[6967 | 50891.35] loss=0.72 avg=0.96
[6968 | 50905.53] loss=1.82 avg=0.97
[6969 | 50919.56] loss=0.92 avg=0.97
[6970 | 50933.81] loss=0.97 avg=0.97
[6971 | 50947.94] loss=0.52 avg=0.97
[6972 | 50962.43] loss=1.14 avg=0.97
[6973 | 50976.61] loss=1.13 avg=0.97
[6974 | 50990.65] loss=0.89 avg=0.97
[6975 | 51004.70] loss=1.14 avg=0.97
[6976 | 51018.78] loss=1.22 avg=0.98
[6977 | 51032.84] loss=0.98 avg=0.98
[6978 | 51047.01] loss=0.62 avg=0.97
[6979 | 51061.13] loss=1.11 avg=0.97
[6980 | 51075.56] loss=0.87 avg=0.97
[6981 | 51089.85] loss=0.78 avg=0.97
[6982 | 51104.07] loss=0.66 avg=0.97
[6983 | 51118.34] loss=0.83 avg=0.97
[6984 | 51132.61] loss=0.91 avg=0.97
[6985 | 51146.87] loss=1.32 avg=0.97
[6986 | 51161.03] loss=0.98 avg=0.97
[6987 | 51175.36] loss=0.86 avg=0.97
[6988 | 51189.73] loss=0.97 avg=0.97
[6989 | 51204.22] loss=1.19 avg=0.97
[6990 | 51218.54] loss=0.68 avg=0.97
[6991 | 51232.58] loss=0.94 avg=0.97
[6992 | 51246.78] loss=0.58 avg=0.96
[6993 | 51260.84] loss=1.16 avg=0.96
[6994 | 51274.82] loss=1.32 avg=0.97
[6995 | 51288.69] loss=0.99 avg=0.97
[6996 | 51302.75] loss=1.36 avg=0.97
[6997 | 51316.94] loss=1.40 avg=0.98
[6998 | 51331.10] loss=1.17 avg=0.98
[6999 | 51345.25] loss=1.36 avg=0.98
Saving checkpoint/run1/model-7000
WARNING:tensorflow:From /Users/eottens/cs230/gpt-2-csn/src/tutorial_env/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Generating samples...
======== SAMPLE 1 ========
 u'.', u'create_table', u'(', u')', u']', u':', u'return', u'data', u'if', u'ltype', u'is', u'not', u'None', u':', u'ltype', u'[', u"'line'", u']', u'=', u'ltype', u'[', u"'rtype'", u']', u'return', u'data', u'elif', u'ltype', u'in', u'ltype', u'and', u'elif', u'ltype', u'in', u'chips', u'and', u'not', u'ltype', u'in', u'bbox_table_namespace', u':', u'if', u'chips', u'is', u'not', u'None', u':', u'chips', u'[', u'"rtype"', u']', u'=', u'chips', u'[', u'"chips"', u']', u'return', u'data', u'return', u'data'] create a new tabular table.
    :param chips: chipset to add.
    :param bbox_table_namespace: bbox to add.

    :param chips: the columns of the Bbox Table named bbox_table_namespace
    :param chips: the bbox tables to add.
     :param cols: the rows in the bbox table.
     :param chips: the columns to add. [u'create', u'a', u'new', u'tabular', u'table', u'.', u':', u'param', u'chips', u':', u'chipset', u'to', u'add', u'.', u':', u'param', u'bbox_table_namespace', u':', u'bbox', u'to', u'add', u'.'] create_tabular_table python def create_tabular_table(chips, bbox_table_namespace: bbox_table_namespace, cols: int) -> BboxTable:
    '''create a new tabular table.
    :param chips: chipset to add.
    :param bbox_table_namespace: bbox to add.
    :param cols: the rows in the bbox table.
    :param chips: the columns to add.
    :param chips: the columns to add.

    '''
    if chips is not None:
        chips['rtype'] = chips['rtype'].create_table()
    else:
        chips['rtype'] = chips['rtype'].create_table(
            ltype='rtype',
            columns='rtype',
            ltype='name',
            chips="',
            cols="',
            chips='%s' % chips['name'],
            chips="%s'" % chips['chips'])
    return data
    elif chips is not None:
        chips['rtype'] = chips['rtype'].create_table()
    else:
        chips['rtype'] = chips['rtype'].create_table(
            chips="rtype",
            columns='rtype',
            chips="%s" % chips['name'],
            chips="%s'" % chips['chips'])
    return data
    elif chips, isinstance(chips, bbox_table_namespace):
        else:
           

[7000 | 52516.33] loss=1.39 avg=0.99
[7001 | 52521.82] loss=0.97 avg=0.99
[7002 | 52535.94] loss=1.19 avg=0.99
[7003 | 52550.19] loss=0.55 avg=0.98
[7004 | 52661.50] loss=1.07 avg=0.99
[7005 | 52666.76] loss=1.08 avg=0.99
[7006 | 52672.30] loss=0.50 avg=0.98
[7007 | 52687.08] loss=0.80 avg=0.98
[7008 | 52701.70] loss=0.85 avg=0.98
[7009 | 52896.05] loss=1.08 avg=0.98
[7010 | 52901.20] loss=0.91 avg=0.98
[7011 | 52907.39] loss=1.03 avg=0.98
[7012 | 52921.64] loss=1.41 avg=0.98
[7013 | 52935.91] loss=0.99 avg=0.98
[7014 | 53098.42] loss=0.54 avg=0.98
[7015 | 53103.57] loss=1.13 avg=0.98
[7016 | 53109.94] loss=1.42 avg=0.98
[7017 | 53124.16] loss=1.17 avg=0.99
[7018 | 53138.51] loss=1.12 avg=0.99
[7019 | 53153.19] loss=1.14 avg=0.99
[7020 | 53167.54] loss=1.25 avg=0.99
[7021 | 53181.56] loss=1.06 avg=0.99
[7022 | 53195.61] loss=1.22 avg=0.99
[7023 | 53219.95] loss=0.67 avg=0.99
[7024 | 53224.94] loss=0.31 avg=0.98
[7025 | 53237.62] loss=0.74 avg=0.98
[7026 | 53251.75] loss=0.75 avg=0.98
[7027 | 53266.10] loss=1.71 avg=0.99
[7028 | 53414.93] loss=1.24 avg=0.99
[7029 | 53420.06] loss=1.43 avg=0.99
[7030 | 53433.03] loss=1.76 avg=1.00
[7031 | 53447.33] loss=1.18 avg=1.00
[7032 | 53461.49] loss=1.62 avg=1.01
[7033 | 53501.41] loss=1.19 avg=1.01
[7034 | 53506.74] loss=1.48 avg=1.02
[7035 | 53520.12] loss=0.53 avg=1.01
[7036 | 53534.82] loss=0.69 avg=1.01
[7037 | 53549.13] loss=1.24 avg=1.01
[7038 | 53652.96] loss=0.47 avg=1.01
[7039 | 53658.16] loss=1.42 avg=1.01
[7040 | 53671.24] loss=1.47 avg=1.01
[7041 | 53685.49] loss=0.87 avg=1.01
[7042 | 53699.82] loss=2.07 avg=1.02
[7043 | 53724.17] loss=1.24 avg=1.03
[7044 | 53729.40] loss=1.09 avg=1.03
[7045 | 53743.12] loss=0.84 avg=1.02
[7046 | 53757.36] loss=0.81 avg=1.02
[7047 | 53771.55] loss=0.95 avg=1.02
[7048 | 53809.29] loss=1.63 avg=1.03
[7049 | 53814.46] loss=1.45 avg=1.03
[7050 | 53827.92] loss=0.75 avg=1.03
[7051 | 53842.23] loss=1.66 avg=1.03
[7052 | 53856.35] loss=0.78 avg=1.03
[7053 | 55678.12] loss=0.91 avg=1.03
[7054 | 55683.27] loss=1.76 avg=1.04
[7055 | 55696.86] loss=0.98 avg=1.04
[7056 | 55711.14] loss=0.99 avg=1.04
[7057 | 55725.36] loss=0.71 avg=1.03
[7058 | 55761.64] loss=0.75 avg=1.03
[7059 | 55766.69] loss=1.25 avg=1.03
[7060 | 55779.34] loss=0.78 avg=1.03
[7061 | 55793.49] loss=0.67 avg=1.03
[7062 | 55807.75] loss=0.86 avg=1.03
[7063 | 56115.65] loss=1.01 avg=1.03
[7064 | 56120.69] loss=0.86 avg=1.02
[7065 | 56133.03] loss=1.29 avg=1.03
[7066 | 56147.18] loss=0.77 avg=1.02
[7067 | 56161.16] loss=0.71 avg=1.02
[7068 | 56501.44] loss=0.83 avg=1.02
[7069 | 56506.52] loss=0.99 avg=1.02
[7070 | 56518.53] loss=0.89 avg=1.02
[7071 | 56533.08] loss=0.69 avg=1.01
[7072 | 56547.51] loss=1.01 avg=1.01
[7073 | 56630.03] loss=0.88 avg=1.01
[7074 | 56635.11] loss=0.99 avg=1.01
[7075 | 56647.44] loss=0.67 avg=1.01
[7076 | 56661.65] loss=0.71 avg=1.01
[7077 | 56675.70] loss=0.87 avg=1.00
[7078 | 57016.97] loss=0.75 avg=1.00
[7079 | 57022.05] loss=0.84 avg=1.00
[7080 | 57034.26] loss=1.00 avg=1.00
[7081 | 57048.24] loss=0.95 avg=1.00
[7082 | 57062.54] loss=0.55 avg=1.00
[7083 | 57400.30] loss=0.93 avg=0.99
[7084 | 57405.50] loss=0.91 avg=0.99
[7085 | 57417.85] loss=2.15 avg=1.01
[7086 | 57432.08] loss=0.94 avg=1.01
[7087 | 57446.27] loss=1.64 avg=1.01
[7088 | 57914.90] loss=1.79 avg=1.02
[7089 | 57919.96] loss=1.12 avg=1.02
[7090 | 57932.44] loss=0.63 avg=1.02
[7091 | 57946.57] loss=0.86 avg=1.01
[7092 | 57960.87] loss=0.66 avg=1.01
[7093 | 58300.20] loss=1.30 avg=1.01
[7094 | 58305.20] loss=1.59 avg=1.02
[7095 | 58317.32] loss=0.72 avg=1.02
[7096 | 58331.57] loss=0.85 avg=1.02
[7097 | 58345.86] loss=1.41 avg=1.02
[7098 | 58815.24] loss=1.28 avg=1.02
[7099 | 58820.30] loss=1.42 avg=1.03
Generating samples...
======== SAMPLE 1 ========
                 tl[:10], tl[:11], tl[:12], tl[:-2], tl[:13], tl[:13},
                                        tl[:14], tl[:14], tl[:-3]],
                tl[:15], tl[:-1], tl[:15], tl[:20], ...)
                                   
                 ld[]:   (
                             rtype_type=[t,tuple,list],
                                               tl[:10], tl[:11], tl[:13], tl[:13]}))
        :param len(tuple): number of tuples in the given range of tuples
                                               # For a given number of tuples, we return the
                                     (
                                                        (
                                                    tl[:10], tl[:11], tl[:15], tl[:-2]
                                   ),
                                     [0,  0.0,  0.0,  0.0, 0.0, 0.0, 1.0, 1.0],
                                                       a0, a1, a2, a3, ...)

                    :param a1: a tuple, in which
                                                                         a0, a1, a2, a3, ...         
                   :param a1: a tuple consisting of a single (n_) tuples (n__
                                                 [0,  1]): a1 is the number of tuples
                                    

[7100 | 60124.38] loss=0.75 avg=1.02
[7101 | 60138.85] loss=1.22 avg=1.03
[7102 | 60613.89] loss=1.00 avg=1.02
[7103 | 60619.02] loss=0.88 avg=1.02
[7104 | 60627.07] loss=0.88 avg=1.02
[7105 | 60641.28] loss=0.65 avg=1.02
[7106 | 60655.53] loss=1.72 avg=1.03
[7107 | 60997.46] loss=0.94 avg=1.02
[7108 | 61002.43] loss=1.17 avg=1.03
[7109 | 61010.19] loss=0.51 avg=1.02
[7110 | 61024.26] loss=0.99 avg=1.02
[7111 | 61038.62] loss=1.44 avg=1.02
[7112 | 61513.78] loss=2.45 avg=1.04
[7113 | 61518.82] loss=0.75 avg=1.04
[7114 | 61526.42] loss=0.85 avg=1.03
[7115 | 61540.63] loss=0.98 avg=1.03
[7116 | 61554.88] loss=0.92 avg=1.03
[7117 | 61897.33] loss=0.87 avg=1.03
[7118 | 61902.44] loss=1.07 avg=1.03
[7119 | 61909.76] loss=0.64 avg=1.03
[7120 | 61923.95] loss=1.31 avg=1.03
[7121 | 61938.43] loss=1.53 avg=1.04
[7122 | 62414.75] loss=1.10 avg=1.04
[7123 | 62419.77] loss=0.75 avg=1.03
[7124 | 62427.20] loss=1.12 avg=1.03
[7125 | 62441.42] loss=0.78 avg=1.03
[7126 | 62455.94] loss=0.86 avg=1.03
[7127 | 62797.52] loss=0.96 avg=1.03
[7128 | 62802.57] loss=1.36 avg=1.03
[7129 | 62810.06] loss=0.69 avg=1.03
[7130 | 62824.11] loss=0.90 avg=1.03
[7131 | 62837.96] loss=0.78 avg=1.02
[7132 | 63312.36] loss=1.37 avg=1.03
[7133 | 63317.49] loss=0.83 avg=1.03
[7134 | 63324.84] loss=0.73 avg=1.02
[7135 | 63339.23] loss=1.13 avg=1.02
[7136 | 63353.52] loss=0.66 avg=1.02
[7137 | 63531.66] loss=0.65 avg=1.02
[7138 | 63536.79] loss=0.61 avg=1.01
[7139 | 63544.14] loss=0.68 avg=1.01
[7140 | 63558.38] loss=1.09 avg=1.01
[7141 | 63572.48] loss=0.61 avg=1.01
[7142 | 63699.05] loss=0.55 avg=1.00
[7143 | 63704.07] loss=0.61 avg=1.00
[7144 | 63710.88] loss=1.37 avg=1.00
[7145 | 63725.00] loss=0.70 avg=1.00
[7146 | 63739.18] loss=0.58 avg=0.99
[7147 | 64269.60] loss=0.44 avg=0.99
[7148 | 64274.78] loss=0.65 avg=0.99
[7149 | 64281.60] loss=1.39 avg=0.99
[7150 | 64295.71] loss=0.99 avg=0.99
[7151 | 64309.67] loss=0.91 avg=0.99
[7152 | 64407.66] loss=0.89 avg=0.99
[7153 | 64412.85] loss=1.31 avg=0.99
[7154 | 64419.61] loss=1.29 avg=0.99
[7155 | 64434.16] loss=1.23 avg=1.00
[7156 | 64448.60] loss=0.50 avg=0.99
[7157 | 64596.87] loss=1.08 avg=0.99
[7158 | 64602.05] loss=1.55 avg=1.00
[7159 | 64609.34] loss=1.34 avg=1.00
[7160 | 64623.41] loss=0.85 avg=1.00
[7161 | 64637.49] loss=1.14 avg=1.00
[7162 | 65112.16] loss=0.95 avg=1.00
[7163 | 65117.14] loss=1.57 avg=1.01
[7164 | 65124.00] loss=0.79 avg=1.00
[7165 | 65138.16] loss=1.74 avg=1.01
[7166 | 65152.22] loss=0.46 avg=1.01
[7167 | 65497.73] loss=0.63 avg=1.00
[7168 | 65502.73] loss=1.14 avg=1.00
[7169 | 65509.14] loss=0.66 avg=1.00
[7170 | 65523.47] loss=0.43 avg=0.99
[7171 | 65537.55] loss=0.53 avg=0.99
[7172 | 65623.11] loss=0.99 avg=0.99
[7173 | 65628.22] loss=1.02 avg=0.99
[7174 | 65634.35] loss=0.61 avg=0.99
[7175 | 65648.49] loss=1.36 avg=0.99
[7176 | 65662.46] loss=1.32 avg=0.99
[7177 | 66011.41] loss=0.68 avg=0.99
[7178 | 66016.54] loss=1.25 avg=0.99
[7179 | 66022.29] loss=0.53 avg=0.99
[7180 | 66036.46] loss=0.93 avg=0.99
[7181 | 66050.56] loss=0.86 avg=0.99
[7182 | 66095.05] loss=1.03 avg=0.99
[7183 | 66100.27] loss=0.84 avg=0.99
[7184 | 66105.84] loss=0.69 avg=0.98
[7185 | 66120.45] loss=1.04 avg=0.98
[7186 | 66134.50] loss=1.22 avg=0.99
[7187 | 66213.14] loss=1.00 avg=0.99
[7188 | 66218.20] loss=0.94 avg=0.99
[7189 | 66223.09] loss=1.65 avg=0.99
[7190 | 66236.69] loss=0.83 avg=0.99
[7191 | 66250.61] loss=0.98 avg=0.99
[7192 | 66264.74] loss=1.41 avg=0.99
[7193 | 66399.81] loss=0.65 avg=0.99
[7194 | 66404.91] loss=0.77 avg=0.99
[7195 | 66418.28] loss=0.85 avg=0.99
[7196 | 66432.38] loss=1.09 avg=0.99
[7197 | 66446.50] loss=1.46 avg=0.99
[7198 | 66915.26] loss=0.89 avg=0.99
[7199 | 66920.23] loss=0.57 avg=0.99
Generating samples...
======== SAMPLE 1 ========
m'", u')', u'the', u'pipeline', u'metadata', u'is', u'config', u'(', u'threads', u'.', u'PIPCODE_PATH', u'or', u"'--ipcp'", u'or', u"'--ipsec'", u')', u'.', u'The', u'opcode', u'and', u'threads', u'.', u'PIPCODE_PATH', u'and', u'ipsec', u'as', u'keyword', u'are', u'config', u'(', u'pipeline_path', u'or', u"'--pip'", u'or', u"'--ipsec'", u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'ret', u'[', u'threads', u'.', u'PIPCODE_PATH', u']', u'.', u'The', u'opcode', u'and', u'threads', u'.', u'PIPCODE_PATH', u'and', u'ipsec', u'as', u'keyword', u'are', u'config', u'(', u'pipeline_path', u'or', u"'--pip'", u'or', u"'--ipsec'", u')', u'ret', u'[', u'threads', u'.', u'PIPCODE_PATH', u']', u'.', u'The', u'threads', u'.', u'PIPCODE_PATH', u'and', u'ipsec', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword' are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The', u'keyword', u'and', u'port', u'as', u'keyword', u'are', u'config', u'(', u'keyword', u'and', u'port', u'as', u'keyword', u')', u'.', u'The',

[7200 | 67905.33] loss=0.72 avg=0.98
[7201 | 67919.35] loss=0.87 avg=0.98
[7202 | 68200.75] loss=0.87 avg=0.98
[7203 | 68205.92] loss=1.00 avg=0.98
[7204 | 68220.03] loss=2.02 avg=0.99
[7205 | 68234.08] loss=1.23 avg=1.00
[7206 | 68248.05] loss=1.22 avg=1.00
[7207 | 68716.06] loss=1.76 avg=1.01
[7208 | 68721.18] loss=1.07 avg=1.01
[7209 | 68735.12] loss=0.61 avg=1.00
[7210 | 68749.08] loss=1.38 avg=1.01
[7211 | 68763.35] loss=1.42 avg=1.01
[7212 | 69102.93] loss=1.63 avg=1.02
[7213 | 69107.90] loss=0.81 avg=1.01
[7214 | 69121.56] loss=0.94 avg=1.01
[7215 | 69135.77] loss=0.91 avg=1.01
[7216 | 69150.08] loss=0.62 avg=1.01
[7217 | 69547.97] loss=0.87 avg=1.01
[7218 | 69553.16] loss=0.31 avg=1.00
[7219 | 69567.50] loss=1.03 avg=1.00
[7220 | 69581.69] loss=0.80 avg=1.00
[7221 | 69595.97] loss=1.16 avg=1.00
[7222 | 69628.10] loss=0.58 avg=1.00
[7223 | 69633.06] loss=0.59 avg=0.99
[7224 | 69646.00] loss=0.90 avg=0.99
[7225 | 69660.37] loss=0.79 avg=0.99
[7226 | 69674.48] loss=0.89 avg=0.99
[7227 | 70222.96] loss=0.72 avg=0.99
[7228 | 70228.04] loss=0.80 avg=0.98
[7229 | 70240.83] loss=1.04 avg=0.98
[7230 | 70254.83] loss=0.62 avg=0.98
[7231 | 70268.83] loss=1.32 avg=0.98
[7232 | 70514.84] loss=0.87 avg=0.98
[7233 | 70519.94] loss=1.32 avg=0.99
[7234 | 70532.40] loss=0.75 avg=0.98
[7235 | 70546.66] loss=1.24 avg=0.99
[7236 | 70561.00] loss=0.79 avg=0.98
[7237 | 70900.52] loss=0.96 avg=0.98
[7238 | 70905.64] loss=0.72 avg=0.98
[7239 | 70918.57] loss=0.79 avg=0.98
[7240 | 70932.68] loss=1.02 avg=0.98
[7241 | 70946.96] loss=0.56 avg=0.98
[7242 | 71417.48] loss=1.17 avg=0.98
[7243 | 71422.51] loss=0.74 avg=0.98
[7244 | 71435.23] loss=1.16 avg=0.98
[7245 | 71449.51] loss=0.77 avg=0.97
[7246 | 71463.45] loss=1.07 avg=0.98
[7247 | 71799.16] loss=0.95 avg=0.98
[7248 | 71804.29] loss=0.99 avg=0.98
[7249 | 71816.61] loss=1.46 avg=0.98
[7250 | 71830.69] loss=0.84 avg=0.98
[7251 | 71844.75] loss=0.88 avg=0.98
[7252 | 72316.85] loss=0.65 avg=0.97
[7253 | 72321.91] loss=1.05 avg=0.98
[7254 | 72334.09] loss=1.00 avg=0.98
[7255 | 72348.22] loss=1.14 avg=0.98
[7256 | 72362.27] loss=1.35 avg=0.98
[7257 | 72699.02] loss=0.95 avg=0.98
[7258 | 72704.09] loss=1.98 avg=0.99
[7259 | 72715.93] loss=1.30 avg=0.99
[7260 | 72730.06] loss=0.63 avg=0.99
[7261 | 72744.24] loss=1.06 avg=0.99
[7262 | 73214.55] loss=1.34 avg=0.99
[7263 | 73219.65] loss=1.34 avg=1.00
[7264 | 73231.22] loss=0.85 avg=1.00
[7265 | 73245.36] loss=0.82 avg=0.99
[7266 | 73259.51] loss=0.53 avg=0.99
[7267 | 73394.32] loss=1.08 avg=0.99
[7268 | 73399.43] loss=0.72 avg=0.99
[7269 | 73411.04] loss=1.90 avg=1.00
[7270 | 73425.17] loss=1.15 avg=1.00
[7271 | 73439.31] loss=0.65 avg=1.00
[7272 | 73600.30] loss=1.16 avg=1.00
[7273 | 73605.34] loss=0.89 avg=1.00
[7274 | 73616.69] loss=1.04 avg=1.00
[7275 | 73630.75] loss=1.77 avg=1.00
[7276 | 73644.65] loss=1.42 avg=1.01
[7277 | 74114.17] loss=0.89 avg=1.01
[7278 | 74119.33] loss=1.17 avg=1.01
[7279 | 74130.23] loss=0.92 avg=1.01
[7280 | 74144.28] loss=0.94 avg=1.01
[7281 | 74158.47] loss=0.94 avg=1.01
[7282 | 74499.05] loss=1.54 avg=1.01
[7283 | 74504.12] loss=1.49 avg=1.02
[7284 | 74515.06] loss=1.13 avg=1.02
[7285 | 74529.41] loss=0.71 avg=1.01
[7286 | 74543.86] loss=0.65 avg=1.01
[7287 | 75018.54] loss=1.03 avg=1.01
[7288 | 75023.62] loss=0.86 avg=1.01
[7289 | 75035.09] loss=1.02 avg=1.01
[7290 | 75049.53] loss=0.52 avg=1.01
[7291 | 75063.87] loss=0.73 avg=1.00
[7292 | 75398.90] loss=1.52 avg=1.01
[7293 | 75403.95] loss=1.12 avg=1.01
[7294 | 75415.59] loss=0.88 avg=1.01
[7295 | 75429.89] loss=1.09 avg=1.01
[7296 | 75444.05] loss=1.27 avg=1.01
[7297 | 75754.88] loss=1.12 avg=1.01
[7298 | 75759.99] loss=0.73 avg=1.01
[7299 | 75771.48] loss=0.79 avg=1.01
Generating samples...
======== SAMPLE 1 ========
Lupus
        :return the unit from the pip:
        :type pip: unit: bool
        """
        unit = unit_to_unit(type):
        return unit.unit train pyunit/utils.py pyunit/pyunit 8d0e8f48ceafb7d54a9829ac9a9fcbd29fe2e54b9 https://github.com/pyunit/pyunit/blob/8d0e8f48ceafb7d54a9829ac9a9fcbd29fe2e54b9/pyunit/utils.py#L39-L44
def unit_to_unit(self):
        """
        Returns a unit from ``self.unit``. If this method is called in a pip
        and the unit does not need to be computed, it will return ``True``.

        This method is useful for creating unit and method instances. If the
        unit is not a unit then this method will ``True``.

        :param unit:

             ``unit_to_unit`` is a list of `**` instances.
             All ```()s are returned to the caller.
        :param unit_to_unit(self):

             If ``self.unit`` is not a unit, then the unit will return ``True``.
             Otherwise it will return ``True``.

        :param unit_to_unit(self):

            If ``self.unit`` is a unit, then this method returns the unit
            instance
             ``self.unit`` will receive the instance in the ``self.unit`` variable.
            (optional) If ``self.unit`` is a unit, then you can return it as
              a tuple:

                 ... [Tuple("self.unit", "self.unit_to"))
                 ... [Tuple(self.unit_to_val))
                 """
        return self.unit_to_unit(self):

        try:
            self.unit = unit.unit()
        except:
            try:
                self.unit = unit.unit()
        except:
                self.unit = unit.unit() [u'def', u'unit_to_unit', u'(', u'self', u')', u':', u'return', u'self', u'.', u'unit_to_unit', u'(', u'self', u')', u':', u'try', u':', u'self', u'.', u'unit', u'=', u'unit', u'.', u'total_val', u'(', u')', u'except', u':', u'self', u'.', u'unit', u'=', u'unit', u'.', u'total_val', u'(', u')'] Returns a unit from ``self.unit``. If this method is called in a pip
        and the unit does not need to be computed, it will return ``True``.

        This method is useful for creating unit and method instances. If the
        unit is not a unit then this method will ``True``.

        :param unit:

       

[7300 | 76340.20] loss=0.36 avg=1.00
[7301 | 76488.70] loss=1.99 avg=1.01
[7302 | 76493.99] loss=0.70 avg=1.01
[7303 | 76502.99] loss=0.71 avg=1.00
[7304 | 76517.36] loss=1.11 avg=1.01
[7305 | 76531.57] loss=0.80 avg=1.00
[7306 | 76561.65] loss=1.37 avg=1.01
[7307 | 76566.89] loss=0.84 avg=1.01
[7308 | 76576.57] loss=0.86 avg=1.00
[7309 | 76590.73] loss=0.57 avg=1.00
[7310 | 76605.32] loss=0.63 avg=1.00
[7311 | 76657.27] loss=0.79 avg=0.99
[7312 | 76662.16] loss=0.86 avg=0.99
[7313 | 76668.80] loss=1.70 avg=1.00
[7314 | 76682.92] loss=0.76 avg=1.00
[7315 | 76696.99] loss=1.05 avg=1.00
[7316 | 76727.20] loss=0.62 avg=0.99
[7317 | 76732.34] loss=0.62 avg=0.99
[7318 | 76739.07] loss=1.32 avg=0.99
[7319 | 76753.18] loss=0.83 avg=0.99
[7320 | 76767.42] loss=2.16 avg=1.00
[7321 | 76811.73] loss=0.96 avg=1.00
[7322 | 76816.74] loss=0.64 avg=1.00
[7323 | 76822.74] loss=0.88 avg=1.00
[7324 | 76836.62] loss=1.03 avg=1.00
[7325 | 76850.75] loss=0.80 avg=1.00
[7326 | 76908.10] loss=0.89 avg=1.00
[7327 | 76913.48] loss=1.19 avg=1.00
[7328 | 76918.40] loss=1.69 avg=1.00
[7329 | 76923.39] loss=0.75 avg=1.00
[7330 | 76928.70] loss=0.72 avg=1.00
[7331 | 76933.97] loss=1.74 avg=1.01
[7332 | 76939.18] loss=0.86 avg=1.00
[7333 | 76944.32] loss=0.62 avg=1.00
[7334 | 76949.71] loss=0.49 avg=1.00
[7335 | 76954.92] loss=1.29 avg=1.00
[7336 | 76960.25] loss=1.26 avg=1.00
[7337 | 76965.40] loss=0.91 avg=1.00
[7338 | 76970.61] loss=0.61 avg=1.00
[7339 | 76975.79] loss=0.88 avg=1.00
[7340 | 76981.03] loss=0.90 avg=0.99
[7341 | 76986.64] loss=0.57 avg=0.99
[7342 | 76991.98] loss=0.94 avg=0.99
[7343 | 76997.42] loss=0.87 avg=0.99
[7344 | 77002.58] loss=1.51 avg=0.99
[7345 | 77007.72] loss=1.63 avg=1.00
[7346 | 77012.94] loss=1.27 avg=1.00
[7347 | 77018.54] loss=0.97 avg=1.00
[7348 | 77024.13] loss=0.91 avg=1.00
[7349 | 77029.44] loss=1.62 avg=1.01
[7350 | 77034.76] loss=2.33 avg=1.02
[7351 | 77040.10] loss=0.87 avg=1.02
[7352 | 77045.51] loss=0.52 avg=1.01
[7353 | 77051.13] loss=1.38 avg=1.02
[7354 | 77056.43] loss=0.83 avg=1.02
[7355 | 77061.74] loss=0.73 avg=1.01
[7356 | 77067.30] loss=0.89 avg=1.01
[7357 | 77072.78] loss=1.10 avg=1.01
[7358 | 77078.27] loss=0.77 avg=1.01
[7359 | 77083.90] loss=1.23 avg=1.01
[7360 | 77089.39] loss=1.11 avg=1.01
[7361 | 77094.85] loss=0.64 avg=1.01
[7362 | 77100.54] loss=1.02 avg=1.01
[7363 | 77105.98] loss=1.13 avg=1.01
[7364 | 77111.26] loss=0.56 avg=1.01
[7365 | 77116.89] loss=0.90 avg=1.01
[7366 | 77122.37] loss=2.15 avg=1.02
[7367 | 77127.85] loss=1.12 avg=1.02
[7368 | 77133.17] loss=0.74 avg=1.02
[7369 | 77138.44] loss=0.91 avg=1.01
[7370 | 77144.18] loss=1.98 avg=1.02
[7371 | 77149.61] loss=0.62 avg=1.02
[7372 | 77155.07] loss=1.21 avg=1.02
[7373 | 77160.72] loss=0.57 avg=1.02
[7374 | 77166.35] loss=0.62 avg=1.01
[7375 | 77171.81] loss=1.00 avg=1.01
[7376 | 77177.20] loss=0.50 avg=1.01
[7377 | 77182.53] loss=0.44 avg=1.00
[7378 | 77187.87] loss=0.77 avg=1.00
[7379 | 77193.24] loss=0.93 avg=1.00
[7380 | 77198.77] loss=1.08 avg=1.00
[7381 | 77204.19] loss=0.65 avg=1.00
[7382 | 77209.78] loss=0.97 avg=1.00
[7383 | 77215.29] loss=1.96 avg=1.01
[7384 | 77220.93] loss=1.21 avg=1.01
[7385 | 77226.34] loss=0.49 avg=1.00
[7386 | 77231.78] loss=1.98 avg=1.01
[7387 | 77237.05] loss=1.35 avg=1.02
[7388 | 77242.42] loss=1.11 avg=1.02
[7389 | 77247.60] loss=1.02 avg=1.02
[7390 | 77252.90] loss=0.80 avg=1.02
[7391 | 77258.55] loss=0.58 avg=1.01
[7392 | 77264.01] loss=1.04 avg=1.01
[7393 | 77269.70] loss=1.39 avg=1.01
[7394 | 77275.18] loss=0.63 avg=1.01
[7395 | 77280.33] loss=1.24 avg=1.01
[7396 | 77285.86] loss=1.27 avg=1.02
[7397 | 77291.53] loss=0.86 avg=1.01
[7398 | 77297.37] loss=1.12 avg=1.02
[7399 | 77302.97] loss=0.66 avg=1.01
Generating samples...
======== SAMPLE 1 ========
 u.__name__)
    """
    # Add a 'version' in the name
    if not hasattr(meth) and not hasattr(meth, 'version')
    self.setVersionFromFile(meth)
    self.setVersionFromFile_withFile(meth)
    self.setVersionFromFile_withFile(meth)
    self.setVersionFromFile_withFile(meth)
    self.setVersionFromFile_withFile(METH)
    self.setVersionFromFile_withFile(METH)
    if os.path.exists(meth) and not os.path.exists(self.version)
    self.deleteFromFile(meth)
    self.deleteFromFile_withFile(meth)
    if isinstance(self.version, filetype.VPRM):
    self.commit()
    self.deleteFromFile()
    self.deleteFromFile_withFile(self)
    if not hasattr(meth, 'version') in self
    if not self.deleteFromFile :
        raise InvalidArgument(
                'You need this as the version.'
                )
    self.deleteFromFile(meth)
    self.deleteFromFile_withFile(self)
    if not hasattr(self.version, filetype.VPRM):
        raise InvalidArgument(
              'You need this as the version.'
              )
    self.deleteFromFile(self)
    return self.getVersionFromFile(self.version) [u'def', u'getVersionFromFile', u'(', u'self', u')', u':', u'# Add a 'version' in the name', u'if', u'not', u'hasattr', u'(', u'meth', u')', u'and', u'not', u'hasattr', u'(', u'meth', u',', u"'version'", u')', u'.', u'notExists', u'(', u'meth', u')', u'and', u'not', u'hasattr', u'(', u'meth', u',', u"'version'", u')', u'.', u'notExists', u'(', u'meth', u',', u"'version'", u',', u'METH', u')', u'self', u'.', u'differentFromFile', u'(', u'meth', u')', u'self', u'.', u'tetVersionFromFile', u'(', u'meth', u')', u'self', u'.', u'tetVersionFromFile_withFile', u'(', u'meth', u')', u'self', u'.', u'setVersionFromFile_withFile', u'(', u'METH', u')', u'self', u'.', u'setVersionFromFile_withFile', u'(', u'METH', u')', u'self', u'.', u'setVersionFromFile_withFile', u'(', u'METH', u'METH', u')', u'self', u'.', u'setVersionFromFile_withFile', u'(', u'METH', u'METH', u')', u'self', u'.', u'setVersionFromFile_withFile', u'(', u'METH', u'METH', u')', u'self', u'.', u'differentFromFile', u'(', u'meth', u')', u'self', u'.', u'tetVersionFromFile_withFile', u'(', u'METH', u')', u'self', u'.', u'getVersionFromFile', u'(', u'self', u'.', u'version', u')'] A method to retrieve an existing file or filetype from. [u'A', u'model', u'to', u'hive', u'a', u'new', u'document

[7400 | 77394.46] loss=0.89 avg=1.01
[7401 | 77399.85] loss=0.66 avg=1.01
[7402 | 77405.38] loss=0.54 avg=1.00
[7403 | 77411.07] loss=1.85 avg=1.01
[7404 | 77416.65] loss=0.71 avg=1.01
[7405 | 77422.33] loss=1.59 avg=1.01
[7406 | 77427.89] loss=0.81 avg=1.01
[7407 | 77433.49] loss=0.99 avg=1.01
[7408 | 77439.00] loss=1.06 avg=1.01
[7409 | 77444.95] loss=0.67 avg=1.01
[7410 | 77450.71] loss=1.78 avg=1.02
[7411 | 77456.35] loss=1.69 avg=1.02
[7412 | 77461.69] loss=0.89 avg=1.02
[7413 | 77466.93] loss=0.61 avg=1.02
[7414 | 77472.21] loss=0.91 avg=1.02
[7415 | 77477.58] loss=0.99 avg=1.02
[7416 | 77482.94] loss=1.55 avg=1.02
[7417 | 77488.46] loss=1.58 avg=1.03
[7418 | 77494.06] loss=1.57 avg=1.03
[7419 | 77499.55] loss=0.95 avg=1.03
[7420 | 77505.00] loss=1.00 avg=1.03
[7421 | 77510.62] loss=0.70 avg=1.03
[7422 | 77516.04] loss=1.19 avg=1.03
[7423 | 77521.45] loss=0.87 avg=1.03
[7424 | 77526.95] loss=1.24 avg=1.03
[7425 | 77532.35] loss=1.14 avg=1.03
[7426 | 77537.73] loss=0.98 avg=1.03
[7427 | 77543.26] loss=1.43 avg=1.03
[7428 | 77548.55] loss=1.25 avg=1.04
[7429 | 77553.95] loss=1.05 avg=1.04
[7430 | 77559.19] loss=1.04 avg=1.04
[7431 | 77564.99] loss=0.89 avg=1.04
[7432 | 77570.41] loss=0.99 avg=1.04
[7433 | 77576.11] loss=1.04 avg=1.04

